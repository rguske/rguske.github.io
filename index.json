[{"categories":["VMware","Tanzu","Management","Kubernetes"],"content":"This post introduces the Tanzu Mission Control Catalog feature and provides a closer look on the provided simplicity to extend your Kubernetes Clusters with the automated installation of Tanzu Packages through the catalog.","date":"2022-01-08","objectID":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/","tags":["January2022","Tanzu","TanzuPackages","TanzuStandard","TMC","Kubernetes"],"title":"Deploying Tanzu Packages using Tanzu Mission Control Catalog","uri":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/"},{"categories":["VMware","Tanzu","Management","Kubernetes"],"content":"The Challenge Just providing a Kubernetes cluster to an application team is in most cases half of the battle. It’s often necessary to also provide certain capabilities (extensions) like e.g. for logging or for ingress to those clusters, to ultimately support the production workload. Easily extending customer Kubernetes environments with official supported open source solutions to support the aforementioned capabilities and even more, should provide a decent level of simplicity in terms of deployment and lifecycle management. ","date":"2022-01-08","objectID":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/:1:0","tags":["January2022","Tanzu","TanzuPackages","TanzuStandard","TMC","Kubernetes"],"title":"Deploying Tanzu Packages using Tanzu Mission Control Catalog","uri":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/"},{"categories":["VMware","Tanzu","Management","Kubernetes"],"content":"A Catalog of Extensions Tanzu Mission Control, a VMware Cloud Service (SaaS), is VMware’s multi-cloud Kubernetes management platform which provides a centralized management for consistently operating and securing Kubernetes infrastructures and modern applications through a centralized policy management across all deployed and attached Kubernetes clusters. By deeply incorporating open source projects like e.g. Velero for Backup \u0026 Recovery or Sonobuoy for cluster conformance checks, just to name a few, as well as the integration in other VMware solutions like Tanzu Service Mesh or Tanzu Observability, Tanzu Mission Control already offers a rich set of features to multiple personas/teams. This set was recently extended by a another great new feature which is called Catalog. W00t w00t!@VMwareTanzu Mission Control Catalog feature is GA 🥳 Deploying your software to different types of #Kubernetes clusters made easy. 🔷 RN: https://t.co/IHIRBeLPem 🔷 Docs: https://t.co/zkgRGnTSvg#VMware #RunAllTheThings pic.twitter.com/mrynjJeYpx — Robert Guske (@vmw_rguske) December 2, 2021 The Catalog is exactly providing this in my introduction mentioned simplicity to exetend your Kubernetes environments by providing an easy and opinionated way to install solutions for e.g. monitoring, logging, ingress, external-dns and others, with have official support by VMware. Furthermore, it’s also possible to add your own repositories of solutions to Catalog to give you even more flexibility and extensibility. Figure I shows the currently available PACKAGES. Figure I: Available Packages through the Catalog\" Figure I: Available Packages through the Catalog ","date":"2022-01-08","objectID":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/:2:0","tags":["January2022","Tanzu","TanzuPackages","TanzuStandard","TMC","Kubernetes"],"title":"Deploying Tanzu Packages using Tanzu Mission Control Catalog","uri":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/"},{"categories":["VMware","Tanzu","Management","Kubernetes"],"content":"Extensions, Solutions, PACKAGES If you’ve read the article diligently up to here, you may noticed that I always used the terms solutions or extensions when talking about extending your Kubernetes environments. In Catalog, VMware is referring to Packages and the reason for this is, that Catalog is using Carvel behind the scenes. Project Carvel provides a comprehensive toolbox for building, templating, packaging, deploying and day2-ing modern applications. This is a 🔥 topic and I’d recommend reading this great blog post 👉 Tanzu Packages Explained 👈 by my dear friend Alex to get to know it. ","date":"2022-01-08","objectID":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/:2:1","tags":["January2022","Tanzu","TanzuPackages","TanzuStandard","TMC","Kubernetes"],"title":"Deploying Tanzu Packages using Tanzu Mission Control Catalog","uri":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/"},{"categories":["VMware","Tanzu","Management","Kubernetes"],"content":"Setting the Scenes To make use of Tanzu Mission Control and ultimately of the introduced features (across clouds!), you have to make your Kubernetes cluster available in TMC. This can be done in two ways. Creating or attaching. Creating a cluster is only possible if you have registered a Tanzu Kubernetes Grid (TKGm) Management cluster or a vSphere with Tanzu (TKGs) Management (Supervisor) cluster in TMC. Attaching on the other hand, can be done to any conformant Kubernetes cluster. Keep the following table in mind before deploying Tanzu Packages using the Catalog to your Kubernetes cluster. Supported Not Supported Tanzu Kubernetes Grid Service workload clusters Tanzu Kubernetes Grid Service Supervisor Clusters Tanzu Kubernetes Grid workload clusters (version 1.4 and later) Tanzu Kubernetes Grid Management Clusters GKE (GCP) Tanzu Kubernetes Grid workload clusters (version 1.3 and earlier) EKS (AWS) Tanzu Standard packages are not supported on clusters where PSP is enabled AKS (Azure) Kind Existing kapp-controller installation If you have previously installed Carvel’s kapp-controller on your cluster, you must remove the installation and all related resources before using the catalog feature. ","date":"2022-01-08","objectID":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/:3:0","tags":["January2022","Tanzu","TanzuPackages","TanzuStandard","TMC","Kubernetes"],"title":"Deploying Tanzu Packages using Tanzu Mission Control Catalog","uri":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/"},{"categories":["VMware","Tanzu","Management","Kubernetes"],"content":"Attach a K8s Cluster to TMC - GUI Attaching a Kubernetes cluster is made simple by e.g. using the TMC UI. On the left pane under the section Clusters, select ATTACH CLUSTER and provide the requested data. Figure II: Attaching K8s Cluster via GUI\" Figure II: Attaching K8s Cluster via GUI Following the steps to the end, a kubectl command is provided which can be executed directly to your Kubernetes cluster in order to attach your cluster to TMC. Figure III: Command to attach your K8s cluster to TMC\" Figure III: Command to attach your K8s cluster to TMC Wasn’t that simple? 🙂 Now let’s do it again but this time we are going to use the TMC CLI. ","date":"2022-01-08","objectID":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/:3:1","tags":["January2022","Tanzu","TanzuPackages","TanzuStandard","TMC","Kubernetes"],"title":"Deploying Tanzu Packages using Tanzu Mission Control Catalog","uri":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/"},{"categories":["VMware","Tanzu","Management","Kubernetes"],"content":"Attach a K8s Cluster to TMC - CLI The TMC CLI can be downloaded directly from the TMC portal. Figure IV: Download Tanzu Mission Control CLI\" Figure IV: Download Tanzu Mission Control CLI Go to Automation center and follow the provided instructions. In order to make use of the CLI you have to be a member of a VMware Cloud Services organization that has access to Tanzu Mission Control. Attaching a cluster requires that you are associated with the clustergroup.edit role on the cluster group in which you want to attach the cluster. The TMC CLI is currently in v1alpha1 version available but it already provides a bunch of resources with which we can interact with. tmc --help ____ _ ____ | |\\/| |___ Usage: tmc [resource|action|helper] [flags] Resources: account Account management audit Run an audit request on an org cluster A Kubernetes Cluster. clustergroup A group of Kubernetes clusters. dataprotection Backup, restore, or migrate cluster data. integration Get available integrations and their information from registry. managementcluster A management cluster maintains the connection between a Tanzu Kubernetes Grid instance and the platform. organization Organization level management. workspace A group of managed Kubernetes namespaces. Actions: login Login to Tanzu Mission Control. update Update the CLI to the latest version. Policies: iam Permission management for resources. policy Policy management for resources. Helpers: completion Generate completion scripts. configure Configure the current context environment system System level configuration and resources. version Print CLI version. Auto Completion If you like CLI completion like I do, you will be delighted that TMC CLI has it too. Just run tmc completion bash or zsh and add the script to your shell profile. Start with tmc login and follow the prompts. You will be asked for an API token which you have to generate via the VMware Cloud Services portal. tmc login If you don't have an API token, visit the VMware Cloud Services console, select your organization, and create an API token with the TMC service roles: https://console.cloud.vmware.com/csp/gateway/portal/#/user/tokens API Token Proxy In case you’re client is behind a proxy server, you have to add the following URL to your proxy allowlist. console.cloud.vmware.com Select the appropriate organization, and select a scope of All Roles. GENERATE token. Figure V: Tanzu Cloud Service Portal - API Tokens\" Figure V: Tanzu Cloud Service Portal - API Tokens Copy the token and paste it when you were prompted for. Figure VI: Tanzu Cloud Service Portal - Generated API Token\" Figure VI: Tanzu Cloud Service Portal - Generated API Token tmc login i If you don't have an API token, visit the VMware Cloud Services console, select your organization, and create an API token with the TMC service roles: https://console.cloud.vmware.com/csp/gateway/portal/#/user/tokens ? API Token **************************************************************** [...] After providing the requested API token, you have to give your login a context name. Choose a name you can easily remember! Otherwise, a random context name will be auto generated. [...] ? Login context name rguske-jarvis-lab ? Select default log level warning ? Management Cluster Name rguske-mgmt-mark50 ? Provisioner Name mark50-ns-1 √ Successfully created context rguske-jarvis-lab, to manage your contexts run `tmc system context -h` Now, let’s attach our first cluster to TMC/to our Cluster Group. For my purposes, I’m using a Tanzu Kubernetes cluster (TKC) which I’ve deployed in a declaritive way on vSphere (vSphere with Tanzu). If you are using a TKC as well, make sure you are logged in. k vsphere login --insecure-skip-tls-verify --vsphere-username administrator@mark50.lab --server=mark50.jarvis.tanzu --tanzu-kubernetes-cluster-name mark50-tkc-1 --tanzu-kubernetes-cluster-namespace mark50-ns-1 KUBECTL_VSPHERE_PASSWORD environment variable is not set. Please enter the password below Password: Logged i","date":"2022-01-08","objectID":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/:3:2","tags":["January2022","Tanzu","TanzuPackages","TanzuStandard","TMC","Kubernetes"],"title":"Deploying Tanzu Packages using Tanzu Mission Control Catalog","uri":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/"},{"categories":["VMware","Tanzu","Management","Kubernetes"],"content":"Install Catalog Package Cert-Manager I know, I know…only now I’m getting to the actual topic of this post 😄 but I thought it is useful to run a quick recap on the cluster attachment process first. You have made it this far, now continue 💪 At the end of this post, I’ll have three packages installed on my attached cluster. Cert-Manager to support numerous ways of generating certificates, Contour as the Kubernetes ingress controller and Harbor as our Enterprise Container Registry. Cert-Manager has always to be installed first, in order to e.g. have self-signed certificates generated for the installations as well as to be kind of an issuer of your own provided certs. If you are installing another package first, it will end up like shown in Figure VIII. Figure VIII: Failed Package installation due to missing cert-manager\" Figure VIII: Failed Package installation due to missing cert-manager Open the Catalog page and select the Cert-Manager package. The first page which will be shown, provides package related information about the maintainers, version as well as the support. On the top right corner, there’s a shiny INSTALL PACKAGE button which brings us to the actual installation steps. Figure IX: Cert-Manager Package Information\" Figure IX: Cert-Manager Package Information When it comes to Kubernetes specific object installations and configurations, like e.g. ServiceAccounts, Roles, Rolebindings, etc. TMC creates those uniquely for the automated installation for us as it is shown at Step 2 (Figure X). Figure X: Cert-Manager Package Installation\" Figure X: Cert-Manager Package Installation If you like to have the package installed in a custom namespace, create the namespace beforehand and select Custom.. Nothing more to add here for Cert-Manager (straight forward). INSTALL PACKAGE ","date":"2022-01-08","objectID":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/:3:3","tags":["January2022","Tanzu","TanzuPackages","TanzuStandard","TMC","Kubernetes"],"title":"Deploying Tanzu Packages using Tanzu Mission Control Catalog","uri":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/"},{"categories":["VMware","Tanzu","Management","Kubernetes"],"content":"Install Catalog Package Contour Exposing HTTP and HTTPS routes from outside your Kubernetes cluster to a service which is running inside your cluster is what an Ingress Controller does. Contour is a popular open source Ingress Controller which greatly supports your Layer-7 wishes and it’s available as a package in Catalog. I’m skipping the first two sections here and jump directly into section 3, which is all about Configuration values. Figure XI: Contour Package - Configuration Values\" Figure XI: Contour Package - Configuration Values Configuration types/values can be added in the following format, depending on the Key. string (sequence of characters) e.g. --log-level warning boolean (true/false) e.g. hostNetwork: false object e.g. pointing to a secret or configmap integer (integer value) e.g. replicas: 2 I oriented myself on the official Contour Configuration File example to create my config. These are my specified values: spec: containers: - args: - --log-level warning replicas: 2 envoy: service: type: LoadBalancer nodePorts: http: null https: null externalTrafficPolicy: Cluster disableWait: false hostPorts: enable: true http: 80 https: 443 hostNetwork: false Right after hitting the INSTALL PACKAGE button again, two new Namespaces were created. k get ns NAME STATUS AGE contour-ingress-14ece452 Active 60s default Active 45h kube-node-lease Active 45h kube-public Active 45h kube-system Active 45h tanzu-package-repo-global Active 21h tanzu-system Active 21h vmware-system-auth Active 45h vmware-system-cloud-provider Active 45h vmware-system-csi Active 45h vmware-system-tmc Active 3h23m The first namespace is contour-ingress-14ece452 and is it where the needed resources for the package installation itself are installed to. k -n contour-ingress-14ece452 get cm,sa NAME DATA AGE configmap/contour-ingress-ctrl 1 13m configmap/contour-ingress-ctrl-change-rpwwf 1 6m15s configmap/kube-root-ca.crt 1 13m NAME SECRETS AGE serviceaccount/contour-ingress 1 13m serviceaccount/default 1 13m The second namespace tanzu-system-ingress is the actual namespace for the package deployment (Pods, ReplicaSets, Deployments, etc.). k -n tanzu-system-ingress get cm,sa,po,deploy,svc NAME DATA AGE configmap/contour 1 95s configmap/kube-root-ca.crt 1 97s configmap/leader-elect 0 84s NAME SECRETS AGE serviceaccount/contour 1 95s serviceaccount/default 1 97s serviceaccount/envoy 1 95s NAME READY STATUS RESTARTS AGE pod/contour-6676bfcfd8-668k5 1/1 Running 0 93s pod/contour-6676bfcfd8-zbc5h 1/1 Running 0 93s pod/envoy-82phh 2/2 Running 0 95s pod/envoy-nrpj8 2/2 Running 0 94s pod/envoy-xqgp7 2/2 Running 0 94s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/contour 2/2 2 2 95s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/contour ClusterIP 195.49.186.113 \u003cnone\u003e 8001/TCP 95s service/envoy NodePort 195.63.98.167 \u003cnone\u003e 80:32242/TCP,443:32025/TCP 95s Here we go! Two ✅ Succeeded package installations. Figure XIII: Successful Installation of two Packages\" Figure XIII: Successful Installation of two Packages ","date":"2022-01-08","objectID":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/:4:0","tags":["January2022","Tanzu","TanzuPackages","TanzuStandard","TMC","Kubernetes"],"title":"Deploying Tanzu Packages using Tanzu Mission Control Catalog","uri":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/"},{"categories":["VMware","Tanzu","Management","Kubernetes"],"content":"Install Catalog Package Harbor Alrighty! Contour is ready for serving our kind: Service and kind: Ingress requests and this is what we need (L4) for our Harbor installation. As we already did for the Cert-Manager as well as for the Contour package, provide all necessary data in Step 1 and 2 and enter your specific values in Step 3. Figure XIV: Harbor Package - Configuration Values\" Figure XIV: Harbor Package - Configuration Values Like I mentioned in the Contour section, I was using the official data-values.yaml files for my configuration. For Harbor, I used the example here: Deploy Harbor Registry as a Shared Service This is how I ended up with: # Edit values here hostname: harbor.jarvis.tanzu port: https: 443 logLevel: warning database: password: VMware1! harborAdminPassword: VMware1! secretKey: M2blh8iIZIWL9Kvv core: secret: jML4MAuLIEgOFmq8 xsrfKey: ZcEYnGlUSBJvP7jrf9zKuZh4OFoNfIUQ jobservice: secret: 6TW3Ylpg8LpYjixp registry: secret: Ee48fjRlkMm4n2l6 trivy: enabled: true persistence: persistentVolumeClaim: registry: storageClass: \"ftt0-r0\" accessMode: ReadWriteOnce size: 10Gi jobservice: storageClass: \"ftt0-r0\" accessMode: ReadWriteOnce size: 1Gi database: storageClass: \"ftt0-r0\" accessMode: ReadWriteOnce size: 1Gi redis: storageClass: \"ftt0-r0\" accessMode: ReadWriteOnce size: 1Gi trivy: storageClass: \"ftt0-r0\" accessMode: ReadWriteOnce size: 5Gi Unfortunately, the notary-signer pod instantiation failed with CrashLoopBackOff. k -n tanzu-system-registry get pod NAME READY STATUS RESTARTS AGE harbor-core-f7ddbf886-bfmwk 1/1 Running 0 43m harbor-database-0 1/1 Running 0 43m harbor-jobservice-7b5857687c-ctlxq 1/1 Running 0 43m harbor-notary-server-677b7b4f9f-rnwmb 1/1 Running 1 43m harbor-notary-signer-8665f66696-q84ks 0/1 CrashLoopBackOff 13 43m harbor-portal-65c68794b9-6r7ws 1/1 Running 0 43m harbor-redis-0 1/1 Running 0 43m harbor-registry-74d4f6d4bc-p58pv 2/2 Running 0 43m harbor-trivy-0 1/1 Running 0 43 Fortunately, searching for the provided error message from the logs… {\"level\":\"info\",\"msg\":\"Version: 0.6.1, Git commit: d6e1431f\",\"time\":\"2022-02-10T05:50:44Z\"}{\"level\":\"fatal\",\"msg\":\"Could not read config at :/etc/notary/server-config.postgres.json, viper error: open : no such file or directory\",\"time\":\"2022-02-10T05:50:44Z\"} …led me directly to a known issue with a provided workaround on the VMware Knowledge Base - KB85725 🎥 Recording KB85725 Provide the right Namespace Make sure that you provide the Namespace where the Harbor package resources were installed to, which is not the tanzu-system-registry! In my case it’s named as harbor-b77eba03. After following the guidance, my Harbor instance was up and running 😃 kubectl -n tanzu-system-registry get pods NAME READY STATUS RESTARTS AGE harbor-core-f7ddbf886-bfmwk 1/1 Running 0 60m harbor-database-0 1/1 Running 0 60m harbor-jobservice-7b5857687c-ctlxq 1/1 Running 0 60m harbor-notary-server-677b7b4f9f-rnwmb 1/1 Running 1 60m harbor-notary-signer-5cb4fd8894-8cmzc 1/1 Running 0 2m36s harbor-portal-65c68794b9-6r7ws 1/1 Running 0 60m harbor-redis-0 1/1 Running 0 60m harbor-registry-74d4f6d4bc-p58pv 2/2 Running 0 60m harbor-trivy-0 1/1 Running 0 60m tanzu package installed list -A / Retrieving installed packages... NAME PACKAGE-NAME PACKAGE-VERSION STATUS NAMESPACE cert-manager cert-manager.tanzu.vmware.com 1.1.0+vmware.1-tkg.2 Reconcile succeeded cert-manager-4ea5f8cb contour-ingress contour.tanzu.vmware.com 1.17.1+vmware.1-tkg.1 Reconcile succeeded contour-ingress-e311d7d7 harbor harbor.tanzu.vmware.com 2.2.3+vmware.1-tkg.1 Reconcile succeeded harbor-b77eba03 Figure XV: Three successfully installed Packages\" Figure XV: Three successfully installed Packages Since the Harbor UI is exposed via the Envoy service load balancer that is running in the tanzu-system-ingress, it’s of high interest for us to get the assigned IP address. Execute kubectl get svc envoy -n tanzu-system-ingress -o jsonpath='{.status.loadBalancer.ingress[0]}' to let it be displayed. Use this IP address to cre","date":"2022-01-08","objectID":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/:5:0","tags":["January2022","Tanzu","TanzuPackages","TanzuStandard","TMC","Kubernetes"],"title":"Deploying Tanzu Packages using Tanzu Mission Control Catalog","uri":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/"},{"categories":["VMware","Tanzu","Management","Kubernetes"],"content":"Optional: Detach Kubernetes Cluster Detach a Cluster The detach process does not remove the catalog-related Kubernetes resources installed on your cluster, as that would impact any workloads you have deployed using the catalog. If you want to remove these resources, run the following kubectl commands: kubectl delete crd packageinstalls.packaging.carvel.dev kubectl delete crd packagerepositories.packaging.carvel.dev kubectl delete crd internalpackagemetadatas.internal.packaging.carvel.dev kubectl delete crd internalpackages.internal.packaging.carvel.dev kubectl delete crd apps.kappctrl.k14s.io kubectl delete APIService v1alpha1.data.packaging.carvel.dev kubectl delete ns tanzu-system kubectl delete ns tanzu-package-repo-global kubectl delete tanzupackage-install-admin-role kapp-controller-cluster-role kubectl delete clusterrolebinding kapp-controller-cluster-role-binding pkg-apiserver:system:auth-delegator kubectl delete psp tanzu-system-kapp-ctrl-restricted kubectl delete rolebinding pkgserver-auth-reader -n kube-system ","date":"2022-01-08","objectID":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/:5:1","tags":["January2022","Tanzu","TanzuPackages","TanzuStandard","TMC","Kubernetes"],"title":"Deploying Tanzu Packages using Tanzu Mission Control Catalog","uri":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/"},{"categories":["VMware","Tanzu","Management","Kubernetes"],"content":"Conclusion See “A Catalog of Extensions” right under my tweet 😃 ","date":"2022-01-08","objectID":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/:6:0","tags":["January2022","Tanzu","TanzuPackages","TanzuStandard","TMC","Kubernetes"],"title":"Deploying Tanzu Packages using Tanzu Mission Control Catalog","uri":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/"},{"categories":["VMware","Tanzu","Management","Kubernetes"],"content":"Resources: Resource Link Pathfinder - VMware Tanzu Standard Overview Click Tanzu - VMware Tanzu Mission Control Starter Click Blog - Tanzu Packages Explained Click Blog - Automatically Manage DNS for Kubernetes with ExternalDNS and Tanzu Mission Control Catalog Click Docs - Tanzu Mission Control v1alpha1 API Resource Click Docs - Install the Tanzu CLI and Other Tools Click Docs - TMC Install a Package Click Docs - Add a Package Repository to Your Cluster Click Docs - Tanzu Packages Harbor Installation Click Docs - harbor-data-values File for vSphere 7 Click Contour Docs - Contour Configuration Reference Click KB - The harbor-notary-signer pod fails to start in a Harbor installation under Tanzu Kubernetes Grid 1.4 (85725) Click ","date":"2022-01-08","objectID":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/:7:0","tags":["January2022","Tanzu","TanzuPackages","TanzuStandard","TMC","Kubernetes"],"title":"Deploying Tanzu Packages using Tanzu Mission Control Catalog","uri":"/post/deploying-tanzu-packages-using-tanzu-mission-control-catalog/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"A hands on guide which covers the deployment and configuration of the Avi Kubernetes Operator and to ultimately integrate it with the NSX Advanced Load Balancer to make use of the modern and comprehensive features of VMware's Next Gen Load Balancer.","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"VMUG UserCon 2021 It was this exiting time of the year again where lots of great VMware User Group events happened around the globe and this year, I had the pleasure to present two topics at the VMUG UserCon DACH (🇩🇪 🇦🇹 🇨🇭). One of the sessions had the title “NSX Advanced Load Balancer (Avi) - Next Gen Load Balancer for Next Gen (\u0026 Traditional) Apps”. This was my first session in which I teamed up for a presentation and I was very happy to have it done with no one else than James Lepthien, a luminary in networking. Next up @vmw_rguske and @0x86DD at DACH #UserCon about AVI Loadbalancer#NSX #Kubernetes pic.twitter.com/veccoK4Fkj — Michael Schroeder - Microlytix 🇪🇺 (@microlytix) July 20, 2021 James and I prepared two demos for our session and both rest on the Avi Kubernetes Operator - AKO. Our goal was to demonstrate L4 and L7 enterprise-grade app services of VMware’s software-defined NSX Advanced Load Balancer as well as the simplicity when it is used for Kubernetes based application deployments. ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:1:0","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"Recording (in German) ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:2:0","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"May I introduce - The Avi Kubernetes Operator The Avi Kubernetes Operator is this necessary entity which has to be installed on a Kubernetes cluster to translate the Kubernetes Objects to Avi objects and automates the implementation of L4 and L7 services on the Service Engines (SE) via the Avi Controller API. As the name suggests, the AKO is a Kubernetes Operator and runs as a stateless POD within the cluster. It’s onward Kubernetes 1.16 supported 1. ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:3:0","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"Deploy the AKO via Helm The deployment is made easy by using Helm (v3)2 for it. Of course it requires an already existing NSX Advanced Load Balancer deployment, which e.g. comes as a deployment option for the vSphere with Tanzu solution. I won’t cover the deployment of the Avi Controller in this post but be sure, lot’s of great content already exists which will serve you well. However, what I can do, is to recommend the usage of this awesome Fling: New Fling: Easy Deploy for NSX Load Balancing a.k.a EasyAvi. Presuming that you have an Avi Controller instance running and vSphere is configured as your Default Cloud, we will kick of the tires now. ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:4:0","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"1. Add the AKO repo To have the ability to deploy the AKO version of our choice (mostly the latest), we have to add the Helm repository first. helm repo add ako https://projects.registry.vmware.com/chartrepo/ako As of writing this post, version 1.4.2 is the latest available version. helm search repo ako/ako --versions NAME CHART VERSION APP VERSION DESCRIPTION ako/ako 1.4.2 1.4.2 A helm chart for Avi Kubernetes Operator ako/ako 1.4.1 1.4.1 A helm chart for Avi Kubernetes Operator ako/ako 1.3.4 1.3.4 A helm chart for Avi Kubernetes Operator ako/ako 1.3.3 1.3.3 A helm chart for Avi Kubernetes Operator ako/ako 1.3.1 1.3.1 A helm chart for Avi Kubernetes Operator ako/ako 1.2.3 1.2.3 A helm chart for Avi Kubernetes Operator ako/ako 1.2.2 1.2.2 A helm chart for Avi Kubernetes Operator ako/ako 1.2.1 1.2.1 A helm chart for Avi Kubernetes Operator ako/ako-operator 1.3.1 1.3.1 A Helm chart for Kubernetes AKO Operator ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:4:1","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"2. Deploy AKO on Kubernetes To get the AKO into the desired operating state, it is necessary to hand over the appropriate configuration parameters as a Kubernetes Configmap as well as Secret, to our Kubernetes Cluster. This can be done in two ways. Method 1: Using the values.yaml file as your configuration source of truth The command helm show values ako/ako --version 1.4.2 \u003e values.yaml will create the values.yaml file into the folder in which you are currently in (pwd). Open the file and adjust the parameters accordingly to your setup: Avi Controller ControllerSettings.controllerHost='' Avi Controller Version ControllerSettings.controllerVersion='' The Log Level (INFO, WARN, ERROR, DEBUG) AKOSettings.logLevel='' Username for the Avi Controller avicredentials.username='' Password for the User avicredentials.password='' Virtual IP network for the Virtual Service NetworkSettings.subnetIP='' Virtual IP network subnetmask NetworkSettings.subnetPrefix='' Virtual IP network name (dPG) NetworkSettings.networkName='' Name of your IaaS Cloud ControllerSettings.cloudName='' Name of the Service Engine Group ControllerSettings.serviceEngineGroupName='' Name of your Kubernetes Cluster AKOSettings.clusterName='' AKOSettings.cniPlugin='' Optional: Define the AKO function mode (default is ClusterIP) L7Settings.serviceType='' The description for the specific parameters/ tunables can be studied here: TUNABLES. Save your adjusted configuration for now and create the namespace for the AKO first. kubectl create ns avi-system Finally install the AKO: helm install ako/ako --generate-name --version 1.4.2 -f values.yaml --namespace=avi-system Method 2: Pass over all configuration parameters directly via helm This method is propably the way to go if you like to automate the AKO deployment. Just replace my used values with yours. helm install ako/ako \\ --generate-name \\ --version 1.4.2 \\ --set ControllerSettings.controllerHost='avi-ctrl.jarvis.lab' \\ --set ControllerSettings.controllerVersion='20.1.5' \\ --set AKOSettings.logLevel='WARNING' \\ --set avicredentials.username='admin' \\ --set avicredentials.password='VMware1!' \\ --set NetworkSettings.subnetIP='10.10.18.0' \\ --set NetworkSettings.subnetPrefix='26' \\ --set NetworkSettings.networkName='dpg-sedatapath-18' \\ --set ControllerSettings.cloudName='Default-Cloud' \\ --set ControllerSettings.serviceEngineGroupName='vmug-se-group' \\ --set AKOSettings.clusterName='veba-of' \\ --set L7Settings.serviceType='NodePort' \\ --namespace=avi-system ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:4:2","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"3. Validate the deployment Directly after firing up the helm install command, you should see: NAME: ako-1627467371 LAST DEPLOYED: Wed Jul 28 12:16:11 2021 NAMESPACE: avi-system STATUS: deployed REVISION: 1 Furthermore, you can check your helm deployments (all or specific ones) by using helm list and with appending the respective namespace. helm list -n avi-system NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ako-1627467371 avi-system 1 2021-07-28 12:16:11.301093764 +0200 CEST deployed ako-1.3.1 1.3.1 And numero tres - The validation with kubectl: kubectl -n avi-system get po,secret,configmap NAME READY STATUS RESTARTS AGE pod/ako-0 1/1 Running 0 10m NAME TYPE DATA AGE secret/ako-sa-token-cvjx5 kubernetes.io/service-account-token 3 10m secret/avi-secret Opaque 2 10m secret/default-token-5kx5g kubernetes.io/service-account-token 3 27d secret/sh.helm.release.v1.ako-1627481569.v1 helm.sh/release.v1 1 10m NAME DATA AGE configmap/avi-k8s-config 32 10m configmap/kube-root-ca.crt 1 27d Of course, it’s always helpful and enlightend to check the logs of the fresh deployed pod, to get first impressions if everything is working as expected. Pro Tip: Make use of cli tools which makes your life easier on the shell. I’m using k9s a lot to check my Kubernetes poppy’s as well as stern for a greater log view. K9s: Figure I: Kubernetes Cluster Management with K9s\" Figure I: Kubernetes Cluster Management with K9s Additionally: Uninstalling the AKO Uninstalling AKO is as simple as the installtion. Just use the full name of the helm deployment and execute it as follows: helm uninstall ako-1627467371 -n avi-system. ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:4:3","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"Configure a Subdomain Delegation in NSX-ALB ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:5:0","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"The NSX-ALB part - Virtual Service for DNS Before heading to my first demo example, it’s worth to stop by the setup and configuration of DNS Provider in Avi Vantage. This allows us to make Kubernetes Services (e.g. type: LoadBalancer) reachable via a resolvable DNS name by using a delegated subdomain configuration with a specified DNS Service Domain name which is or which are (in case of multiple) configured in the Avi DNS Profile. I followed the documentation to setup the DNS Service: DNS Provider (Avi Vantage) Parent topic: Avi DNS Architecture and Features Figure I, II and III gives you an idea on how it will look like by configuring it. DNS Profile Templates -\u003e Profiles -\u003e IPAM/DNS Profiles Create a new DNS Profile and configure the DNS Service Domain(s). Figure II: DNS Profile in Avi Vantage\" Figure II: DNS Profile in Avi Vantage Create a Virtual Service for DNS Applications -\u003e Virtual Services -\u003e Create Virtual Service Create a new DNS Virtual Service which will host FQDN records for your applications . Uncheck Auto Allocate Assign a static IP Address Set Application Profile to System-DNS Set Services to port 53 Figure III: Virtual Service for DNS\" Figure III: Virtual Service for DNS DNS A-Reord for this new Virtual Service Don’t forget to configure the DNS A-Record for the new created Virtual Service on your DNS Server! DNS Service in Avi Vantage Administration -\u003e Settings -\u003e DNS Service Finally, select the new DNS Virtual Service in your Avi Vantage system settings. Figure IV: Select the configured DNS Service\" Figure IV: Select the configured DNS Service Test if you can resolve the DNS name of the created VS: dig avi-dns.avi.jarvis.lab ; \u003c\u003c\u003e\u003e DiG 9.11.3-1ubuntu1.15-Ubuntu \u003c\u003c\u003e\u003e avi-dns.avi.jarvis.lab ;; global options: +cmd ;; Got answer: ;; -\u003e\u003eHEADER\u003c\u003c- opcode: QUERY, status: NOERROR, id: 54355 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 65494 ;; QUESTION SECTION: ;avi-dns.avi.jarvis.lab. IN A ;; ANSWER SECTION: avi-dns.avi.jarvis.lab. 29 IN A 10.10.18.50 ;; Query time: 1 msec ;; SERVER: 127.0.0.53#53(127.0.0.53) ;; WHEN: Wed Jul 28 15:16:32 CEST 2021 ;; MSG SIZE rcvd: 67 ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:5:1","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"The Windows Server part - New Delegation On the Windows Server (if you are using a Windows Server for your Domain Services) you have to add a New Delegation to your DNS configuration. It’s straight forward. Figure V: New Delegation\" Figure V: New Delegation Figure VI: Configured Delegation for DNS\" Figure VI: Configured Delegation for DNS Having done this, Avi Vantage DNS will be serving records for these particular domain(s). Next - Demo time 👨‍💻 ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:5:2","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"Layer 4 Load Balancing | type: LoadBalancer Our first demo was basic Kubernetes magic and covered the deployment of a web application which had a Kubernetes Service type: LoadBalancer configured. The deployment will request a VIP from the Avi Service Engine through the AKO, to make our app reachable via http(port: 80). The following slide was used to visualize and explain what we’ve demoed (start with Helm): Figure VII: Overview Demo 1 - Layer-4 Load Balancing\" Figure VII: Overview Demo 1 - Layer-4 Load Balancing The cool thing even though is, that we made use of a pretty nice feature from the NSX-ALB which is called Auto FQDN (L4Settings.autoFQDN). This feature is used to control how the layer 4 service of type Loadbalancer’s FQDN is generated. This tunable can also be configured via the value.yaml file or directly used as a --set configuration for the helm install CLI: [...]### This section outlines all the knobs used to control Layer 4 loadbalancing settings in AKO.L4Settings:autoFQDN: default # ENUM:default(\u003csvc\u003e.\u003cns\u003e.\u003csubdomain\u003e), flat (\u003csvc\u003e-\u003cns\u003e.\u003csubdomain\u003e), \"disabled\" If the value is disabled then the FQDN generation is disabled.[...] As you can read from the description, the generated FQDN will have the following structure by default: \u003csvc\u003e.\u003cns\u003e.\u003csubdomain\u003e. Let’s validate the generation of the FQDN by deploying the demo web application hackazon, which simulates an e-commerce shop. This application was deployed into a seperate namespace named vmug (kubectl create ns vmug). ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:6:0","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"Demo #1: Web App deployment with L4 Service Application specification: kind: Deployment + kind: Service kubectl -n vmug create -f - \u003c\u003cEOFkind:DeploymentapiVersion:apps/v1metadata:name:avitest-deploymentlabels:app:avitestspec:replicas:2selector:matchLabels:app:avitesttemplate:metadata:labels:app:avitestspec:containers:- name:avitestimage:ianwijaya/hackazonports:- name:httpcontainerPort:80protocol:TCP---kind:ServiceapiVersion:v1metadata:name:avisvc-lblabels:svc:avisvc-lbspec:ports:- name:httpport:80targetPort:80selector:app:avitesttype:LoadBalancerEOF kubectl validation of the deployment: kubectl -n vmug get deploy,po,svc NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/avitest-deployment 2/2 2 2 23m NAME READY STATUS RESTARTS AGE pod/avitest-deployment-66645bdb9b-nw9f2 1/1 Running 0 23m pod/avitest-deployment-66645bdb9b-r5c84 1/1 Running 0 23m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/avisvc-lb LoadBalancer 10.98.85.14 10.10.18.15 80:31851/TCP 23m The Controller Management GUI (Avi Vantage) is showing that the Virtual Service is up and provided by the Service Engine Group vmug-se-group which I’ve defined at the deployment of the AKO (--set ControllerSettings.serviceEngineGroupName='vmug-se-group'). Figure VIII: Overview and health state SE’s, vSphere DPG, VIP\" Figure VIII: Overview and health state SE’s, vSphere DPG, VIP Figure IX: Virtual Service Details\" Figure IX: Virtual Service Details Figure IX is showing that the autoFQDN feature generated an App Domain Name (remember \u003csvc\u003e.\u003cns\u003e.\u003csubdomain\u003e), which is avisvc-lb.vmug.avi.jarvis.lab, and on which our application is now reachable at via http. I would like to spare you another screenshot and use a curl output instead of: curl -I http://avisvc-lb.vmug.avi.jarvis.lab HTTP/1.1 200 OK Date: Wed, 28 Jul 2021 21:40:46 GMT Server: Apache/2.4.7 (Ubuntu) X-Powered-By: PHP/5.5.9-1ubuntu4.11 Set-Cookie: PHPSESSID=j174pd3706i2cm6ecemdv0g0k1; path=/ Expires: Thu, 19 Nov 1981 08:52:00 GMT Cache-Control: no-store, no-cache, must-revalidate, post-check=0, pre-check=0 Pragma: no-cache Content-Length: 64734 Content-Type: text/html; charset=utf-8 HTTP/1.1 200 OK The app is reachable via it’s App Domain Name. Onto Demo #2! ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:6:1","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"Layer 7 Load Balancing | type: Ingress In this second demo, James and I wanted to show the Layer-7 Ingress capabilities of the NSX-ALB and added the necessary kind: Ingress to our web app. Kubernetes Ingress Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource. Source: Kubernetes Documentation At Figure X, you see now the Ingress icon is showing up as well as the Ingress rule, which specifies the URL for our application. Figure X: Overview Demo 2 - Layer-7 Load Balancing | Ingress\" Figure X: Overview Demo 2 - Layer-7 Load Balancing | Ingress ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:7:0","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"AKO is also an Ingress Controller First and foremost! To leverage such functionalities, we need an Ingress Controller, don’t we?! We do! And this is another great value add of the Avi Kubernetes Operator. If not specifically disabled during the deployment, the AKO will serve your Kubernetes Cluster as an Ingress controller too. Here’s the according snipped from the values.yaml file: [...]### This section outlines all the knobs used to control Layer 7 loadbalancing settings in AKO.L7Settings:defaultIngController:'true'[...] Deploying the AKO will add the missing Kubernetes resource ingressclasses.networking.k8s.io to your cluster. kubectl get ingressclasses.networking.k8s.io NAME CONTROLLER PARAMETERS AGE avi-lb ako.vmware.com/avi-lb IngressParameters.ako.vmware.com/external-lb 32m In more detail: kubectl describe ingressclasses.networking.k8s.io avi-lb -n avi-system Name: avi-lb Labels: app.kubernetes.io/managed-by=Helm Annotations: ingressclass.kubernetes.io/is-default-class: true meta.helm.sh/release-name: ako-1627507213 meta.helm.sh/release-namespace: avi-system Controller: ako.vmware.com/avi-lb Parameters: APIGroup: ako.vmware.com Kind: IngressParameters Name: external-lb Events: \u003cnone\u003e ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:7:1","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"Demo #2: Web App deployment with L7 Service Now that we’ve set the scene right, let’s add the Ingress specification to our existing web app and apply the new configuration. Application specification: kind: Deployment + kind: Service + kind: Ingress: kubectl -n vmug apply -f - \u003c\u003cEOFkind:DeploymentapiVersion:apps/v1metadata:name:avitest-deploymentlabels:app:avitestspec:replicas:2selector:matchLabels:app:avitesttemplate:metadata:labels:app:avitestspec:containers:- name:avitestimage:ianwijaya/hackazonports:- name:httpcontainerPort:80protocol:TCP---kind:ServiceapiVersion:v1metadata:name:avisvc-lblabels:svc:avisvc-lbspec:ports:- name:httpport:80targetPort:80selector:app:avitesttype:LoadBalancer---apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:avisvcingresslabels:app:avisvcingressannotations:kubernetes.io/ingress.class:avispec:rules:- host:hackazon.avi.jarvis.labhttp:paths:- pathType:Prefixpath:/backend:service:name:avisvc-lbport:number:80EOF We should now see the new Ingress service available for our web app. kubectl -n vmug get po,svc,ingress NAME READY STATUS RESTARTS AGE pod/avitest-deployment-66645bdb9b-c52gv 1/1 Running 0 59s pod/avitest-deployment-66645bdb9b-st9z7 1/1 Running 0 59s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/avisvc-lb LoadBalancer 10.101.249.194 10.10.18.15 80:32678/TCP 59s NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/avisvcingress \u003cnone\u003e hackazon.avi.jarvis.lab 10.10.18.19 80 12h When we now have a look at the Avi Controller, we are noticing a complete new VIP was assigned for our Ingress configuration and another striking aspect is, that the NSX-ALB automatically creates a https service port for our app. To be clear, we’ve deployed an Insecure Ingress configuration and the NSX-ALB takes care of it for us (SSL Termination). Figure XI: New Virtual Service for Ingress | SSL Termination\" Figure XI: New Virtual Service for Ingress | SSL Termination Furthermore, if we are deploying another application with an Ingress config specified, the NSX-ALB will assign a complete new VIP to it and will not share the already existing one. Figure XII: New Virtual Service for Ingress | SSL Termination\" Figure XII: New Virtual Service for Ingress | SSL Termination You can read more about how the AKO handles Insecure Ingress and Secure Ingress at the repo - AKO Objects ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:7:2","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"Advanced Analytics in NSX-ALB With the configuration of the Ingress, our NSX-ALB is now fuelled with plenty of processable data for the Analytics Engine. I’m not an expert in this topic 🤷‍♂️ and therefore I’m making use of the following: VMware NSX-ALB Data Sheet The Avi SE represent full-featured, enterprise-grade load balancers, web application firewall (WAF), and analytics that provide traffic management and application security while collecting real-time analytics from the traffic flows. The Avi Controller provides comprehensive observability based on closed-loop telemetry and presents actionable insights to make decisions based on application monitoring, end-to-end timing, searchable traffic logs, security insights, log insights, client insights, and more. Source: VMware NSX Advanced Load Balancer (by Avi Networks) To really get an idea what kind of application related informations are available for the VS, I had to tick the boxes for Log all headers as well as Non-significant logs within the configuration of the VS. Figure XIII: Virtual Service - Analytics option\" Figure XIII: Virtual Service - Analytics option And here’s a glimpse into how extensively and beautiful (\u0026 of course useful) the data is prepared and presented: Figure XIV: Available data for my Web App\" Figure XIV: Available data for my Web App ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:8:0","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"Conclusion Deploying the Avi Kubernetes Operator on any Kubernetes Cluster is made simple and straight forward. The VMware NSX Advanced Load Balancer is enorm feature-rich and can perfectly serve your traditional as well as modern application demands. Features like the introduced Auto FQDN for Layer 4 services as well as the Ingress Capabilities are really just a short outlook what the AKO and NSX Advanced Load Balancer are capable of. I hope this post touched your points of interest and you are considering to put your hands on it as well. ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:9:0","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["VMware","Kubernetes","Open Source","Cloud Native"],"content":"Resources Avi on Github Frequently Asked Questions for AKO AKO - Networking/v1 Ingress Support Default Secret for TLS Ingresses Docs Avi Kubernetes OPerator VMware Hands-on Labs NSX Advanced Load Balancer Compatibility Guide for AKO ↩︎ AKO: Avi Kubernetes Operator - Pre-requisites ↩︎ ","date":"2021-07-30","objectID":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/:10:0","tags":["July2021","Helm","Kubernetes","Tanzu","NSX","Avi"],"title":"Hands-on the Avi Kubernetes Operator (AKO)","uri":"/post/hands-on-the-avi-kubernetes-operator-nsx-advanced-loadbalancer/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"The vSphere ESX Manager manages the life-cycle of agent VIBs and VMs. This post will give you an idea how it can help if one of your Supervisor Control Plane VMs stucks in state NotReady.","date":"2021-04-25","objectID":"/post/vsphere-with-tanzu-supervisor-control-plane-vm-stucks-in-state-not-ready/","tags":["April2021","Kubernetes","Tanzu","TKGs","vSphere"],"title":"vSphere with Tanzu - SupervisorControlPlaneVM stucks in state NotReady","uri":"/post/vsphere-with-tanzu-supervisor-control-plane-vm-stucks-in-state-not-ready/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Failed to get available workloads: bad gateway The last couple of weeks I spent a lot of time using my Tanzu Kubernetes Cluster(s)1 to get my head around as well as my hands dirty on this awesome project Knative2. More to come soon 😉 Interacting with a healthy vSphere Supervisor Cluster3 is necessary for e.g. the provisioning of new Tanzu Kubernetes Cluster or the instantiation of vSphere Native Pods. Unfortunately, my attempt to login into mine after a recent power outage ends quicker than expected with the error message: kubectl vsphere login --server=10.10.18.10 --vsphere-username administrator@jarvis.lab --insecure-skip-tls-verify Password: FATA[0004] Failed to get available workloads: bad gateway Please contact your vSphere server administrator for assistance. My first instinct was to quickly have a look at the Workload Management subsection in the vSphere Client (Menu –\u003e Workload Management or ctrl + alt + 7) and here my suspicion that something is wrong was confirmed. The Config Status of my cluster Malibu was in Configuring state and the following two Status Messages as shown in Figure I were displayed. Figure I: Supervisor Cluster stucks in configuring state\" Figure I: Supervisor Cluster stucks in configuring state I observed this Configuring state for a while longer and I was hoping it gets fixed automagically but it didn’t. Seriously, I do believe that this is really due to various (again) circumstances which I was facing with my homelab and normally the desired state (ready) for the Supervisor Cluster or more specifically, for the Supervisor Control Plane VMs, will be recovered automatically. ","date":"2021-04-25","objectID":"/post/vsphere-with-tanzu-supervisor-control-plane-vm-stucks-in-state-not-ready/:1:0","tags":["April2021","Kubernetes","Tanzu","TKGs","vSphere"],"title":"vSphere with Tanzu - SupervisorControlPlaneVM stucks in state NotReady","uri":"/post/vsphere-with-tanzu-supervisor-control-plane-vm-stucks-in-state-not-ready/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Into troubleshooting ","date":"2021-04-25","objectID":"/post/vsphere-with-tanzu-supervisor-control-plane-vm-stucks-in-state-not-ready/:2:0","tags":["April2021","Kubernetes","Tanzu","TKGs","vSphere"],"title":"vSphere with Tanzu - SupervisorControlPlaneVM stucks in state NotReady","uri":"/post/vsphere-with-tanzu-supervisor-control-plane-vm-stucks-in-state-not-ready/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"HealthState WCP service unhealthy The next move I did was checking the state of my Tanzu Kubernetes Cluster also via the vSphere Client but they were no longer displayed at all. Things went strange 👻. Consequently, I checked the overall health of my vCenter Server as well as the health state of the services and especially my attention was on the wcp service (Workload Control Plane). Checking the service state can be done in two ways: vCenter Appliance Management Interface aka VAMI (vcenter url:5480) via the shell - which requires an enabled and running ssh service on the vCenter Server Appliance The shell output was the following: root@vcsa [ ~ ]# vmon-cli --status wcp Name: wcp Starttype: AUTOMATIC RunState: STARTED RunAsUser: root CurrentRunStateDuration(ms): 72360341 HealthState: UNHEALTHY FailStop: N/A MainProcessId: 15679 HealthState: UNHEALTHY And here a little bit more coloured: Figure II: vCenter Server service wcp state unhealthy\" Figure II: vCenter Server service wcp state unhealthy Let’s see if the HealthState will change after restarting the service: root@vcsa [ ~ ]# vmon-cli --restart wcp Completed Restart service request. root@vcsa [ ~ ]# vmon-cli --status wcp Name: wcp Starttype: AUTOMATIC RunState: STARTED RunAsUser: root CurrentRunStateDuration(ms): 9709 HealthState: HEALTHY FailStop: N/A MainProcessId: 46317 HealthState: HEALTHY Well, way better. Let’s move on from here. ","date":"2021-04-25","objectID":"/post/vsphere-with-tanzu-supervisor-control-plane-vm-stucks-in-state-not-ready/:2:1","tags":["April2021","Kubernetes","Tanzu","TKGs","vSphere"],"title":"vSphere with Tanzu - SupervisorControlPlaneVM stucks in state NotReady","uri":"/post/vsphere-with-tanzu-supervisor-control-plane-vm-stucks-in-state-not-ready/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Supervisor Control Plane node status NotReady Having the wcp service back in operating state led me to start over from where I began. This time logging in to my Supervisor Cluster went well and I checked the state of the three Control Plane Nodes by executing kubectl get nodes. kubectl get nodes NAME STATUS ROLES AGE VERSION 42100ed03ae877fd39716f909d57822e NotReady master 7d19h v1.19.1+wcp.2 4210a2e2fa945c1a13308fac6d00ed96 Ready master 7d19h v1.19.1+wcp.2 4210c09deaaa5a2e2ddc86d3e0c0b0f3 Ready master 7d19h v1.19.1+wcp.2 Control Plane Node 1: STATUS NotReady With this new gains, I also checked the Virtual Machine state via the Remote Console and surprisingly, it seems that the power outages affected this particular Node (VM) badly. The Remote Console window was swarmed with error messages saying print_req_error: I/O error, dev sda, sector ...[counting up] Figure III: Control Plane VM status NotReady\" Figure III: Control Plane VM status NotReady I was also trying to figure out if there’s a way to get this fixed on the Operating System level but there was no chance or at least there wasn’t one for my expertise. ","date":"2021-04-25","objectID":"/post/vsphere-with-tanzu-supervisor-control-plane-vm-stucks-in-state-not-ready/:2:2","tags":["April2021","Kubernetes","Tanzu","TKGs","vSphere"],"title":"vSphere with Tanzu - SupervisorControlPlaneVM stucks in state NotReady","uri":"/post/vsphere-with-tanzu-supervisor-control-plane-vm-stucks-in-state-not-ready/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"vSphere ESXi Agent Manager If the desired state cannot be recovered automatically again, what option remains? Well, there’s always something to learn. My well appreciated colleague Dominik Zorgnotti was pointing me to the vSphere ESXi Agent Manager (EAM), which in the end turned out to be the solution for my problem. To be honest, I never was in the situation to make use of the EAM before and therefore it wasn’t on my radar at all but I was quite happy to get to know this component now. What it does? vSphere ESX Agent Manager The vSphere ESX Agent Manager automates the process of deploying and managing ESX and ESXi agents, which extend the function of the host to provide additional services to a vSphere solution. You will find the EAM in the vSphere Client under Menu -\u003e Administration -\u003e vCenter Server Extensions -\u003e vSphere ESXi Agent Manager. Figure IV: vSphere ESXi Agent Manager\" Figure IV: vSphere ESXi Agent Manager The three Supervisor Control Plane VMs can be found via the Configure tab and are listed in the column as Agency named with prefix vmware-vsc-apiserver-xxxxxx. By selecting the three dots besides the name, it gives us the two options Delete Agency as well as Remove All Issues. See Figure V. Figure V: vSphere ESXi Agent Manager - List of Agency’s\" Figure V: vSphere ESXi Agent Manager - List of Agency’s But which of the three “Agency’s” is our affected one? I’ve never recognized this used naming pattern before, not in the vSphere Client nor by using kubectl (e.g. kubectl describe node). Ultimately, I took a look at the summary page of the VM and the Notes widget enlightened me. This Virtual Machine is a VMware agent implementing support for vSphere Workloads. Its lifecycle operations are managed by VMware vCenter Server. EAM Agency: vmware-vsc-apiserver-w8mqd8 See also Figure VI: Figure VI: Agency name displayed on Notes widget\" Figure VI: Agency name displayed on Notes widget Having found the missing piece, I first went with the Remove All Issues option but it didn’t solve my problem. With having in mind, that the Supervisor Cluster is a high available construct consisting of three members and it’s current degraded state, I checked twice if the one I picked is the right one before hitting the Delete Agency 🔴 button. Immediately thereafter, the affected VM got deleted and a new one was on it’s way. Figure VII: Deployment of a new SupervisorControlPlaneVM\" Figure VII: Deployment of a new SupervisorControlPlaneVM At the end, my cluster reached the Config Status Running, all three Nodes were Ready and the wcp service never became (until now) unhealthy again. Figure VIII: Cluster config status Running again\" Figure VIII: Cluster config status Running again kubectl get nodes NAME STATUS ROLES AGE VERSION 42103deb5d43d5f75f4623a336165684 Ready master 3m37s v1.19.1+wcp.2 4210a2e2fa945c1a13308fac6d00ed96 Ready master 7d20h v1.19.1+wcp.2 4210c09deaaa5a2e2ddc86d3e0c0b0f3 Ready master 7d20h v1.19.1+wcp.2 What Is a Tanzu Kubernetes Cluster? ↩︎ Knative Homepage ↩︎ Configuring and Managing a Supervisor Cluster ↩︎ ","date":"2021-04-25","objectID":"/post/vsphere-with-tanzu-supervisor-control-plane-vm-stucks-in-state-not-ready/:3:0","tags":["April2021","Kubernetes","Tanzu","TKGs","vSphere"],"title":"vSphere with Tanzu - SupervisorControlPlaneVM stucks in state NotReady","uri":"/post/vsphere-with-tanzu-supervisor-control-plane-vm-stucks-in-state-not-ready/"},{"categories":["Workaround","Linux","Desktop","VMware"],"content":"I couldn't install the VMware ovftool on my Ubuntu 20.04 LTS client and because of this, I'm going to explain how you can run it without having it installed.","date":"2021-03-21","objectID":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/","tags":["March2021","Linux","Desktop","Workaround","VMware"],"title":"VMware ovftool installation was unsuccessful on Ubuntu 20.04 - A workaround","uri":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/"},{"categories":["Workaround","Linux","Desktop","VMware"],"content":"Introduction The VMware OVF Tool 1 is a powerful cli utility with which you can import and export Open Virtualization Format (OVF) packages to and from various VMware products. It’s e.g. used for the creation of several awesome VMware Fling projects like the Demo Appliance for Tanzu Kubernetes Grid 2, the VMware Appliance for Folding@Home 3 as well as for the VMware Event Broker Appliance 4. Also non-fling open source projects like e.g. the Netshoot Virtual Appliance 5 using the ovftool in their appliance build process. William Lam blogged about it several times in the past and you could use this as a great reference for the tool 6. I recently wanted to make use of the ovftool for the validation of a customer use-case and needed to install it on my “Universal Workbench” 7, my Ubuntu 20.04 vDesk (20.04.1-Ubuntu x86_64), which I am using for all my homelab work. As of writing this article, the latest available version for the ovftool is version 4.4.1 8. ","date":"2021-03-21","objectID":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/:1:0","tags":["March2021","Linux","Desktop","Workaround","VMware"],"title":"VMware ovftool installation was unsuccessful on Ubuntu 20.04 - A workaround","uri":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/"},{"categories":["Workaround","Linux","Desktop","VMware"],"content":"Ovftool installation options I downloaded the appropriate 64 bit version for my Linux system, which is the VMware-ovftool-4.4.1-16812187-lin.x86_64.bundle file and made it executable for the installation: $ chmod +755 VMware-ovftool-4.4.1-16812187-lin.x86_64.bundle The ovftool installation file is a bundled package (.bundle) which contains the ovftool as well as the vmware-installer, plus the associated files for those. The installation can be executed with various options and by running e.g. ./VMware-ovftool-4.4.1-16812187-lin.x86_64.bundle --help, the vmware-installer will be temporarily extracted and provides these options. The following listed are an abstract of the ones which are relevant for this post and which I’ve used for my installation attempts. Options: --gtk Use the Gtk+ UI (Default) --console Use the console UI --required Displays only questions absolutely required --eulas-agreed Agree to the EULA --extract=DIR Extract the contents of the bundle into DIR ","date":"2021-03-21","objectID":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/:2:0","tags":["March2021","Linux","Desktop","Workaround","VMware"],"title":"VMware ovftool installation was unsuccessful on Ubuntu 20.04 - A workaround","uri":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/"},{"categories":["Workaround","Linux","Desktop","VMware"],"content":"Installation was unsuccessful On Linux, you can choose either to use the --gtk UI (default) or the --console UI option. My first attempt, and I used this several times before, was with the --console option. In a Terminal window I executed sudo ./VMware-ovftool-4.4.1-16812187-lin.x86_64.bundle --console --required --eulas-agreed and got the following output as a failed result: Extracting VMware Installer...done. Traceback (most recent call last): File \"/tmp/tmpt0pd6m09.vmis.env\", line 132, in \u003cmodule\u003e exec(fileObj.read(), mod.__dict__) File \"\u003cstring\u003e\", line 243 except OSError, e: ^ SyntaxError: invalid syntax Exception info : [Component did not register an installer]. Check the log /var/log/vmware-installer for details. [######################################################################] 100% Installation was unsuccessful. Installation was unsuccessful I couldn’t really figure out the output and went straight for the --gtk option as my 2nd attempt: $ sudo ./VMware-ovftool-4.4.1-16812187-lin.x86_64.bundle --gtk --required --eulas-agreed Extracting VMware Installer...done. (vmware-installer.py:233744): Gtk-WARNING **: Unable to locate theme engine in module_path: \"adwaita\", (vmware-installer.py:233744): Gtk-WARNING **: Unable to locate theme engine in module_path: \"adwaita\", Gtk-Message: Failed to load module \"canberra-gtk-module\" Traceback (most recent call last): File \"/tmp/tmp1nqoz5ld.vmis.env\", line 132, in \u003cmodule\u003e exec(fileObj.read(), mod.__dict__) File \"\u003cstring\u003e\", line 243 except OSError, e: ^ SyntaxError: invalid syntax Exception info : [Component did not register an installer]. Check the log /var/log/vmware-installer for details. [######################################################################] 100% Installation was unsuccessful. Gtk-WARNING: Unable to locate theme engine in module_path: adwaita Gtk-Message: Failed to load module canberra-gtk-module Some research on both failure indications pointed me to presumable missing packages related to the GTK graphical user interface library in Ubuntu and therefore I followed some of the described hints which were for example: sudo apt install gnome-themes-standard9 sudo apt install canberra-gtk-module10 but non of those solved the problem nor changed the failure message. Also the /var/log/vmware-installer log couldn’t really help (at least to me). ","date":"2021-03-21","objectID":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/:3:0","tags":["March2021","Linux","Desktop","Workaround","VMware"],"title":"VMware ovftool installation was unsuccessful on Ubuntu 20.04 - A workaround","uri":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/"},{"categories":["Workaround","Linux","Desktop","VMware"],"content":"Output /var/log/vmware-installer $ less /var/log/vmware-installer [2021-02-25 11:28:13,465] code for hash sha256 was not found. Traceback (most recent call last): File \"/tmp/vmis.Lu1nSs/install/vmware-installer/python/lib/hashlib.py\", line 147, in \u003cmodule\u003e globals()[__func_name] = __get_hash(__func_name) File \"/tmp/vmis.Lu1nSs/install/vmware-installer/python/lib/hashlib.py\", line 97, in __get_builtin_constructor raise ValueError('unsupported hash type ' + name) ValueError: unsupported hash type sha256 [2021-02-25 11:28:13,465] code for hash sha384 was not found. Traceback (most recent call last): File \"/tmp/vmis.Lu1nSs/install/vmware-installer/python/lib/hashlib.py\", line 147, in \u003cmodule\u003e globals()[__func_name] = __get_hash(__func_name) File \"/tmp/vmis.Lu1nSs/install/vmware-installer/python/lib/hashlib.py\", line 97, in __get_builtin_constructor raise ValueError('unsupported hash type ' + name) ValueError: unsupported hash type sha384 [2021-02-25 11:28:13,465] code for hash sha512 was not found. Traceback (most recent call last): File \"/tmp/vmis.Lu1nSs/install/vmware-installer/python/lib/hashlib.py\", line 147, in \u003cmodule\u003e globals()[__func_name] = __get_hash(__func_name) File \"/tmp/vmis.Lu1nSs/install/vmware-installer/python/lib/hashlib.py\", line 97, in __get_builtin_constructor raise ValueError('unsupported hash type ' + name) ValueError: unsupported hash type sha512 [2021-02-25 11:28:13,494] [2021-02-25 11:28:13,494] [2021-02-25 11:28:13,494] Installer running. [2021-02-25 11:28:13,494] Command Line Arguments: [2021-02-25 11:28:13,494] ['/tmp/vmis.Lu1nSs/install/vmware-installer/vmware-installer.py', '--set-setting', 'vmware-installer', 'libconf', '', '--install-component', '/tmp/vmis.Lu1nSs/install/vmware-installer', '--install-bundle', '/home/rguske/Downloads/./VMware-ovftool-4.4.0-16360108-lin.x86_64.bundle', ''] [2021-02-25 11:28:13,740] Successfully loaded GTK libraries. [2021-02-25 11:28:13,789] Using UI type gtk [2021-02-25 11:28:13,795] System installer version is: 3.0.0.17287072 [2021-02-25 11:28:13,795] Running installer version is: 2.1.0.14928104 [2021-02-25 11:28:13,795] Running newer system installer. [2021-02-25 11:28:14,151] [2021-02-25 11:28:14,151] [2021-02-25 11:28:14,151] Installer running. [2021-02-25 11:28:14,151] Command Line Arguments: [2021-02-25 11:28:14,151] ['/usr/lib/vmware-installer/3.0.0/vmware-installer.py', '--set-setting', 'vmware-installer', 'libconf', '', '--install-component', '/tmp/vmis.Lu1nSs/install/vmware-installer', '--install-bundle', '/home/rguske/Downloads/./VMware-ovftool-4.4.0-16360108-lin.x86_64.bundle', ''] [2021-02-25 11:28:14,151] Failed to load GTK libraries, falling back to console. [2021-02-25 11:28:14,157] Using UI type console [2021-02-25 11:28:14,158] System installer version is: 3.0.0.17287072 [2021-02-25 11:28:14,158] Running installer version is: 3.0.0.17287072 [2021-02-25 11:28:14,158] Opening database file /etc/vmware-installer/database [2021-02-25 11:28:14,236] Could not locate installer App Control. [2021-02-25 11:28:14,247] Error: Cannot load installer for component: vmware-installer. [2021-02-25 11:28:14,248] Top level exception handler Traceback (most recent call last): File \"/usr/lib/vmware-installer/3.0.0/vmis/core/transaction.py\", line 470, in RunThreadedTransaction txn.Execute(actions) File \"/usr/lib/vmware-installer/3.0.0/vmis/core/transaction.py\", line 252, in Execute i.Load(self.temp) File \"/usr/lib/vmware-installer/3.0.0/vmis/core/install.py\", line 325, in Load self._module, self._installer = LoadInstaller(self.component, File \"/usr/lib/vmware-installer/3.0.0/vmis/core/env.py\", line 406, in LoadInstaller raise ComponentError('Component did not register an installer', component) vmis.core.errors.ComponentError: Component did not register an installer ","date":"2021-03-21","objectID":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/:3:1","tags":["March2021","Linux","Desktop","Workaround","VMware"],"title":"VMware ovftool installation was unsuccessful on Ubuntu 20.04 - A workaround","uri":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/"},{"categories":["Workaround","Linux","Desktop","VMware"],"content":"The workaround If we cannot reach our goal via the conventional way, we have to find a detour, a workaround to get there. For this particular issue with the failed installation on Ubuntu 20.04, it is the extraction of the ovftool files from the installation .bundle, copying them to /usr/bin/ and configure an alias for the ovftool executable. ","date":"2021-03-21","objectID":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/:4:0","tags":["March2021","Linux","Desktop","Workaround","VMware"],"title":"VMware ovftool installation was unsuccessful on Ubuntu 20.04 - A workaround","uri":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/"},{"categories":["Workaround","Linux","Desktop","VMware"],"content":"Execute ovftool without installing it This is how it works step-by-step: Extract the files and change into directory ovftool (you can name the extracted directory as you want): sudo ./VMware-ovftool-4.4.1-16812187-lin.x86_64.bundle --extract ovftool \u0026\u0026 cd ovftool ls -rtl total 16K drwxr-xr-x 4 root root 4,0K Mär 9 14:07 . drwxr-xr-x 5 rguske domain^users 4,0K Mär 9 14:07 .. drwxr-xr-x 9 root root 4,0K Mär 9 14:07 vmware-installer drwxr-xr-x 6 root root 4,0K Mär 9 14:07 vmware-ovftool Here’s what’s in it: $ tree -L 2 . ├── vmware-installer │ ├── artwork │ ├── bin │ ├── bootstrap │ ├── lib │ ├── manifest.xml │ ├── python │ ├── sopython │ ├── vmis │ ├── vmis-launcher │ ├── vmware-installer │ ├── vmware-installer.py │ ├── vmware-uninstall │ └── vmware-uninstall-downgrade └── vmware-ovftool ├── certs ├── env ├── icudt44l.dat ├── libcares.so.2 ├── libcrypto.so.1.0.2 ├── libcurl.so.4 ├── libexpat.so ├── libgcc_s.so.1 ├── libgoogleurl.so.59 ├── libicudata.so.60 ├── libicuuc.so.60 ├── libssl.so.1.0.2 ├── libssoclient.so ├── libstdc++.so.6 ├── libvim-types.so ├── libvmacore.so ├── libvmomi.so ├── libxerces-c-3.2.so ├── libz.so.1 ├── manifest.xml ├── open_source_licenses.txt ├── ovftool ├── ovftool.bin ├── README.txt ├── schemas ├── vmware.eula └── vmware-eula.rtf 11 directories, 31 files Move the vmware-ovftool directory to /usr/bin/: sudo mv vmware-ovftool /usr/bin/ Make the two files ovftool as well as ovftool.bin executable: sudo chmod +x /usr/bin/vmware-ovftool/ovftool ovftool.bin Configure an alias for your used shell to execute it as usual: alias ovftool=/usr/bin/vmware-ovftool/ovftool I am using zsh 11 as the shellof my choice and I created a dedicated file for all my aliases - it’s named ~/.zsh_aliases. $ which ovftool ovftool: aliased to /usr/bin/vmware-ovftool/ovftool This allows me to run the ovftool as usual. ","date":"2021-03-21","objectID":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/:4:1","tags":["March2021","Linux","Desktop","Workaround","VMware"],"title":"VMware ovftool installation was unsuccessful on Ubuntu 20.04 - A workaround","uri":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/"},{"categories":["Workaround","Linux","Desktop","VMware"],"content":"Conclusion Ovftool installation was unsuccessful and issued the following errors: Unable to locate theme engine in module_path: \"adwaita\" and Failed to load module \"canberra-gtk-module Installation of Linux packages gnome-themes-standard and canberra-gtk-module didn’t solve anything Extracted the ovftool binaries from the installation .bundle package by using vmware-installer option --extract Moved extracted folder vmware-ovftool to /usr/bin/ Made files ovftool and ovftool.bin executable configured an alias for our shell to use it as usual VMware ovftool {code} page ↩︎ Demo Appliance for Tanzu Kubernetes Grid ↩︎ VMware Appliance for Folding@Home ↩︎ VMware Event Broker Appliance ↩︎ Netshoot Virtual Appliance ↩︎ virtuallyGhetto - OVF / OVFTOOL ↩︎ A Linux Development Desktop with VMware Horizon - Part I: Horizon ↩︎ Download OVFTOOL 4.4.1 ↩︎ Package gnome-themes-standard ↩︎ Package libcanberra-gtk-module ↩︎ A Linux Development Desktop with VMware Horizon - Part III: Shell ↩︎ ","date":"2021-03-21","objectID":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/:5:0","tags":["March2021","Linux","Desktop","Workaround","VMware"],"title":"VMware ovftool installation was unsuccessful on Ubuntu 20.04 - A workaround","uri":"/post/vmware-ovftool-installation-was-unsuccessful-on-ubuntu-20/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"VMware Fusion can instantiate KinD clusters locally on your desktop (project Nautilus) but recently I discovered some restrictions which I'm explaining in this post.","date":"2021-03-02","objectID":"/post/vmware-vctl-kind-writing-configuration-failed/","tags":["March2021","Fusion","Workstation","Kubernetes","Desktop"],"title":"VMware vctl KinD - Writing configuration failed","uri":"/post/vmware-vctl-kind-writing-configuration-failed/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"Introduction Quote “Everything fails all the time” – Werner Vogels, Amazon CTO ^^But why always at the most inappropriate moments?! Just recently it happened to me, that one day before I needed my homelab for a presentation, the vSAN caching disk failed and had to be replaced. Nooooooooooo 😩👹 pic.twitter.com/GHELTrPeTn — Robert Guske (@vmw_rguske) February 23, 2021 Normally there is no real issue with loosing a caching device in a vSAN cluster but let me put it that way, I’m really using all my given 2-node cluster resources and a potential risk of data-loss was calculated. Not everybody can have a “homelab” like the Homelab King1, right? 😉 Unfortunately, some of my Tanzu Kubernetes Grid clusters2 suffered from the outage and was the one providing my Harbor3 registry service which I needed for my demo. However, there are other ways to help yourself when your homelab is in “maintenance” and I wanted to leverage VMware Fusion vctl4 to quickly instantiate a KinD5 cluster locally on my machine, to install Harbor via a Helm chart6 afterwards on it. For the purposes of my presentation this would have been totally sufficient. But then it turned out that I have to find an alternative again 🤦. ","date":"2021-03-02","objectID":"/post/vmware-vctl-kind-writing-configuration-failed/:1:0","tags":["March2021","Fusion","Workstation","Kubernetes","Desktop"],"title":"VMware vctl KinD - Writing configuration failed","uri":"/post/vmware-vctl-kind-writing-configuration-failed/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"Adjusting the CRX VM with appropriate resources By default vctl assigns 2 GB of memory and 2 CPU cores for the CRX VM that hosts the Kubernetes node container. For my Harbor 1-node deployment I sticked to the minimum requirements7 (2 CPUs, 4 GB Mem) for the Kubernetes node but also decided to give the CRX VM a little bit more, to keep the option open to run a Multo-Node deployment as well. Therefore, I applied the following configuration: vctl system config --vm-cpus 4 --vm-mem 8192 --k8s-cpus 2 --k8s-mem 4096 which will adjust the config.yaml in ~/.vctl/ accordingly. cache-location:/Users/rguske/.vctlk8s-cpus:2k8s-mem:4096log-level:infolog-location:/Users/rguske/.vctl/containerd.logmount-name:Fusion Container Storagepid:45414storage:128gvm-cpus:4vm-mem:8192vmnet:vmnet8 ","date":"2021-03-02","objectID":"/post/vmware-vctl-kind-writing-configuration-failed/:2:0","tags":["March2021","Fusion","Workstation","Kubernetes","Desktop"],"title":"VMware vctl KinD - Writing configuration failed","uri":"/post/vmware-vctl-kind-writing-configuration-failed/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"KinD - Writing configuration failed Moving forward to the KinD cluster creation itself. I first looked up for the latest kindest/node image version I can choose. Figure I: kindest/node available images\" Figure I: kindest/node available images You can see at Figure I that v1.19.7 is at the top (sort by: Newest) and so my choice was taken. kind create cluster --image kindest/node:v1.19.7 --name harbor Creating cluster \"harbor\" ... ✓ Ensuring node image (kindest/node:v1.19.7) 🖼 ✓ Preparing nodes 📦 ✗ Writing configuration 📜 ERROR: failed to create cluster: failed to generate kubeadm config content: failed to get kubernetes version from node: failed to get file: command \"docker exec --privileged harbor-control-plane cat /kind/version\" failed with error: exit status 1 Command Output: level=error msg=\"Error starting process: container = harbor-control-plane, command = [cat /kind/version] env = [PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin TERM=xterm PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin container=docker TERM=xterm] error = container_linux.go:345: starting container process caused \\\"read init-p: connection reset by peer\\\": unknown\" What is this error msg telling us? Let’s activate debug mode by adding option -v 1 to the command. kind create cluster --image kindest/node:v1.19.7 --name harbor -v 1 Creating cluster \"harbor\" ... DEBUG: docker/images.go:58] Image: kindest/node:v1.19.7 present locally ✓ Ensuring node image (kindest/node:v1.19.7) 🖼 ✓ Preparing nodes 📦 ✗ Writing configuration 📜 ERROR: failed to create cluster: failed to generate kubeadm config content: failed to get kubernetes version from node: failed to get file: command \"docker exec --privileged harbor-control-plane cat /kind/version\" failed with error: exit status 1 Command Output: level=error msg=\"Error starting process: container = harbor-control-plane, command = [cat /kind/version] env = [PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin TERM=xterm PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin container=docker TERM=xterm] error = container_linux.go:345: starting container process caused \\\"read init-p: connection reset by peer\\\": unknown\" Stack Trace: sigs.k8s.io/kind/pkg/errors.WithStack sigs.k8s.io/kind/pkg/errors/errors.go:51 sigs.k8s.io/kind/pkg/exec.(*LocalCmd).Run sigs.k8s.io/kind/pkg/exec/local.go:124 sigs.k8s.io/kind/pkg/cluster/internal/providers/docker.(*nodeCmd).Run sigs.k8s.io/kind/pkg/cluster/internal/providers/docker/node.go:146 sigs.k8s.io/kind/pkg/exec.CombinedOutputLines sigs.k8s.io/kind/pkg/exec/helpers.go:67 sigs.k8s.io/kind/pkg/cluster/nodeutils.KubeVersion sigs.k8s.io/kind/pkg/cluster/nodeutils/util.go:35 sigs.k8s.io/kind/pkg/cluster/internal/create/actions/config.getKubeadmConfig sigs.k8s.io/kind/pkg/cluster/internal/create/actions/config/config.go:170 sigs.k8s.io/kind/pkg/cluster/internal/create/actions/config.(*Action).Execute.func1.1 sigs.k8s.io/kind/pkg/cluster/internal/create/actions/config/config.go:82 sigs.k8s.io/kind/pkg/errors.UntilErrorConcurrent.func1 sigs.k8s.io/kind/pkg/errors/concurrent.go:30 runtime.goexit runtime/asm_amd64.s:1374 I have to admit that I couldn’t really use the output for further debugging and also searching the web helped me neither. Consequently, I reached out internally and got the hint that v1.20.0 should work. v1.20.0 ❓ ❗ I haven’t seen this version on the list of available images (Figure I) and so I went back to Docker Hub and searched for this particular version. Okay, it exists and besides it a v1.20.2. as well. I learned Newest isn’t related to Tags, what IMO would make sense, but it’s related to last pushed. Figure II: hidden kindest image version 1.20.0\" Figure II: hidden kindest image version 1.20.0 Give it a another try and this time with v1.20.0. kind create cluster --image kindest/node:v1.20.0 --name harbor Creating cluster \"harbor\" ... ✓ Ensuring node image (kindest/node:v1.20.0) 🖼 ✓ Preparing nodes 📦 ✓ Writing configu","date":"2021-03-02","objectID":"/post/vmware-vctl-kind-writing-configuration-failed/:3:0","tags":["March2021","Fusion","Workstation","Kubernetes","Desktop"],"title":"VMware vctl KinD - Writing configuration failed","uri":"/post/vmware-vctl-kind-writing-configuration-failed/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"My Test Results kindest/node Version Result 1.20.2 ❌ 1.20.0 ✅ 1.19.7 ❌ 1.19.1 ✅ 1.18.8 ✅ All my tests were made with the following versions: OS, App, CLI, Runtime Version macOS Big Sur 11.2.1 (20D75) Fusion 12.1.0 (17195230) vctl 1.1.1 containerd github.com/containerd/containerd v1.3.2-vmw ","date":"2021-03-02","objectID":"/post/vmware-vctl-kind-writing-configuration-failed/:3:1","tags":["March2021","Fusion","Workstation","Kubernetes","Desktop"],"title":"VMware vctl KinD - Writing configuration failed","uri":"/post/vmware-vctl-kind-writing-configuration-failed/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"Heads up! No solution yet I don’t have a solution for this issue yet but I will keep this post updated. For the time being, I recommend using a version which passed my test. ","date":"2021-03-02","objectID":"/post/vmware-vctl-kind-writing-configuration-failed/:4:0","tags":["March2021","Fusion","Workstation","Kubernetes","Desktop"],"title":"VMware vctl KinD - Writing configuration failed","uri":"/post/vmware-vctl-kind-writing-configuration-failed/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"Bonus: Multi-Node deployment I have mentioned at the beginning, that I have assigned enough resources to the CRX VM to run a multi-node KinD deployment and I’d like to show you now how to instantiate this by using the --config option. Just create a file in a directory of your choice and add the following description to it: Example: vim ~/.kube/kind_worker # two node (one ctrl plane \u0026 one worker node) cluster config kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane - role: worker - role: worker kind create cluster --image kindest/node:v1.20.0 --name kind-cluster-harbor --config ~/.kube/kind_worker Creating cluster \"kind-cluster-harbor\" ... ✓ Ensuring node image (kindest/node:v1.20.0) 🖼 ✗ Preparing nodes 📦 📦 ERROR: failed to create cluster: docker run error: command \"docker run --hostname kind-cluster-harbor-control-plane --name kind-cluster-harbor-control-plane --label io.x-k8s.kind.role=control-plane --privileged --security-opt seccomp=unconfined --security-opt apparmor=unconfined --tmpfs /tmp --tmpfs /run --volume /var --volume /lib/modules:/lib/modules:ro --detach --tty --label io.x-k8s.kind.cluster=kind-cluster-harbor --net kind --restart=on-failure:1 --publish=127.0.0.1:61721:6443/TCP kindest/node:v1.20.0\" failed with error: exit status 1 Command Output: level=error msg=\"failed to create container: container name (kind-cluster-harbor-control-plane) length is greater than the maximum allowed length (30)\" container name length exeeded Command Output: level=error msg=“failed to create container: container name (kind-cluster-harbor-control-plane) length is greater than the maximum allowed length (30)” Ooookay 🙄 the given cluster name is too long. Shortened: kind create cluster --image kindest/node:v1.20.0 --name kind-harbor --config ~/.kube/kind_worker Creating cluster \"kind-harbor\" ... ✓ Ensuring node image (kindest/node:v1.20.0) 🖼 ✓ Preparing nodes 📦 📦 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜 Set kubectl context to \"kind-kind-harbor\" You can now use your cluster with: kubectl cluster-info --context kind-kind-harbor Thanks for using kind! 😊 kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kind-harbor-control-plane Ready control-plane,master 67s v1.20.0 192.168.43.6 \u003cnone\u003e Ubuntu Groovy Gorilla (development branch) 4.19.138-7.ph3-esx containerd://1.4.0 kind-harbor-worker Ready \u003cnone\u003e 39s v1.20.0 192.168.43.5 \u003cnone\u003e Ubuntu Groovy Gorilla (development branch) 4.19.138-7.ph3-esx containerd://1.4.0 kind-harbor-worker2 Ready \u003cnone\u003e 39s v1.20.0 192.168.43.7 \u003cnone\u003e Ubuntu Groovy Gorilla (development branch) 4.19.138-7.ph3-esx containerd://1.4.0 Wohoo, a multi-node Kubernetes cluster running locally on my desktop. ","date":"2021-03-02","objectID":"/post/vmware-vctl-kind-writing-configuration-failed/:5:0","tags":["March2021","Fusion","Workstation","Kubernetes","Desktop"],"title":"VMware vctl KinD - Writing configuration failed","uri":"/post/vmware-vctl-kind-writing-configuration-failed/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"Commands I used These are the commands I used in this post: vctl version vctl system config --vm-cpus 4 --vm-mem 8192 --k8s-cpus 2 --k8s-mem 4096 kind create cluster --image kindest/node:v1.20.0 --name kind-cluster kind create cluster --image kindest/node:v1.20.0 --name kind-cluster -v 1 vim ~/.kube/kind_worker kind create cluster --image kindest/node:v1.20.0 --name kind-harbor --config ~/.kube/kind_worker Homelab King Marc Huppert - HomeLab Actual State ↩︎ Provisioning Tanzu Kubernetes Clusters ↩︎ Harbor Cloud Native Registry ↩︎ A closer look at VMware’s Project Nautilus ↩︎ KinD - Docker in Kubernetes ↩︎ vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid ↩︎ Harbor Installation Prerequisites ↩︎ ","date":"2021-03-02","objectID":"/post/vmware-vctl-kind-writing-configuration-failed/:6:0","tags":["March2021","Fusion","Workstation","Kubernetes","Desktop"],"title":"VMware vctl KinD - Writing configuration failed","uri":"/post/vmware-vctl-kind-writing-configuration-failed/"},{"categories":["VEBA","VMware","Kubernetes","Logging","Open Source","Cloud Native"],"content":"A guide how to leverage the open source project FluentBit to forward logs from a Tanzu Kubernetes Cluster to vRealize Log Insight.","date":"2021-01-13","objectID":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/","tags":["FluentBit","vRLI","Kubernetes","VEBA","Tanzu","TKG","OSS"],"title":"Leveraging Fluent Bit on Tanzu Kubernetes Grid to send VEBA logs to vRealize Log Insight","uri":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/"},{"categories":["VEBA","VMware","Kubernetes","Logging","Open Source","Cloud Native"],"content":"Introduction With the recent v0.5 release of the VMware Event Broker Appliance (VEBA) project, the ability of deploying the core component, the VMware Event Router, via a Helm chart to an existing Kubernetes cluster was introduced. William Lam introduced this and all the other great new features and enhancements in his corresponding blog post. This earned very positive feedback from the community and opens even more doors in terms of flexibility as well as extensibility. Speaking of which, extensibility… During the v0.5 pre-release team meeting, William came up with the idea to let VEBA sending logs to our log analysis solutions VMware vRealize Log Insight or vRealize Log Insight Cloud. He asked me if I would do it and of course I found it a great idea. This post is a good addition to my previous post “Monitoring the VMware Event Broker Appliance with vRealize Operations Manager”1, in which I’m describing how I used cAdvisor2 to get resource usage and performance data from the various VEBA components and ultimately to monitor them. At this point, I have to be clear that I didn’t used the VEBA appliance this time. Instead, I used Helm charts to deploy every component like e.g. the mentioned Event Router or the Event Processor to my Tanzu Kubernetes Grid - TKGs cluster (also referred to as Guestcluster) running on VMware vSphere. However, I’m considering to write a small follow-up article which will cover the Appliance variant. The Open Source projects Fluentd and Fluent Bit can be leveraged for multi-platform log processing and log forwarding. For my use case with VEBA, my requirements on the solution were basically simple. It should be lightweight, fast and if possible, with few or no dependencies. Comparing both mentioned projects with each other, it brought me multiple times back to Fluent Bit, even besides the fact, that good content on how to integrate Fluentd with TKG already exist. ","date":"2021-01-13","objectID":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/:1:0","tags":["FluentBit","vRLI","Kubernetes","VEBA","Tanzu","TKG","OSS"],"title":"Leveraging Fluent Bit on Tanzu Kubernetes Grid to send VEBA logs to vRealize Log Insight","uri":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/"},{"categories":["VEBA","VMware","Kubernetes","Logging","Open Source","Cloud Native"],"content":"The Hummingbird (Note: no technical relevance) The power of symbols thrills me. It’s that power that makes you think or remember a specific brand, a company or an event. Just think about the Kubernetes steering wheel for example. If you feel home in the cloud native space and you see a steering wheel in real life nowadays, you will probably think of Kubernetes. This is the tremendous power of symbols3. That is why there are fields of study for Symbology and movies with famous actors who are playing characters like Robert Langdon 😉. Figure I: Project Logos Fluentd and Fluent Bit\" Figure I: Project Logos Fluentd and Fluent Bit Now, if we look at the logos of both projects (Figure I), the maintainers did a great job in choosing a Carrier Pigeon4 for Fluentd and a Hummingbird5 for Fluent Bit. Fluentd supports an extensive library of over 1000 6 plugins to provide extended support and functionality for anything related to log and data management. A Carrier Pigeon is known for it’s impressive performance e.g. in terms of distance covered…and it carries out data 😃💡. BTW Fluentd -issue #924 from 2016 was about “A Fluentd new logo proposal” and is an intersting short read, why a change was requested and why it was important. Hummingbirds in comparison to, are lightweight and PRETTY fast. They have wing-flapping rates between 12 and 80 beats per second (depending on their actual size). Fluent Bit is designed to perform fast and efficient with least amount of resource usage. From the official Fluent Bit homepage: Quote Fluent Bit is designed with performance in mind: high throughput with low CPU and Memory usage. ","date":"2021-01-13","objectID":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/:2:0","tags":["FluentBit","vRLI","Kubernetes","VEBA","Tanzu","TKG","OSS"],"title":"Leveraging Fluent Bit on Tanzu Kubernetes Grid to send VEBA logs to vRealize Log Insight","uri":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/"},{"categories":["VEBA","VMware","Kubernetes","Logging","Open Source","Cloud Native"],"content":"A short comparison Both projects are very strong and feature-rich and great articles already exist on “how to deploy” Fluentd to forward logs to e.g. vRealize Log Insight Cloud. I put some of them in the resource section below. The following table compares both projects in specific areas. Fluentd Fluent Bit Scope Containers / Servers Embedded Linux / Containers / Servers Language C \u0026 Ruby C Memory ~40MB ~650KB Performance High Performance High Performance Dependencies Built as a Ruby Gem, it requires a certain number of gems. Zero dependencies, unless some special plugin requires them Plugins More than 1000 plugins available Around 70 plugins available License Apache License v2.0 Apache License v2.0 Source: Fluentd \u0026 Fluent Bit ","date":"2021-01-13","objectID":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/:3:0","tags":["FluentBit","vRLI","Kubernetes","VEBA","Tanzu","TKG","OSS"],"title":"Leveraging Fluent Bit on Tanzu Kubernetes Grid to send VEBA logs to vRealize Log Insight","uri":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/"},{"categories":["VEBA","VMware","Kubernetes","Logging","Open Source","Cloud Native"],"content":"Logging Primitives Before we start deploying and leveraging Fluent Bit to read and forward logs to VMware’s vRealize Log Insight, I wanted to briefly explain or refer you to some of the logging primitives of Cloud Native Applications or Services itself as well as how Fluent Bit is and can be configured. In general the following applies: Quote By default, container engines such as Docker capture the standard output or error and leverage the JSON-file driver on each host to write messages to files. Docker maintains a separate log file for each container and stores it in the /var/log/containers directory of the Docker host. Annotation for each log entry consists of the following: Log message Message origin - stdout or stderr Timestamp Source: The Big Easy: Visualizing Logging Data by Integrating Fluentd and vRealize Log Insight with VMware PKS Also, I found this good article during my research - Logging from Docker Containers to Elasticsearch with Fluent Bit - in which the author quoted the following statement from the Twelve-Factor App7 website regarding Logs. https://12factor.net/logs A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. During local development, the developer will view this stream in the foreground of their terminal to observe the app’s behavior. Doesn’t it make absolutely sense? Fluent Bit offers several Input plugins to collect logs and data from sources. From a high-level perspective, the structure of Fluent Bit looks as follows : To structure the collected data, Fluent Bit provides Parsers which can be used to convert the collected data into a structured format. Also, Filters are configurable to e.g. append additional information or to filter out unwanted log statements. Links to even more details can also be found in the resource section. ","date":"2021-01-13","objectID":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/:4:0","tags":["FluentBit","vRLI","Kubernetes","VEBA","Tanzu","TKG","OSS"],"title":"Leveraging Fluent Bit on Tanzu Kubernetes Grid to send VEBA logs to vRealize Log Insight","uri":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/"},{"categories":["VEBA","VMware","Kubernetes","Logging","Open Source","Cloud Native"],"content":"Prerequisites To get Fluent Bit in working state on your Kubernetes cluster, we do need a ServiceAccount with an assigned ClusterRole as well as with it’s appropriate ClusterRoleBinding. We will deploy Fluent Bit as a Kubernetes DaemonSet8 in a Namespace called fluent-bit. I’ve created the following repository on - rguske/fluent-bit-vmware-loginsight - which you can clone, make the appropriate adjustments to the files and apply everything easily onto your cluster. It contains the following four files which sequence of deplyoment can be identified based on the number at the beginning of the filename (except #4). 1-tkg-fluent-bit-preps.yaml 2-tkg-fluent-bit-configmap-cri.yaml 3-tkg-fluent-bit-ds.yaml 4-tkg-fluent-bit-configmap-docker.yaml ","date":"2021-01-13","objectID":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/:5:0","tags":["FluentBit","vRLI","Kubernetes","VEBA","Tanzu","TKG","OSS"],"title":"Leveraging Fluent Bit on Tanzu Kubernetes Grid to send VEBA logs to vRealize Log Insight","uri":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/"},{"categories":["VEBA","VMware","Kubernetes","Logging","Open Source","Cloud Native"],"content":" Repository Clone the repository locally and change into the directory fluent-bit-vmware-loginsight: git clone https://github.com/rguske/fluent-bit-vmware-loginsight.git \u0026\u0026 cd fluent-bit-vmware-loginsight ","date":"2021-01-13","objectID":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/:5:1","tags":["FluentBit","vRLI","Kubernetes","VEBA","Tanzu","TKG","OSS"],"title":"Leveraging Fluent Bit on Tanzu Kubernetes Grid to send VEBA logs to vRealize Log Insight","uri":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/"},{"categories":["VEBA","VMware","Kubernetes","Logging","Open Source","Cloud Native"],"content":"Adjustments Make your adjustments to the following files to meet your requirements and environment specifications: 2-tkg-fluent-bit-configmap-cri.yaml 3-tkg-fluent-bit-ds.yaml Containerd as Runtime - CRI Parser used! As mentioned at the beginning, I’m using a TKG cluster for my example respectively in my environment. VMware is using Containerd9 as it’s container runtime and therefore I had to adjust my configuration accordingly. If you are using Docker as your runtime on Kubernetes, use the file 4-tkg-fluent-bit-configmap-docker.yaml as an alternative. Fluent Bit Configmap The first adjustment you have to make is the replacement of your TKG clustername as well as the hostname of your vRealize Log Insight appliance. File 2-tkg-fluent-bit-configmap-cri.yaml: Replace the data for your TKG cluster in the filter-record.conf: | section Note: for non TKG cluster, just delete the record filter-record.conf: | [FILTER] Name record_modifier Match * Record tkg_instance tkg-veba Record tkg_cluster tkg-veba Enter your Log Insight server hostname in the output-syslog.conf section: output-syslog.conf: | [OUTPUT] Name syslog Match * Host vrli.jarvis.lab Port 514 Mode tcp Syslog_Format rfc5424 Syslog_Hostname_key tkg_cluster Syslog_Appname_key pod_name Syslog_Procid_key container_name Syslog_Message_key message syslog_msgid_key msgid Syslog_SD_key k8s Syslog_SD_key labels Syslog_SD_key annotations Syslog_SD_key tkg Fluent Bit DaemonSet Basically, it’s not necessary to change the image for Fluent Bit but you could replace it with the one of your choice. I used the official Fluent Bit image which is at the time of writing this post version 1.6.9 - fluent/fluent-bit:1.6.9. You could also use e.g. the VMware Fluent Bit Image in version 1.5.3: registry.tkg.vmware.run/fluent-bit:v1.5.3_vmware.1. I’ve validated the functionality of both images during my tests. More images: Official: https://hub.docker.com/r/fluent/fluent-bit Bitnami: https://hub.docker.com/r/bitnami/fluent-bit VMware Registry: registry.tkg.vmware.run/fluent-bit:v1.5.3_vmware.1 File 3-tkg-fluent-bit-ds.yaml: Make your edits at the spec section for the container: [...]spec:containers:- name:fluent-bitimage:fluent/fluent-bit:1.6.9[...] ","date":"2021-01-13","objectID":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/:5:2","tags":["FluentBit","vRLI","Kubernetes","VEBA","Tanzu","TKG","OSS"],"title":"Leveraging Fluent Bit on Tanzu Kubernetes Grid to send VEBA logs to vRealize Log Insight","uri":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/"},{"categories":["VEBA","VMware","Kubernetes","Logging","Open Source","Cloud Native"],"content":"Deploy Fluent Bit on Kubernetes Start with applying the 1-tkg-fluent-bit-preps.yaml file to have the prerequisites (Namespace, ServiceAccount, etc.) done: $ kubectl create -f 1-tkg-fluent-bit-preps.yaml namespace/fluent-bit created serviceaccount/fluent-bit created clusterrole.rbac.authorization.k8s.io/fluent-bit-read created clusterrolebinding.rbac.authorization.k8s.io/fluent-bit-read created Next is to apply the configmap - 2-tkg-fluent-bit-configmap-cri.yaml - for Fluent Bit: kubectl create -f 2-tkg-fluent-bit-configmap-cri.yaml configmap/fluent-bit-config created As mentioned in the Prerequisites section before, Fluent Bit will be deployed as a DaemonSet by using the 3-tkg-fluent-bit-ds.yaml file: kubectl create -f 3-tkg-fluent-bit-ds.yaml \u0026\u0026 kubectl wait pod --timeout=-1s --for=condition=Ready -l '!job-name' -n fluent-bit daemonset.apps/fluent-bit created pod/fluent-bit-dd4ql condition met kubectl get daemonsets.apps -n fluent-bit NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE fluent-bit 2 2 2 2 2 \u003cnone\u003e 95s kubectl get pods -n fluent-bit NAME READY STATUS RESTARTS AGE fluent-bit-9m7qq 1/1 Running 0 108s fluent-bit-dd4ql 1/1 Running 0 108s More information about the successful deployment and as a first indication that everything is working fine can be grabbed from the logs of one of the pods. kubectl logs -n fluent-bit fluent-bit-dd4ql Fluent Bit v1.6.9 * Copyright (C) 2019-2020 The Fluent Bit Authors * Copyright (C) 2015-2018 Treasure Data * Fluent Bit is a CNCF sub-project under the umbrella of Fluentd * https://fluentbit.io [2021/01/12 14:05:10] [ info] [engine] started (pid=1) [2021/01/12 14:05:10] [ info] [storage] version=1.0.6, initializing... [2021/01/12 14:05:10] [ info] [storage] in-memory [2021/01/12 14:05:10] [ info] [storage] normal synchronization mode, checksum disabled, max_chunks_up=128 [2021/01/12 14:05:11] [ info] [input:systemd:systemd.1] seek_cursor=s=f34691953f354fc19dbd06139503c160;i=156... OK [2021/01/12 14:05:11] [ info] [filter:kubernetes:kubernetes.0] https=1 host=kubernetes.default.svc port=443 [2021/01/12 14:05:11] [ info] [filter:kubernetes:kubernetes.0] local POD info OK [2021/01/12 14:05:11] [ info] [filter:kubernetes:kubernetes.0] testing connectivity with API server... [2021/01/12 14:05:11] [ info] [filter:kubernetes:kubernetes.0] API server connectivity OK [2021/01/12 14:05:11] [ info] [output:syslog:syslog.0] setup done for vrli.jarvis.lab:514 [2021/01/12 14:05:11] [ info] [http_server] listen iface=0.0.0.0 tcp_port=2020 [2021/01/12 14:05:11] [ info] [sp] stream processor started ","date":"2021-01-13","objectID":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/:6:0","tags":["FluentBit","vRLI","Kubernetes","VEBA","Tanzu","TKG","OSS"],"title":"Leveraging Fluent Bit on Tanzu Kubernetes Grid to send VEBA logs to vRealize Log Insight","uri":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/"},{"categories":["VEBA","VMware","Kubernetes","Logging","Open Source","Cloud Native"],"content":"What vRealize Log Insight shows The log output from the Fluent Bit pods itself looks promising. Now let’s check what vRLI has to offer. Login and check the Hosts section under Administration: Figure II: TKG cluster is listed in vRLI\" Figure II: TKG cluster is listed in vRLI My TKG cluster tkg-veba is showing up - nice! Warning During my tests, I deleted the filter-record value tkg_cluster at some point in the Configmap which resulted in the following misbehavior - Figure III. Figure III: vRLI Dashboard for Function Invocation Count\" Figure III: vRLI Dashboard for Function Invocation Count The cluster hostname is not showing up anymore - so this isn’t applicable. Now switching to Interactive Analytics to finally see what Fluent Bit is providing us and if our configuration is working as expected. I’ve filtered for hostname contains tkg-veba: Figure IV : vRLI Interactive Analytics\" Figure IV : vRLI Interactive Analytics ","date":"2021-01-13","objectID":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/:7:0","tags":["FluentBit","vRLI","Kubernetes","VEBA","Tanzu","TKG","OSS"],"title":"Leveraging Fluent Bit on Tanzu Kubernetes Grid to send VEBA logs to vRealize Log Insight","uri":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/"},{"categories":["VEBA","VMware","Kubernetes","Logging","Open Source","Cloud Native"],"content":"Monitoring Function Invocation Count Coming to the end of this post, I’ve created a simple Dashboard which is showing the invocation count of two functions. Both functions were invoked on a DrsVmPoweredOn and DrsVmPoweredOff event. This is how the log output looks like for e.g. the PowerCLI tagging function and the Python restpost-fn. Figure V: vRLI Dashboard Filters\" Figure V: vRLI Dashboard Filters Figure VI below is showing the dashboard and the invocation count of both functions based on the incoming logs delivered by Fluent Bit. Figure VI: vRLI Dashboard for Function Invocation Count\" Figure VI: vRLI Dashboard for Function Invocation Count Stay healthy and stay safe! ","date":"2021-01-13","objectID":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/:8:0","tags":["FluentBit","vRLI","Kubernetes","VEBA","Tanzu","TKG","OSS"],"title":"Leveraging Fluent Bit on Tanzu Kubernetes Grid to send VEBA logs to vRealize Log Insight","uri":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/"},{"categories":["VEBA","VMware","Kubernetes","Logging","Open Source","Cloud Native"],"content":"Resources Fluentd Log Insight Plugin Configure log forwarding from Tanzu Kubernetes Grid (TKG) to vRealize Log Insight Cloud Introducing vRealize LogInsight Cloud Helm Chart for Kubernetes Logs Implementing Log Forwarding with Fluent Bit Fluent Bit - Configuration File/ Configmap Fluent Bit - Input Plugins Fluent Bit - Input Plugins Github Fluent Bit - Output Plugins Fluent Bit Log Level vRealize Log Insight - Ports Supported RFC’s LogInsight CRI Website Monitoring the VMware Event Broker Appliance with vRealize Operations Manager ↩︎ cAdvisor ↩︎ Symbols ↩︎ Carrier Pigeon ↩︎ Hummingbird ↩︎ Fluentd - List of All Plugins ↩︎ Twelve-Factor App ↩︎ Kubernetes DaemonSet ↩︎ Containerd Homepage ↩︎ ","date":"2021-01-13","objectID":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/:9:0","tags":["FluentBit","vRLI","Kubernetes","VEBA","Tanzu","TKG","OSS"],"title":"Leveraging Fluent Bit on Tanzu Kubernetes Grid to send VEBA logs to vRealize Log Insight","uri":"/post/leveraging-fluent-bit-on-tkg-to-send-veba-logs-to-vrli/"},{"categories":["VMware","Troubleshooting","Kubernetes","Open Source"],"content":"What I've learned from troubleshooting a failed vSphere with Tanzu HAProxy deployment.","date":"2020-11-23","objectID":"/post/vsphere-with-tanzu-troubleshooting-haproxy/","tags":["November2020","vSphere","Kubernetes","Tanzu","TKG","HAProxy","Troubleshooting"],"title":"vSphere with Tanzu - Troubleshooting HAProxy deployment","uri":"/post/vsphere-with-tanzu-troubleshooting-haproxy/"},{"categories":["VMware","Troubleshooting","Kubernetes","Open Source"],"content":"Introduction You may have already heard and read about our latest changes regarding our Kubernetes offering(s) vSphere with Tanzu also former known as vSphere with Kubernetes. Personally, I was totally excited and full of anticipation to make my first hands-on experience with this new deployment option –\u003e vSphere Networking as an alternative to NSX-T and how HAProxy is doing it’s job within this construct. I don’t know how you are dealing with installations of the “NEW” but I always read the documentation first… … not … 🙊 … but I should! Honestly! Doesn’t matter of which kind of implementation we are talking, is it a Homelab (test environment), a Proof of Concept implementation or in production, our final goal is a working solution and this is why we should be prepared best. Through this article, I will even more stress this point because I did a mistake which did cost me time in troubleshooting the failure I’ve received. On the other hand and “as always”, it enlightened me and enriched my wealth of experience. Thank You I want to give James Lepthien a big thanks for all of his help and guidance. He’s my first go-to person when it comes to all networking related topics. ","date":"2020-11-23","objectID":"/post/vsphere-with-tanzu-troubleshooting-haproxy/:1:0","tags":["November2020","vSphere","Kubernetes","Tanzu","TKG","HAProxy","Troubleshooting"],"title":"vSphere with Tanzu - Troubleshooting HAProxy deployment","uri":"/post/vsphere-with-tanzu-troubleshooting-haproxy/"},{"categories":["VMware","Troubleshooting","Kubernetes","Open Source"],"content":"Preperations This article will not describe the vSphere with Tanzu installation itself. For this kind of details, I would like to point you to VMware’s official documentation or to this vSphere with Tanzu Quick Start Guide V1a (for evaluation purposes) which VMware is maintaining on the Cloud Platform Tech Zone. Also! For being prepared better as well as for documentation purposes, we are providing a checklist-file which can be downloaded through the Workload Management section in the vSphere Client and then shared or forwarded to the networking engineer of your confidence (Figure I). Figure I: Network Stack Checklist\" Figure I: Network Stack Checklist ","date":"2020-11-23","objectID":"/post/vsphere-with-tanzu-troubleshooting-haproxy/:2:0","tags":["November2020","vSphere","Kubernetes","Tanzu","TKG","HAProxy","Troubleshooting"],"title":"vSphere with Tanzu - Troubleshooting HAProxy deployment","uri":"/post/vsphere-with-tanzu-troubleshooting-haproxy/"},{"categories":["VMware","Troubleshooting","Kubernetes","Open Source"],"content":"Resources VMware Docs - vSphere with Tanzu Basics VMware Cloud Platform Tech Zone - vSphere with Tanzu Quick Start Guide V1a - HAProxy Direct Download Link - HAProxy Powershell/PowerCLI Deployment Script - HAProxy Build the Appliance from scratch Blogs: Cormac Hogan - Deploy HA-Proxy for vSphere with Tanzu Cormac Hogan - Enabling vSphere with Tanzu using HA-Proxy Frank Denneman - vSphere with Tanzu vCenter Server Network Configuration Overview ","date":"2020-11-23","objectID":"/post/vsphere-with-tanzu-troubleshooting-haproxy/:2:1","tags":["November2020","vSphere","Kubernetes","Tanzu","TKG","HAProxy","Troubleshooting"],"title":"vSphere with Tanzu - Troubleshooting HAProxy deployment","uri":"/post/vsphere-with-tanzu-troubleshooting-haproxy/"},{"categories":["VMware","Troubleshooting","Kubernetes","Open Source"],"content":"The Frontend Network Step 6 in the Quick Start Guide describes the deployment of the HAProxy Virtual Appliance on vSphere. OVA deployment step 7 gives us the ability to deploy HAProxy with an additional network interface to seperate the Kubernetes nodes of our clusters from the network used by clients or services to access these clusters (Figure II). Figure II: Option to choose Frontend Network\" Figure II: Option to choose Frontend Network Frontend Network Description Description Deploy the Appliance with 3 nics: a Management network (Supervisor -\u003e HAProxy dataplane), a single Workload network and a dedicated Frontend network. Load-balanced IPs are assigned on the Frontend network. The configuration of the additional Frontend network is not covered through the aforementioned Quick Start Guide (for simplicity reasons) but if you are going to deploy vSphere with Tanzu into production, it definitely should be used. I tried to simplify the traffic flow coming from the client and/ or service through the Kubernetes clusters roughly via the following chart: ","date":"2020-11-23","objectID":"/post/vsphere-with-tanzu-troubleshooting-haproxy/:3:0","tags":["November2020","vSphere","Kubernetes","Tanzu","TKG","HAProxy","Troubleshooting"],"title":"vSphere with Tanzu - Troubleshooting HAProxy deployment","uri":"/post/vsphere-with-tanzu-troubleshooting-haproxy/"},{"categories":["VMware","Troubleshooting","Kubernetes","Open Source"],"content":"It must be a valid Subnetwork OVA deployment step 2.7 requests the static IP address for the Frontend interface and this IP must be outside of the Load Balancer IP Range which you have to define in the next step 3.1. Furthermore, this IP range should not overlap with the assigned IP address of the Frontend network interface as well as with any other VMs in this range! I’ve misinterpreted the description for Loadbalancer IP ranges in step 3.1 (see Figure III) which I should have “painfully” realised after the complete deployment. Figure III: Wrong configuration\" Figure III: Wrong configuration At this point, I’d like to make a little excursion in networking 101 to make sure you also see my mistake which I’ve made through this misinterpretation. I entered 10.10.18.18/28 as an IP range which isn’t a valid Subnetwork. A valid subnetwork configuration for my evaluation deployment should have looked like: Description Address Network ID 10.10.18.16 Broadcast 10.10.18.31 Netmask 255.255.255.240 Host IP starts with 10.10.18.17 Host IP ends with 10.10.18.30 10.10.18.16/28 is valid and because I decided to use a /28 network, the available IPs for the virtual server range, which you have to provide in Step 5 in the Workload Management configuration (Figure IV) later on, starts with .17 and ends with .30. Figure IV: Load Balancer Configuration Workload Management Wizard\" Figure IV: Load Balancer Configuration Workload Management Wizard The following table gives you a quick overview of available Host IPs for a specific subnetwork. Subnet Mask /29 /28 /27 /26 /25 /24 Host IPs 6 14 30 62 162 254 * I excluded Network-ID and Broadcast-ID Don’t be confused The following has nothing to do with the actual problem but the hint could avoid an uncertainty. Figure V shows the configuration summary of Step 5 and Ingress CIDR/ IP Ranges: 10.10.18.19/11 (in my example) could be confusing because of the CIDR notation. Figure V: Load Balancer Configuration Workload Management Wizard\" Figure V: Load Balancer Configuration Workload Management Wizard What it shows, is the first IP - .19 - and the maximum number of remaining IPs - /11 - for your defined IP Ranges for Virtual Servers. ","date":"2020-11-23","objectID":"/post/vsphere-with-tanzu-troubleshooting-haproxy/:3:1","tags":["November2020","vSphere","Kubernetes","Tanzu","TKG","HAProxy","Troubleshooting"],"title":"vSphere with Tanzu - Troubleshooting HAProxy deployment","uri":"/post/vsphere-with-tanzu-troubleshooting-haproxy/"},{"categories":["VMware","Troubleshooting","Kubernetes","Open Source"],"content":"Connect: No route to host. At first sight, the installation seems to be successful and the Cluster Config Status in the vSphere client doesn’t indicate the opposite (Figure VI). Figure VI: Workload Management Status\" Figure VI: Workload Management Status But appearances are deceptive! I created a vSphere Namespace to deploy a Tanzu Kubernetes Cluster (Guest Cluster) and clicked on the OPEN button/ link to the CLI tools (Figure VII) to check the reachability of my Supervisor Cluster and got an ERR_ADDRESS_UNREACHABLE back. Figure VII: Link to CLI Tools\" Figure VII: Link to CLI Tools Of course that made me suspicious and I tried to login via kubectl vsphere login: kubectl vsphere login --insecure-skip-tls-verify --vsphere-username administrator@jarvis.lab --server=10.10.18.19 and got the following error: ERROR ERRO[0005] Error occurred during HTTP request: Get https://10.10.18.19/wcp/loginbanner: dial tcp 10.10.18.19:443: connect: no route to host There was an error when trying to connect to the server.\\n Please check the server URL and try again.FATA[0005] Error while connecting to host 10.10.18.19: Get https://10.10.18.19/wcp/loginbanner: dial tcp 10.10.18.19:443: connect: no route to host. ","date":"2020-11-23","objectID":"/post/vsphere-with-tanzu-troubleshooting-haproxy/:4:0","tags":["November2020","vSphere","Kubernetes","Tanzu","TKG","HAProxy","Troubleshooting"],"title":"vSphere with Tanzu - Troubleshooting HAProxy deployment","uri":"/post/vsphere-with-tanzu-troubleshooting-haproxy/"},{"categories":["VMware","Troubleshooting","Kubernetes","Open Source"],"content":"HAProxy Frontend VIP bind service Having a look at my above drawing it shows us that our incoming request tries to reach the Frontend VIP 10.10.18.19 trough HAProxy. Because HAProxy is providing those VIP’s, it’s logical to start troubleshooting the HAProxy appliance first. SSH is enabled by default. I started with a simple status check of all services by executing systemctl status. The output was surprisingly degraded. Let`s narrow it down! root@haproxy [ / ]# systemctl list-units --state=failed UNIT LOAD ACTIVE SUB DESCRIPTION ● anyip-routes.service loaded failed failed anyip-routes.service ● haproxy.service loaded failed failed HAProxy Load Balancer ● route-tables.service loaded failed failed route-tables.service Three services have failed to start. I’m starting from the top with verifying the anyip-routes.service. root@haproxy [ / ]# systemctl status anyip-routes.service ● anyip-routes.service Loaded: loaded (/etc/systemd/system/anyip-routes.service; enabled; vendor preset: enabled) Active: failed (Result: exit-code) since Fri 2020-11-20 15:30:07 UTC; 3min 5s ago Process: 783 ExecStopPost=/var/lib/vmware/anyiproutectl.sh down (code=exited, status=0/SUCCESS) Process: 777 ExecStartPre=/var/lib/vmware/anyiproutectl.sh up (code=exited, status=2) Process: 776 ExecStartPre=/bin/mkdir -p /var/log/vmware (code=exited, status=0/SUCCESS) Nov 20 15:30:07 haproxy.jarvis.lab systemd[1]: Starting anyip-routes.service... Nov 20 15:30:07 haproxy.jarvis.lab anyiproutectl.sh[777]: adding route for 10.10.18.18/28 Nov 20 15:30:07 haproxy.jarvis.lab anyiproutectl.sh[777]: RTNETLINK answers: Invalid argument Nov 20 15:30:07 haproxy.jarvis.lab systemd[1]: anyip-routes.service: Control process exited, code=exited status=2 Nov 20 15:30:07 haproxy.jarvis.lab systemd[1]: anyip-routes.service: Failed with result 'exit-code'. Nov 20 15:30:07 haproxy.jarvis.lab systemd[1]: Failed to start anyip-routes.service. The mentioned script in this line Process: 783 ExecStopPost=/var/lib/vmware/anyiproutectl.sh down (code=exited, status=0/SUCCESS) seems to do something and by reviewing it, I found the following hint to the configuration file location for the broken service. [...] # The path to the config file used by this program. CONFIG_FILE=\"${CONFIG_FILE:-/etc/vmware/anyip-routes.cfg}\" [...] root@haproxy [ /etc/vmware ]# cat anyip-routes.cfg # # Configuration file that contains a line-delimited list of CIDR values # that define the network ranges used to bind the load balancer's frontends # to virtual IP addresses. # # * Lines beginning with a comment character, #, are ignored # * This file is used by the anyip-routes service # 10.10.18.18/28 Verifying the file and thus my made configuration finally brought me enlightenment. I changed it consequently from 10.10.18.18/28 to a valid Subnetwork range, which is the already mentioned 10.10.18.16/28 and rebooted the HAProxy appliance (reboot). Just restarting the service with systemctl restart anyip-routes.service does not apply all necessary changes. Figure VIII: Adjusted anyip-routes.cfg file | reboot appliance | ping Supervisor VIP\" Figure VIII: Adjusted anyip-routes.cfg file | reboot appliance | ping Supervisor VIP Finally, the validation of the restored functionality: kubectl vsphere login --insecure-skip-tls-verify --vsphere-username administrator@jarvis.lab --server=10.10.18.19 Password: Logged in successfully. You have access to the following contexts: 10.10.18.19 tkg-app If the context you wish to use is not in this list, you may need to try logging in again later, or contact your cluster administrator. To change context, use `kubectl config use-context \u003cworkload name\u003e Figure IX: Validating Supervisor VIP reachability\" Figure IX: Validating Supervisor VIP reachability Done ✅ ","date":"2020-11-23","objectID":"/post/vsphere-with-tanzu-troubleshooting-haproxy/:4:1","tags":["November2020","vSphere","Kubernetes","Tanzu","TKG","HAProxy","Troubleshooting"],"title":"vSphere with Tanzu - Troubleshooting HAProxy deployment","uri":"/post/vsphere-with-tanzu-troubleshooting-haproxy/"},{"categories":["VEBA","FaaS","Desktop","Kubernetes","Cloud Native","VMware"],"content":"A practitioner's guide to spin up a local VMware Event Broker Appliance (VEBA) environment by using KinD and the vCenter simulator.","date":"2020-10-16","objectID":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/","tags":["October2020","VEBA","Kubernetes","KinD","vcsim","FaaS","Fusion","Workstation"],"title":"A practitioner's guide - Spin up a local VEBA environment with KinD and vCenter simulator","uri":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/"},{"categories":["VEBA","FaaS","Desktop","Kubernetes","Cloud Native","VMware"],"content":"Introduction In my previous blog post I took A closer look at VMware’s Project Nautilus and went through some of the implementation details (Fusion 12). I described how it provides a single development platform on the desktop by enabling you to run, build and manage OCI compliant Containers as well as how easy it is now to instantiate Kubernetes Clusters besides Virtual Machines. ","date":"2020-10-16","objectID":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/:1:0","tags":["October2020","VEBA","Kubernetes","KinD","vcsim","FaaS","Fusion","Workstation"],"title":"A practitioner's guide - Spin up a local VEBA environment with KinD and vCenter simulator","uri":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/"},{"categories":["VEBA","FaaS","Desktop","Kubernetes","Cloud Native","VMware"],"content":"The question This post is intented to answer a question which came into my mind during my work on VMware’s Open Source project VMware Event Broker Appliance (VEBA) and as I’ve heard of Project Nautilus for the first time. The Question Given these planned innovations (Container and Kubernetes integration), could they help me and others developing new functions locally with just a minimal effort and resources? To be honest, the answer to this question was already partially answered with YES before the idea was born to leverage VMware Fusion (or Workstation) for this use case. Fortunately, now seems to be the right time to write about it. Michael Gasch, the maintainer of the Event Router helped me great in supporting the joint idea by not only creating a vcsim container image but also by making necessary code changes to the Event Router to support vcsim as a supported Event Provider ( PR #231) . UPDATE: October 19, 2020 The aforementioned PR #231 was merged in PR #237. vcsim is now supported as a Event Provider for the Event Router. Adjustments to this post were made accordingly. Thanks Michael! ","date":"2020-10-16","objectID":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/:2:0","tags":["October2020","VEBA","Kubernetes","KinD","vcsim","FaaS","Fusion","Workstation"],"title":"A practitioner's guide - Spin up a local VEBA environment with KinD and vCenter simulator","uri":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/"},{"categories":["VEBA","FaaS","Desktop","Kubernetes","Cloud Native","VMware"],"content":"Modular Architecture A look into the inners of VEBA shows us that it is built out of several pieces (modular approach) like e.g. VMware’s Photon OS (Operating System), Kubernetes (Orchestrator/ Runtime) and the Event-Processor (Application). The event provider part is covered by the Event Router which is responsible for connecting to event stream sources, such as the VMware vCenter Server and forward events to an event processor like e.g. OpenFaaS or AWS EventBridge. The following picture of the high-level architecture gives you an idea how everything is in concert. Figure I: VMware Event Broker Appliance Architecture\" Figure I: VMware Event Broker Appliance Architecture Source: VEBA Documentation - Architecture ","date":"2020-10-16","objectID":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/:3:0","tags":["October2020","VEBA","Kubernetes","KinD","vcsim","FaaS","Fusion","Workstation"],"title":"A practitioner's guide - Spin up a local VEBA environment with KinD and vCenter simulator","uri":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/"},{"categories":["VEBA","FaaS","Desktop","Kubernetes","Cloud Native","VMware"],"content":"Heavyweight vs Lightweight ","date":"2020-10-16","objectID":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/:4:0","tags":["October2020","VEBA","Kubernetes","KinD","vcsim","FaaS","Fusion","Workstation"],"title":"A practitioner's guide - Spin up a local VEBA environment with KinD and vCenter simulator","uri":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/"},{"categories":["VEBA","FaaS","Desktop","Kubernetes","Cloud Native","VMware"],"content":"Heavyweight Normally, you would deploy VEBA into a dedicated environment, be it a homelab, a development environment or even in production. Since we are listening to the vCenter Event stream, we also need a vCenter Server (Appliance) to which we connect VEBA to. Your developed functions would then be tested against those environments and since we are talking about testing or prototyping and given the fact that our customers are already using VEBA in production, it would be good (if not even necessary) to have a development environment as tiny and fast deployable as posibble available. With a decent desktop model, you could run VEBA as well as the vCenter Server locally but from a hardware requirement point of view this would mean that your desktop should be equipped with at least 24 GB of RAM for adequate testing. And not to forget about the ESXi host (nested) and at least one VM! Appliance(s) deployment requirements Appliance CPU Memory Documentation vCenter Server Appliance 2 12 GB Documentation VMware Event Broker Appliance 2 8 GB Documentation TOTAL TOTAL 4 vCPU’s 20 GB Memory ","date":"2020-10-16","objectID":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/:4:1","tags":["October2020","VEBA","Kubernetes","KinD","vcsim","FaaS","Fusion","Workstation"],"title":"A practitioner's guide - Spin up a local VEBA environment with KinD and vCenter simulator","uri":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/"},{"categories":["VEBA","FaaS","Desktop","Kubernetes","Cloud Native","VMware"],"content":"Lightweight The lightweight alternative would be, to e.g. leverage VMware’s Fusion or Workstation to spin up a Kubernetes cluster locally and to deploy the Event Router, OpenFaaS as well as your function(s) on top of it. Asking yourself now: And what about the vCenter Server? Good question! Here comes vcsim into play. vcsim is a vSphere API mock framework which is part of the govmomi project, a Go library for interacting with VMware vSphere APIs - or in simple terms - it’s a vCenter and ESXi API based simulator. The following content in the table below serves as an overview of what and how we are going to deploy in the next sections. As I have already indicated, my intention is to use Fusion (I’m a Mac user) to instantiate the Kubernetes cluster but ultimately, you could use any tool that runs Kubernetes local on your machine. Component Kind Installation via Documentation Kubernetes 1 Node via Fusion - vctl and kind Enabling KIND to Use vctl Container as Nodes to Run Kubernetes Clusters OpenFaaS Kubernetes Deployment via .yaml files Deployment guide for Kubernetes vcsim Binary or Container via kubectl or vctl govcsim Installation Function Container via faas-cli Get started: Install the CLI CLI’s we are going to use: kubectl - except you are using Fusion or Workstation: LINK git faas-cli Tip Using a “pimped” terminal is really helpful, efficient and FUN. Have a look at my blog post about the “High Way to Shell” to arm yourself accordingly. 😁 Just to give you an idea… Figure II: Pimped Terminal\" Figure II: Pimped Terminal Want to see it in action? I’ve putted a short recording in the Conclusion section at the end of this post. ","date":"2020-10-16","objectID":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/:4:2","tags":["October2020","VEBA","Kubernetes","KinD","vcsim","FaaS","Fusion","Workstation"],"title":"A practitioner's guide - Spin up a local VEBA environment with KinD and vCenter simulator","uri":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/"},{"categories":["VEBA","FaaS","Desktop","Kubernetes","Cloud Native","VMware"],"content":"Let’s get started ","date":"2020-10-16","objectID":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/:5:0","tags":["October2020","VEBA","Kubernetes","KinD","vcsim","FaaS","Fusion","Workstation"],"title":"A practitioner's guide - Spin up a local VEBA environment with KinD and vCenter simulator","uri":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/"},{"categories":["VEBA","FaaS","Desktop","Kubernetes","Cloud Native","VMware"],"content":"1. Create the Kubernetes cluster node Start with the deployment of the CRX VM that hosts the Kubernetes node container by executing (skip this if you aren’t using Fusion): vctl system start vctl assigns 2 GB of memory and 2 CPU cores by default for the CRX VM Depending on your needs, you can adjust the configuration by using e.g. vctl system config --k8s-cpus 4 --k8s-mem 8192 to have more CPUs and Memory assigned The next step is the Kubenretes cluster creation itself by leveraging KinD which will be available after executing vctl kind Create a new Kubernetes node with e.g. the following parameters: kind create cluster --image kindest/node:v1.19.1 --name kind-1.19.1 More details here: rguske - KinD create Cluster Validate the creation of your Kubernetes cluster by executing: kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kind-1.19.1-control-plane Ready master 96s v1.19.1 192.168.43.153 \u003cnone\u003e Ubuntu Groovy Gorilla (development branch) 4.19.138-7.ph3-esx containerd://1.4.0 Off to Step 2! ","date":"2020-10-16","objectID":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/:5:1","tags":["October2020","VEBA","Kubernetes","KinD","vcsim","FaaS","Fusion","Workstation"],"title":"A practitioner's guide - Spin up a local VEBA environment with KinD and vCenter simulator","uri":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/"},{"categories":["VEBA","FaaS","Desktop","Kubernetes","Cloud Native","VMware"],"content":"2. Installing OpenFaaS® (event processor) Installing OpenFaaS® as the event processor on Kubernetes is pretty easy, straight forward and well documented. The following guidance is based on the official documentation. I’m reusing what already exists just to avoid forcing you to switch back and forth the browser tabs. I am going to cover the deployment by using plain yaml files. You could also use arkade or a helm chart for the installation. kubectl and plain YAML Clone the official repository from Github and switch into the faas-netes folder: git clone https://github.com/openfaas/faas-netes \u0026\u0026 cd faas-netes Validate that you are using the right Kubernetes Context: kubectl config current-context Your context should start with kind- like e.g. mine: kind-kind-1.19.1 Create the necessary Kubernetes Namespaces for OpenFaaS® by applying the namespaces.yml file: kubectl apply -f namespaces.yml Validate that the namespaces openfaas and openfaas-fn were created: kubectl get ns NAME STATUS AGE default Active 18m kube-node-lease Active 18m kube-public Active 18m kube-system Active 18m local-path-storage Active 17m openfaas Active 55s openfaas-fn Active 55s The next step is to create a password to access the OpenFaaS® Gateway with the user admin: Generate a random password: PASSWORD=$(head -c 12 /dev/urandom | shasum| cut -d' ' -f1) Use your own: PASSWORD=VMware1! Create the Kubernetes Secret for OpenFaaS®: kubectl -n openfaas create secret generic basic-auth \\ --from-literal=basic-auth-user=admin \\ --from-literal=basic-auth-password=\"$PASSWORD\" The basis is settled and now off to the main deployment. Every component of OpenFaaS® is described in a yaml file which is stored in the according /yaml directory: tree -L 2 yaml yaml ├── README.md ├── alertmanager-cfg.yml ├── alertmanager-dep.yml ├── alertmanager-svc.yml ├── basic-auth-plugin-dep.yml ├── basic-auth-plugin-svc.yml ├── controller-rbac.yml ├── crd.yml ├── faas-idler-dep.yml ├── gateway-dep.yml ├── gateway-external-svc.yml ├── gateway-svc.yml ├── nats-dep.yml ├── nats-svc.yml ├── prometheus-cfg.yml ├── prometheus-dep.yml ├── prometheus-rbac.yml ├── prometheus-svc.yml └── queueworker-dep.yml Deploy all the listed components now to your cluster by executing: kubectl apply -f yaml Validate the deployment by checking if the pods are in state running: kubectl get pods -n openfaas NAME READY STATUS RESTARTS AGE alertmanager-68bf7d4b6f-kxhw8 1/1 Running 0 39m basic-auth-plugin-5b89b8c4c9-dkfmv 1/1 Running 0 39m faas-idler-97694ffb4-hhwv7 1/1 Running 2 39m gateway-fc64ff4f6-d9bgt 2/2 Running 1 39m nats-7d86c64647-vl764 1/1 Running 0 39m prometheus-84dcddb5c8-h2dvt 1/1 Running 0 39m queue-worker-77f4446d48-jflvj 1/1 Running 0 39m Desired state fulfilled! To access the OpenFaaS® Gateway UI, we need the according IP address of it. Copy the name of the OpenFaaS Gateway pod and replace it in the following command: kubectl get pods/gateway-fc64ff4f6-d9bgt -n openfaas -o=jsonpath='{.status.hostIP}' 192.168.43.153 And now by using the port :31112 the portal will show up in your browser tab: Figure III: OpenFaaS® Portal accessible via port forwarding\" Figure III: OpenFaaS® Portal accessible via port forwarding ","date":"2020-10-16","objectID":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/:5:2","tags":["October2020","VEBA","Kubernetes","KinD","vcsim","FaaS","Fusion","Workstation"],"title":"A practitioner's guide - Spin up a local VEBA environment with KinD and vCenter simulator","uri":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/"},{"categories":["VEBA","FaaS","Desktop","Kubernetes","Cloud Native","VMware"],"content":"3. vcsim - run the vSphere API Simulator Before our function(s) can be invoked based on a vSphere event, we need a stream of such coming from a source to which the VMware Event Router is listening to. Like aforementioned, vcsim will help us out here. Interestingly, vcsim is not new and William Lam already wrote about it in 2017(!): govcsim – Neat incubation project (vCenter Server \u0026 ESXi API based simulator). For our purposes, Michael prepared a Docker image which we are going to deploy as a Kubernetes pod and in which vcsim is running as our simulated vSphere endpoint. Furthermore, the necessary govc CLI binaries are also available via the container. The image is available HERE. We will deploy vcsim as well as the Event Router in a dedicated namespace called vmware: Create the namespace: kubectl create ns vmware Start deploying and running vcsim in Kubernetes: kubectl run vcsim --image=embano1/vcsim:0.24.0 --port=8989 --image-pull-policy=Always -n vmware Inspect the logs of the Pod to get to know the username and password which we are going to use for the Event Router configuration later on: kubectl logs -f -n vmware vcsim export GOVC_URL=https://administrator:REPLACEME@10.244.0.17:8989/sdk GOVC_SIM_PID=1 Expose vcsim as a service to receive traffic: kubectl expose pod vcsim -n vmware Connect to the deployed pod and verify that vcsim is ready for use: kubectl exec -i -t vcsim -n vmware bash Set the necessary environment variables for GOVC_URL and GOVC_INSECURE: vcsim@vcsim:~$ export GOVC_URL=https://administrator:REPLACEME@vcsim:8989/sdk vcsim@vcsim:~$ export GOVC_INSECURE=1 Check if your simulated vSphere Endpoint is working properly: vcsim@vcsim:~$ govc ls /DC0/vm /DC0/host /DC0/datastore /DC0/network List all available Virtual Machines: vcsim@vcsim:~$ govc ls \"/DC0/vm/*\" /DC0/vm/DC0_H0_VM0 /DC0/vm/DC0_H0_VM1 /DC0/vm/DC0_C0_RP0_VM0 /DC0/vm/DC0_C0_RP0_VM1 Power Off and On a Virtual Machine: # By default, the VMs are in Powered On state # Trying to power on a powered on VM would throw a \"govc: *types.InvalidPowerState\" error vcsim@vcsim:~$ govc vm.power -off /DC0/vm/DC0_H0_VM0 Powering off VirtualMachine:vm-57... OK vcsim@vcsim:~$ govc vm.power -on /DC0/vm/DC0_H0_VM0 Powering on VirtualMachine:vm-57... OK Awesome! ","date":"2020-10-16","objectID":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/:5:3","tags":["October2020","VEBA","Kubernetes","KinD","vcsim","FaaS","Fusion","Workstation"],"title":"A practitioner's guide - Spin up a local VEBA environment with KinD and vCenter simulator","uri":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/"},{"categories":["VEBA","FaaS","Desktop","Kubernetes","Cloud Native","VMware"],"content":"4. Deploy the Event Router During my work on this post, Michael and I discussed various aspects of the overall deployment of a development environment and especially the usability and simplicity was a main focus for us. Long story short - it ended up with the decision to include vcsim as a supported Event Provider for the Event Router 😁 👏. Before we can deploy the Router into our recently created namespace vmware, we need to create the event-router-config.yaml file from which we will create our Kubernetes Secret in a bit. If VSCode (code) isn’t your default editor adjust it accordingly. code event-router-config.yaml You can copy the complete provided code and paste into your empty event-router-config.yaml file: Attention This configuration is only valid if you followed my example. Otherwise adapt it accordingly. apiVersion:event-router.vmware.com/v1alpha1kind:RouterConfigmetadata:name:router-config-1labels:key:valueeventProvider:name:veba-demo-vcsim-01type:vcsimvcsim:insecureSSL:trueaddress:https://vcsim.vmware:8989/sdkauth:type:basic_authbasicAuth:username:administratorpassword:REPLACEMEeventProcessor:type:openfaasname:veba-demo-openfaasopenfaas:address:http://gateway.openfaas:8080async:falseauth:type:basic_authbasicAuth:username:adminpassword:VMware1!metricsProvider:type:defaultname:veba-demo-metricsdefault:bindAddress:\"0.0.0.0:8082\" Create the Kubernetes Secret: kubectl -n vmware create secret generic event-router-config --from-file=event-router-config.yaml Download the Event Router deployment specification locally and change the image version from latest to development: Available Images on Dockerhub wget https://raw.githubusercontent.com/vmware-samples/vcenter-event-broker-appliance/development/vmware-event-router/deploy/event-router-k8s.yaml \u0026\u0026 code event-router-k8s.yaml apiVersion:apps/v1kind:Deploymentmetadata:labels:app:vmware-event-routername:vmware-event-routerspec:replicas:1selector:matchLabels:app:vmware-event-routertemplate:metadata:labels:app:vmware-event-routerspec:containers:- image:vmware/veba-event-router:developmentargs:[\"-config\",\"/etc/vmware-event-router/event-router-config.yaml\",\"-verbose\"]name:vmware-event-routerresources:requests:cpu:200mmemory:200MivolumeMounts:- name:configmountPath:/etc/vmware-event-router/readOnly:truevolumes:- name:configsecret:secretName:event-router-config---apiVersion:v1kind:Servicemetadata:labels:app:vmware-event-routername:vmware-event-routerspec:ports:- port:8080protocol:TCPtargetPort:8080selector:app:vmware-event-routersessionAffinity:None Deploy the Event Router: kubectl apply -f event-router-k8s.yaml -n vmware Check the logs to see if everything works as desired (replace the pod name with yours): kubectl logs -f -n vmware vmware-event-router-7bb7fd8577-jhlg9 Finally, we’ve reached the last step - The deployment of an example function. ","date":"2020-10-16","objectID":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/:5:4","tags":["October2020","VEBA","Kubernetes","KinD","vcsim","FaaS","Fusion","Workstation"],"title":"A practitioner's guide - Spin up a local VEBA environment with KinD and vCenter simulator","uri":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/"},{"categories":["VEBA","FaaS","Desktop","Kubernetes","Cloud Native","VMware"],"content":"5. Test with an example function Clone the VEBA repository locally and directly change into the echo function folder: git clone https://github.com/vmware-samples/vcenter-event-broker-appliance.git \u0026\u0026 cd vcenter-event-broker-appliance/examples/python/echo Edit the stack.yml code stack.yml For gateway: add your noted IP (OpenFaaS® Gateway) version:1.0provider:name:openfaasgateway:http://192.168.43.153:31112functions:veba-echo:lang:pythonhandler:./handlerimage:vmware/veba-python-echo:latestenvironment:write_debug:trueread_debug:trueannotations:topic:\"VmPoweredOnEvent,VmPoweredOffEvent\" Login with faas-cli login and by using the option -g, which obvisouly stands for Gateway: faas-cli login --username admin --password 'VMware1!' --tls-no-verify -g http://192.168.43.153:31112 Deploy the veba-echo function: faas-cli deploy -f stack.yml --tls-no-verify Double check that the function is in state running: via faas-cli list -g http://192.168.43.153:31112 --verbose Function Image Invocations Replicas powercli-tag vmware/veba-powercli-tagging:latest 0 1 veba-echo vmware/veba-python-echo:latest 0 1 And by using kubectl get pods -n openfaas-fn -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES powercli-tag-58466957cc-vrfqp 1/1 Running 0 8h 10.244.0.27 kind-1.19.1-control-plane \u003cnone\u003e \u003cnone\u003e veba-echo-69cd854b7c-btjhn 1/1 Running 0 9h 10.244.0.26 kind-1.19.1-control-plane \u003cnone\u003e \u003cnone\u003e I have also deployed the powercli-tag function for my tests and you can watch the result via the recording below. ","date":"2020-10-16","objectID":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/:5:5","tags":["October2020","VEBA","Kubernetes","KinD","vcsim","FaaS","Fusion","Workstation"],"title":"A practitioner's guide - Spin up a local VEBA environment with KinD and vCenter simulator","uri":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/"},{"categories":["VEBA","FaaS","Desktop","Kubernetes","Cloud Native","VMware"],"content":"Conclusion (+ Recording) With the ability to leverage VMware Fusion or Workstation to quickly create a KinD Kubernetes cluster, leads you really to this at the beginning mentioned “single development platform on the desktop” experience. Of course, every tool which makes it easy to run Kubernetes locally is great and useable. We’ve deployed the necessary components like e.g. OpenFaaS®, the VMware Event Router, the vCenter Simulator vcsim as well as example functions onto the local running (KinD) Kubernetes cluster. To finally test our deployed functions, we had to invoke them by creating events from a source. We leveraged a container image for that, which was created by Michael Gasch and which includes the vCenter Simulator vcsim as well as the binaries for the govc CLI to execute the relevant commands. Watch it here: ","date":"2020-10-16","objectID":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/:6:0","tags":["October2020","VEBA","Kubernetes","KinD","vcsim","FaaS","Fusion","Workstation"],"title":"A practitioner's guide - Spin up a local VEBA environment with KinD and vCenter simulator","uri":"/post/spin-up-a-local-veba-environment-with-kind-and-vcenter-simulator/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"How project Nautilus provides a single development platform on the desktop by enabling users to run OCI compliant Containers as well as Kubernetes besides Virtual Machines on the desktop.","date":"2020-09-20","objectID":"/post/a-closer-look-at-vmwares-project-nautilus/","tags":["September2020","Fusion","Workstation","Kubernetes","Desktop"],"title":"A closer look at VMware's Project Nautilus","uri":"/post/a-closer-look-at-vmwares-project-nautilus/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"VMware’s Desktop Hypervisor solutions Fusion for Mac users and Workstation for the Windows and Linux userbase were launched in it’s newest versions on September 15th. Lots of new features, enhancements and support around VM Guest Operating Systems, VM Scaling, GPU, Containers and Kubernetes has made it into these releases. The made enhancements will enrich Developers tool kit as well as will provide great new capabilities for IT Admins and everyone else who is keen on spinning up and down Virtual Machines, Containers and NOW also Kubernetes. ","date":"2020-09-20","objectID":"/post/a-closer-look-at-vmwares-project-nautilus/:0:0","tags":["September2020","Fusion","Workstation","Kubernetes","Desktop"],"title":"A closer look at VMware's Project Nautilus","uri":"/post/a-closer-look-at-vmwares-project-nautilus/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"A short recap on Project Nautilus With Fusion Pro Tech Preview 20H1, VMware introduced Project Nautilus earlier this year (January). The main goal of the project is to provide a single development platform on the desktop by enabling users to run OCI compliant Containers as well as Kubernetes besides Virtual Machines on the desktop. A couple of months later (May), Fusion version 11.5 went GA and with this, the possibility was given to manage containers (build \u0026 run) as well as VMware’s containerd based runtime. Checkout the corresponding blog post to get to know more about it: ➡️ Fusion 11.5.5 Available Now. In order to make use of the intruduced “Container feature”, a new command-line utility named vctl is automatically installed when installing Fusion version 11.5 and higher or Workstation 16 (depending on your OS). Users who are familiar with Docker will propably get a confidential feeling very quickly when using it. Let me show you what I mean by executing vctlin my terminal: vctl vctl - A CLI tool for the container engine powered by VMware Fusion vctl Highlights: • Build and run OCI containers. • Push and pull container images between remote registries \u0026 local storage. • Use a lightweight virtual machine (CRX VM) based on VMware Photon OS to host a container. Use 'vctl system config -h' to learn more. • Easy shell access into virtual machine that hosts container. See 'vctl execvm’. USAGE: vctl COMMAND [OPTIONS] COMMANDS: build Build a container image from a Dockerfile. create Create a new container from a container image. describe Show details of a container. exec Execute a command within a running container. execvm Execute a command within a running virtual machine that hosts container. help Help about any command. images List container images. ps List containers. pull Pull a container image from a registry. push Push a container image to a registry. rm Remove one or more containers. rmi Remove one or more container images. run Run a new container from a container image. start Start an existing container. stop Stop a container. system Manage the container engine. tag Tag container images. version Print the version of vctl. Run 'vctl COMMAND --help' for more information on a command. OPTIONS: -h, --help Help for vctl What you see are pretty basic capabilities to build, run and manage containers locally with vctl. To get started with vctl, I’d like to point you directly to the official vctl Getting Started Guide on Github. ","date":"2020-09-20","objectID":"/post/a-closer-look-at-vmwares-project-nautilus/:1:0","tags":["September2020","Fusion","Workstation","Kubernetes","Desktop"],"title":"A closer look at VMware's Project Nautilus","uri":"/post/a-closer-look-at-vmwares-project-nautilus/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"New versions bring a KinD way to deploy Kubernetes Clusters As mentioned in the previous section of this post, it has been planned from the very beginning of Project Nautilus to provide both, the ability to run containers AND to instantiate Kubernetes clusters on the desktop. This has now been made possible by making use of the open source project KinD (Kubernetes in Docker), which is using Docker container “nodes” to run local Kubernetes clusters. KinD has reached a decent popularity in the community and it can be easily installed on various platforms by e.g. using Brew for Mac, Chocolatey for Windows and curl in general. See also the KinD Quick Start guide. So, what’s in for you when using Fusion or Workstation instead of the “known, step-by-step, manual way”? Advantages I see are e.g. not having the binaries of docker, kind and kubectl installed in advance on your platform as well as not to carry about the installation process/ method(s) itself. But let me show you some of the implementation details now, to hopefully enlighten you more. vctl is available after the installation of Fusion or Workstation. One thing before I go on! I will not cover both solutions! I’m a Mac user, so from here on I’m going to concentrate just on Fusion 12. ","date":"2020-09-20","objectID":"/post/a-closer-look-at-vmwares-project-nautilus/:2:0","tags":["September2020","Fusion","Workstation","Kubernetes","Desktop"],"title":"A closer look at VMware's Project Nautilus","uri":"/post/a-closer-look-at-vmwares-project-nautilus/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"Let’s dive in First! The ~/.vctl directory doesn’t exist before you have executed vctl system start for the first time! cd ~/.vctl cd: no such file or directory: /Users/rguske/.vctl Run vctl system start to start the containerd runtime daemon in the background and to have the aforementioned directory created. vctl system start Preparing storage... Container storage has been prepared successfully under /Users/rguske/.vctl/storage Launching container runtime... Container runtime has been started. How does it look now? ~/.vctl tree -L 2 . ├── Fusion\\ Container\\ Storage.sparseimage ├── config.json ├── config.toml ├── config.yaml ├── containerd.log ├── opt │ └── containerd └── storage └── containerd The directory has been created succesfully in $HOME (users home directory). As you can see via the following output, the binaries for docker, kind as well as kubectl aren’t installed on my local system: which docker docker not found which kind kind not found which kubectl kubectl not found Asking how vctl kind will --help us here. vctl kind --help Get system environment ready for vctl-based KIND. Using vctl as the provider for KIND instead of Docker, set up system environment to be ready for vctl-based KIND. * KIND will be downloaded and installed if it's not detected. * All Docker commands will be aliased to vctl in the current terminal. * System is only configured for current terminal. All configuration for KIND will be lost when the terminal is closed. USAGE: vctl kind [OPTIONS] OPTIONS: -h, --help Help for kind There is some really important information in here! If kind wasn’t already installed before, vctl will download and install it for you. In the current terminal session only, all docker commands will be aliased to vctl. All made configurations apply only to the running terminal session. vctl kind Downloading 3 files... Downloading [crx.vmdk 99.20% kubectl 62.50%] Finished crx.vmdk 100.00% Downloading [kubectl 100.00%] Finished kubectl 100.00% Downloading [kind-darwin-amd64 92.55%] Finished kind-darwin-amd64 100.00% 3 files successfully downloaded. vctl-based KIND is ready now. KIND will run local Kubernetes clusters by using vctl containers as \"nodes\" * All Docker commands has been aliased to vctl in the current terminal. Docker commands performed in current window would be executed through vctl. If you need to use regular Docker commands, please use a separate terminal window. sessions should be nested with care, unset $TMUX to force As you can see on the previous terminal output, the command will download the necessary binaries for kind and kubectl as well as a crx.vmdk which will be used by the CRX VM. VMware Documentation So in the Terminal window, the three binaries under user home ~/.vctl/bin folder will take precedence over other existing versions of kubectl/kind/docker binaries that were installed before. Aha! This means that in case I already had e.g. kubectl installed on my system, the execution of vctl kind will affect my current running terminal by adjusting the $PATH for the according binaries. Let me validate this by leveraging which(*) again: *The which utility takes a list of command names and searches the $PATH for each executable file. which kubectl /Users/rguske/.vctl/bin/kubectl which docker /Users/rguske/.vctl/bin/docker which kind /Users/rguske/.vctl/bin/kind That looks good to me! Also the ~/.vctl directory has been filled up too. tree -L 2 . ├── Fusion\\ Container\\ Storage.sparseimage ├── bin │ ├── crx.vmdk │ ├── docker -\u003e /Applications/VMware\\ Fusion.app/Contents/Library/vkd/bin/vctl │ ├── kind │ └── kubectl ├── config.json ├── config.toml ├── config.yaml ├── containerd.log ├── opt │ └── containerd └── storage └── containerd 5 directories, 9 files ","date":"2020-09-20","objectID":"/post/a-closer-look-at-vmwares-project-nautilus/:2:1","tags":["September2020","Fusion","Workstation","Kubernetes","Desktop"],"title":"A closer look at VMware's Project Nautilus","uri":"/post/a-closer-look-at-vmwares-project-nautilus/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"KinD create Cluster It’s time now to create a Kubernetes node. Note! vctl system start has to be executed first before leveraging kind for a deployment! Otherwise you will get the following error message: Error FATAL Container engine is not running. Run ‘vctl system start’ to launch it vctl assigns 2 GB of memory and 2 CPU cores by default for the CRX VM that hosts the Kubernetes node container. You can adjust the default vctl system configuration with the options --k8s-cpus and --k8s-mem. Example: vctl system config --k8s-cpus 4 --k8s-mem 8192 The results can be seen in the config.yaml file: cat ~/.vctl/config.yaml cache-location:/Users/rguske/.vctlk8s-cpus:4k8s-mem:8192log-level:infolog-location:/Users/rguske/.vctl/containerd.logmount-name:Fusion Container Storagepid:2671storage:128gvm-cpus:2vm-mem:1024vmnet:vmnet8 To create the Kubernetes node, basically kind create cluster is all you need to get started. If you like to give the node(s) a name or if you like to use a specific Kubernetes version which should run on your node, checkout the official Docker repository, pick your version and run e.g. kind create cluster --image kindest/node:v1.19.1 --name kind-1.19.1. After a couple of seconds, you will have a Kubernetes node running on your desktop locally. kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kind-1.19.1-control-plane Ready master 65s v1.19.1 192.168.43.153 \u003cnone\u003e Ubuntu Groovy Gorilla (development branch) 4.19.138-7.ph3-esx containerd://1.4.0 kubectl cluster-info --context kind-kind-1.19.1 Kubernetes master is running at https://127.0.0.1:63448 KubeDNS is running at https://127.0.0.1:63448/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. kubectl get all -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-f9fd979d6-spv95 1/1 Running 0 116m kube-system pod/coredns-f9fd979d6-vh22z 1/1 Running 0 116m kube-system pod/etcd-kind-1.19.1-control-plane 1/1 Running 0 116m kube-system pod/kindnet-2jjmc 1/1 Running 0 116m kube-system pod/kube-apiserver-kind-1.19.1-control-plane 1/1 Running 0 116m kube-system pod/kube-controller-manager-kind-1.19.1-control-plane 1/1 Running 0 116m kube-system pod/kube-proxy-6dnwg 1/1 Running 0 116m kube-system pod/kube-scheduler-kind-1.19.1-control-plane 1/1 Running 0 116m local-path-storage pod/local-path-provisioner-78776bfc44-pnfbl 1/1 Running 0 116m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 116m kube-system service/kube-dns ClusterIP 10.96.0.10 \u003cnone\u003e 53/UDP,53/TCP,9153/TCP 116m NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/kindnet 1 1 1 1 1 \u003cnone\u003e 116m kube-system daemonset.apps/kube-proxy 1 1 1 1 1 kubernetes.io/os=linux 116m NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/coredns 2/2 2 2 116m local-path-storage deployment.apps/local-path-provisioner 1/1 1 1 116m NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/coredns-f9fd979d6 2 2 2 116m local-path-storage replicaset.apps/local-path-provisioner-78776bfc44 1 1 1 116m Again! When closing the terminal, the following applies: Info The vctl-based KIND context will be lost if you close the Terminal window. Next time you want to interact with the Kubernetes clusters, run the vctl kind command. ","date":"2020-09-20","objectID":"/post/a-closer-look-at-vmwares-project-nautilus/:2:2","tags":["September2020","Fusion","Workstation","Kubernetes","Desktop"],"title":"A closer look at VMware's Project Nautilus","uri":"/post/a-closer-look-at-vmwares-project-nautilus/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"Recording The details of the last two sections can be watched via the following recording. I’ve used asciinema to record my terminal in and -output. ","date":"2020-09-20","objectID":"/post/a-closer-look-at-vmwares-project-nautilus/:2:3","tags":["September2020","Fusion","Workstation","Kubernetes","Desktop"],"title":"A closer look at VMware's Project Nautilus","uri":"/post/a-closer-look-at-vmwares-project-nautilus/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"Conclusion Before closing this post with my conclusion, I wanted to mention the following as well. Besides the addition of kind to the vctl utility, it also got some new options which e.g. allows you to login into a container registry like Harbor for example as well as to manage volumes (currently only support volume prune). vctl USAGE: vctl COMMAND [OPTIONS] COMMANDS: inspect Return low-level information on objects. kind Get system environment ready for vctl-based KIND. login Log in to a registry. logout Log out from a registry. volume Manage volumes. This is also documented in the Documentation. I enjoyed digging deeper into the implementation details of Project Nautilus which is now fully integrated in Fusion 12 and Workstation 16. This is a great addition to VMware’s Desktop Hypervisor solutions and provides a great experience not even for those who want to get started with containers and Kubernetes but also for the more experienced users among us. ","date":"2020-09-20","objectID":"/post/a-closer-look-at-vmwares-project-nautilus/:3:0","tags":["September2020","Fusion","Workstation","Kubernetes","Desktop"],"title":"A closer look at VMware's Project Nautilus","uri":"/post/a-closer-look-at-vmwares-project-nautilus/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"Resources Propose an Idea: LINK Join the Slack channel #fusion-workstation ","date":"2020-09-20","objectID":"/post/a-closer-look-at-vmwares-project-nautilus/:4:0","tags":["September2020","Fusion","Workstation","Kubernetes","Desktop"],"title":"A closer look at VMware's Project Nautilus","uri":"/post/a-closer-look-at-vmwares-project-nautilus/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"#Fusion Release Notes Download FAQ File a bug on Github: LINK ","date":"2020-09-20","objectID":"/post/a-closer-look-at-vmwares-project-nautilus/:4:1","tags":["September2020","Fusion","Workstation","Kubernetes","Desktop"],"title":"A closer look at VMware's Project Nautilus","uri":"/post/a-closer-look-at-vmwares-project-nautilus/"},{"categories":["Desktop","VMware","Kubernetes"],"content":"#Workstation Release Notes Download FAQ ","date":"2020-09-20","objectID":"/post/a-closer-look-at-vmwares-project-nautilus/:4:2","tags":["September2020","Fusion","Workstation","Kubernetes","Desktop"],"title":"A closer look at VMware's Project Nautilus","uri":"/post/a-closer-look-at-vmwares-project-nautilus/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"In this post I will focus on the installation of Harbor using Helm and also on the preperations you have to do upfront before you are able to let the Supervisor Cluster pull images out of Harbor and to subsequently instantiate them as a native Pod on vSphere.","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"I recently had to prepare my homelab for a customer workshop to demo our new Tanzu Runtime \u0026 Hybrid Infrastructure Services. This includes e.g. the deployment of a Tanzu Kubernetes Grid Cluster on vSphere (TKG Service), the enterprise cloud native registry Harbor as well as the instantiation of a native Pod on vSphere (vSphere Pod Service). Figure I: Tanzu Runtime \u0026 Hybrid Infrastructure Services\" Figure I: Tanzu Runtime \u0026 Hybrid Infrastructure Services My demo was basically all about the deployment of an application running natively (Native Pod) on vSphere, whose container image I pushed beforehand into my Harbor registry and which in turn was then obtained from there during the application deployment. It also covered the deep integration into vSAN and NSX by providing Persistent Volume Claims (Storage Service) as well as LoadBalancing services (Network Service) for my application. Figure II: Application Deployment Flowchart I do have to mention that I used the upstream Harbor version for my demo and not the embedded Harbor registry which can be enabled through the Registry Service of vSphere 7 with Kubernetes. The reason for this? I combined the possibilities to demonstrate the latest features of Harbor version 2.0 on the one hand and on the other hand the simplicity of deploying the registry via Helm on a Tanzu Kubernetes Grid Cluster. In this post I will focus on the installation of Harbor using Helm and also on the preperations you have to do upfront before you are able to let the Supervisor Cluster pull images out of Harbor and to subsequently instantiate them as a native Pod on vSphere. ","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/:0:0","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Prerequisites vSphere 7 with Kubernetes installed Subscribed Content Library for Tanzu Kubernetes Grid Clusters A Tanzu Kubernetes Grid Cluster running Kubernetes CLI Tool for vSphere installed Helm installed Docker CLI installed ","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/:1:0","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Tanzu Kubernetes Grid Cluster (Recap) The simple way of creating a Tanzu Kubernetes Grid (TKG) cluster will be done by invoking the TKG Service declarative API. A very detailed “How to Guide” is provided by Cormac Hogan through this post: Building a TKG Cluster in vSphere with Kubernetes I recommend reading this post or of course VMware’s official documentation first if you haven’t deployed a TKG cluster yet. Otherwise, here is my “How to” in a nutshell… ","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/:2:0","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Create a vSphere Namespace Go to Home –\u003e Workload Management and click on NEW NAMESPACE Figure III: Create a Namespace in vSphere\" Figure III: Create a Namespace in vSphere Add a user or a group to the Namespace, assign the appropriate permission and select the vSphere Storage Policy(s) for your Persistent Volume Claims. ","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/:2:1","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Declaritive deployment of a TKG Cluster We first login into our vSphere Supervisor Cluster by making use of the Kubernetes CLI Tool for vSphere with the before assigned user: kubectl vsphere login --server=172.25.16.1 --vsphere-username jarvis@jarvis.lab --insecure-skip-tls-verify A successful login will show you the following output: Logged in successfully. You have access to the following contexts: 172.25.16.1 harbor-on-tkg veba If the context you wish to use is not in this list, you may need to try logging in again later, or contact your cluster administrator. To change context, use `kubectl config use-context \u003cworkload name\u003e` The context harbor-on-tkg, which was automatically created during the vSphere Namespace creation, is showing up and will be used for our next step. Change the context by executing: kubectl config use-context harbor-on-tkg Now that we switched into our desired context, we can start deploying the TKG cluster and as already mentioned a couple of times before, this will be done in a declaritive way and by using a specification file like e.g. the following: apiVersion:run.tanzu.vmware.com/v1alpha1kind:TanzuKubernetesClustermetadata:name:harbor-on-tkgnamespace:harbor-on-tkgspec:distribution:version:v1.17.7topology:controlPlane:count:1class:best-effort-smallstorageClass:ftt1-r1workers:count:1class:best-effort-mediumstorageClass:ftt1-r1 A few words to the parameters version, class and storageClass. The available Kubernetes versions depend on which versions are provided by VMware through the Subscribed Content Library. You can verify which versions are available by executing kubectl get virtualmachineimages which in turn will show you e.g.: NAME AGE ob-15957779-photon-3-k8s-v1.16.8---vmware.1-tkg.3.60d2ffd 83d ob-16466772-photon-3-k8s-v1.17.7---vmware.1-tkg.1.154236c 8d ob-16545581-photon-3-k8s-v1.16.12---vmware.1-tkg.1.da7afe7 13h ob-16551547-photon-3-k8s-v1.17.8---vmware.1-tkg.1.5417466 13h v1.17.7 is the version I’ve picked for my deployment. The next one is class. You can choose between two types of Virtual Machine Classes to define the virtual hardware settings as well as the requests and limits on those. Just type kubectl get virtualmachineclasses to let you show the available classes. NAME AGE best-effort-large 90d best-effort-medium 90d best-effort-small 90d best-effort-xlarge 90d best-effort-xsmall 90d guaranteed-large 90d guaranteed-medium 90d guaranteed-small 90d guaranteed-xlarge 90d guaranteed-xsmall 90d kubectl describe virtualmachineclasses best-effort-medium for example gives you a detailed output of the specifications of a specific Virtual Machine Class. Also very important is to specify the storageClass. You defined it during the vSphere Namespace creation. If you cannot exactly remember which class you have defined…you are also not logged in into the vSphere Client…you feel lazy…just type kubectl describe ns harbor-on-tkg and get the desired information back. Name: harbor-on-tkg Labels: vSphereClusterID=domain-c7 Annotations: ncp/extpoolid: domain-c7:a4b59b96-9e94-4158-95ff-e1dfeb58c6bf-ippool-172-25-64-1-172-25-127-254 ncp/snat_ip: 172.25.88.14 ncp/subnet-0: 10.244.0.240/28 vmware-system-resource-pool: resgroup-11035 vmware-system-vm-folder: group-v11036 Status: Active Resource Quotas Name: harbor-on-tkg-storagequota Resource Used Hard -------- --- --- ftt1-r1.storageclass.storage.k8s.io/requests.storage 0 9223372036854775807 No resource limits. Enough summarized 😉 ! kubectl apply -f tkg-cluster.yml and let vSphere do it’s magic. An increased count of 1 will show up on the Tanzu Kubernetes tile at the vSphere Namespace page (Figure IV). Figure IV: Deployed Tanzu Kubernetes Grid cluster\" Figure IV: Deployed Tanzu Kubernetes Grid cluster The terminal can also be used for verification. kubectl get tanzukubernetescluster or in short kubectl get tkc: NAME CONTROL PLANE WORKER DISTRIBUTION AGE PHASE harbor-on-tkg 1 1 v1.17.7+vmware.1-tkg.1.154236c 26m running ","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/:2:2","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Installing Harbor via Helm ","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/:3:0","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Login into the TKG Cluster One important step has to be done before we pay our attention to this next part. You remember that we logged in into the Supervisor Cluster via kubectl vsphere login --server=172.25.16.1 --vsphere-username jarvis@jarvis.lab --insecure-skip-tls-verify. This means that we are still connected to it and even if you are in the right context (harbor-on-tkg), doesn’t mean that a desired application deployment will be processed on the TKG cluster. Because the TKG cluster is running inside the vSphere Namespace (harbor-on-tkg) and we have to connect to it seperately. To verify, just run kubectl get ns to get all Namespaces displayed which your Supervisor Cluster provides and not those from your TKG cluster. NAME STATUS AGE default Active 90d ghost-app Active 4d8h harbor-on-tkg Active 4d4h kube-node-lease Active 90d kube-public Active 90d kube-system Active 90d veba Active 35d vmware-system-capw Active 90d vmware-system-csi Active 90d vmware-system-kubeimage Active 90d vmware-system-nsx Active 90d vmware-system-registry Active 90d vmware-system-tkg Active 90d vmware-system-ucs Active 90d vmware-system-vmop Active 90d Logout from the Supervisor Cluster with kubectl vsphere logout and login into the TKG cluster: kubectl vsphere login --insecure-skip-tls-verify --vsphere-username jarvis@jarvis.lab --server=172.25.16.1 --tanzu-kubernetes-cluster-name harbor-on-tkg --tanzu-kubernetes-cluster-namespace harbor-on-tkg Enter kubectl get ns again to verify the established connection: NAME STATUS AGE default Active 2d1h kube-node-lease Active 2d1h kube-public Active 2d1h kube-system Active 2d1h vmware-system-auth Active 2d1h vmware-system-cloud-provider Active 2d1h vmware-system-csi Active 2d1h Let’s turn our attention to Helm - The modern package manager Helm helps us to define, deploy and upgrade applications easily. I’m going to use Helm to deploy Harbor on the Tanzu Kubernetes Grid cluster. ICYMI: Installing Helm ","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/:3:1","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Add Harbor Helm repository For Harbor, you can use various Helm repositories. There exist e.g. the official repo from Harbor - https://github.com/goharbor/harbor-helm or also the one from Bitnami - https://github.com/bitnami/charts/tree/master/bitnami/harbor which I’m going to use. Add the repository of your choice to your client… helm repo add harbor https://helm.goharbor.io helm repo add bitnami https://charts.bitnami.com/bitnami …and update Helm subsequently. helm repo update ","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/:3:2","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Installing Harbor - 1st attempt We will deploy Harbor in a new Kubernetes Namespace which we will name (suprise) harbor. Create the Namespace with kubectl create ns harbor and start the deployment process by executing the following helm command with some corresponding options: helm install harbor bitnami/harbor \\ --set harborAdminPassword='VMware1!' \\ --set global.storageClass=ftt1-r1 \\ --set service.type=LoadBalancer \\ --set externalURL=harbor-dev.jarvis.lab \\ --set service.tls.commonName=harbor-dev.jarvis.lab \\ -n harbor I monitored the deployment progress by watching (watch) the command kubectl get deployment -n harbor and I became sceptical because nothing happened for a long time. Checking the events of my deployment kubectl get events -n harbor was the next logical step and the following Message immediately catched my attention: Message forbidden: unable to validate against any pod security policy TKG Pod Security Policies (PSP) The Tanzu Kubernetes Grid Service does not provide default RoleBinding and ClusterRoleBinding for Tanzu Kubernetes clusters. Thus we have to apply a Pod Security Policy (PSP) to our TKG cluster. My well appreciated colleague Alexander Ullah has written a great article about PSP’s on Tanzu Kubernetes Grid Integrated Edition (former Enterprise PKS) –\u003e LINK. Apply the following Pod Security Policy example to your TKG cluster: kubectl apply -f tkg-psp.yml apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:psp:privilegedrules:- apiGroups:['policy']resources:['podsecuritypolicies']verbs:['use']resourceNames:- vmware-system-privileged---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:all:psp:privilegedroleRef:kind:ClusterRolename:psp:privilegedapiGroup:rbac.authorization.k8s.iosubjects:- kind:Groupname:system:serviceaccountsapiGroup:rbac.authorization.k8s.io ","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/:3:3","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Installing Harbor - 2nd attempt Let’s redo the Harbor deployment via helm again! By running kubectl get pods -n harbor you should see how every single Pod will “pop up” one by one and will end up in the status RUNNING. Figure V: Harbor running on a Tanzu Kubernetes Grid Cluster\" Figure V: Harbor running on a Tanzu Kubernetes Grid Cluster kubectl get svc -n harbor will give you the assigned external IP address through which you can access the Harbor Login page. See also the lower right terminal window in Figure V. With the gained IP address you can create an appropriate DNS entry. Remember that we’ve used --set externalURL=harbor-dev.jarvis.lab for our deployment. ","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/:3:4","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Deploy a Demo Application on vSphere 7 You should now be able to login into your fresh deployed Harbor instance via the GUI and also to start creating and configuring your first projects. So far it’s not possible to login via Docker CLI by using docker login. This is because of the Docker certificate-based client-server authentication. It’s necessary to create a new directory under /etc/docker/cert.d/ using the same name as the registry hostname (FQDN). I created a script for another blog post to simplify this process. The script is available on my Github repository here: https://github.com/rguske/download-harbor-cert-script. Take a look at the post if you need additional information about the steps as well as to learn how you can create and configure projects in Harbor. Pushing the application container image onto Harbor shouldn’t be a problem after a successful docker login harbor-dev.jarvis.lab --username admin. Ghost will serve us as a demo application here. It is quickly deployable and also pretty cool for demonstration purposes, because it needs a Kubernetes Persistent Volume Claim (–\u003e vSAN) and type: LoadBalancer can also be configured as parameter in the specification .yml file (–\u003e NSX). Even more cool demo applications were put together by William Lam in this post: Interesting Kubernetes application demos. Check-out the KubeDoom demo 😁 👍 Follow the next steps to have the image safely stored in Harbor: docker pull ghost:3.25.0 docker tag ghost:3.25.0 harbor-dev.jarvis.lab/ghost/ghost:3.25.0 docker push harbor-dev.jarvis.lab/ghost/ghost:3.25.0 Check the repository of your project Ghost in Harbor: Figure VI: Image Ghost:3.25.0 available in Harbor Project Ghost\" Figure VI: Image Ghost:3.25.0 available in Harbor Project Ghost If you are following this post step-by-step, I assume that you are still connected to your TKG Cluster via your terminal. Do a kubectl vsphere logout now. ","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/:4:0","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Demo Application Ghost Create two new files and name them e.g. ghost.yml and ghost-pvc.yml. Add the following specification parameters to the respective file: Application deployment - ghost.yml apiVersion:v1kind:Servicemetadata:labels:name:blogname:blogspec:ports:- port:80targetPort:2368selector:app:blogtype:LoadBalancer---apiVersion:apps/v1kind:Deploymentmetadata:name:bloglabels:app:blogspec:replicas:1selector:matchLabels:app:blogtemplate:metadata:labels:app:blogspec:containers:- name:blogimage:harbor-dev.jarvis.lab/ghost/ghost:3.25.0imagePullPolicy:Alwaysports:- containerPort:2368env:- name:urlvalue:http://my-blog.corp.localvolumeMounts:- mountPath:/var/lib/ghost/contentname:contentvolumes:- name:contentpersistentVolumeClaim:claimName:blog-content Persistent Volume Claim - ghost-pvc.yml ---kind:PersistentVolumeClaimapiVersion:v1metadata:name:blog-contentspec:accessModes:- ReadWriteOncestorageClassName:ftt0-r0resources:requests:storage:5Gi Create a new vSphere Namespace called ghost-demo-app and configure it similar to the one for your TKG cluster. Login into your Supervisor Cluster again and switch into the new context: kubectl vsphere login --server=172.25.16.1 --vsphere-username jarvis@jarvis.lab --insecure-skip-tls-verify kubectl config use-context ghost-demo-app Deploy the Persistent Volume Claim first: kubectl apply -f ghost-pvc.yml Deploy the demo application kubectl apply -f ghost.yml While I was waiting for my demo application to become available, I noticed that several native vSphere Pod’s showed up in the vSphere Client and powered on and powered off again and again. This is not the expected behaviour! I had to check the events again (kubectl get events -n ghost-demo-app) and the following Warning told me what’s wrong here: Warning esxi02: Failed to resolve image: Http request failed. Code 400: ErrorType(2) failed to do request: Head https://harbor-dev.jarvis.lab/v2/ghost/ghost/manifests/3.25.0: x509: certificate signed by unknown authority ","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/:4:1","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Add Harbor Certificate to the Supervisor Cluster The warning indicates that my Supervisor Cluster cannot pull the image out of Harbor due to the unknown certificate. Adding it to our local machine is easy but how can we realize it for vSphere? Luckily, that’s easy too. Interrupt the failed deployment loop of our application kubectl delete -f ghost.yml Open the downloaded Harbor root certificate (ca.crt) and copy the whole content out of it Switch back to your terminal and add the copied certificate data to your image-fetcher-ca-bundle configmap as shown in Figure VII kubectl edit configmap image-fetcher-ca-bundle -n kube-system Figure VII: Kubernetes Configmap image-fetcher-ca-bundle\" Figure VII: Kubernetes Configmap image-fetcher-ca-bundle save and close :wq Start the deployment again and watch the progress kubectl apply -f ghost.yml kubectl get deployment -n ghost-demo-app NAME READY UP-TO-DATE AVAILABLE AGE blog 1/1 1 1 3h26m kubectl get svc -n ghost-demo-app NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE blog LoadBalancer 10.96.0.10 172.25.16.2 80:32604/TCP 3h26m kubectl get pvc -n ghost-demo-app NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE blog-content Bound pvc-ee93cb30-309e-4627-b7df-7c27213ea14b 5Gi RWO ftt0-r0 3h48m kubectl get pods -n ghost-demo-app NAME READY STATUS RESTARTS AGE blog-b68cddc4c-tckvp 1/1 Running 0 3h26m Our demo application Ghost will be reachable via it’s external IP address which is automatically provided by the NSX Load Balancer Virtual Server (Figure VIII). Quote The creation of a Load Balancer type service causes NCP to orchestrate the creation of NSX Virtual Servers associated with the Load Balancer created in the initial Supervisor Cluster deployment. The virtual server is assigned an IP and port that is used to access the service. Source: vSphere 7 with Kubernetes Network Service Part 1: The Supervisor Cluster Figure VIII: Automatically created NSX Virtual Server\" Figure VIII: Automatically created NSX Virtual Server The data will be written to it’s Persistent Volume on vSAN (Figure IX) Figure IX: vSAN Persistent Volume\" Figure IX: vSAN Persistent Volume ","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/:4:2","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["Kubernetes","Cloud Native","VMware"],"content":"Conclusion Deploying a VMware Tanzu Kubernetes Grid cluster on vSphere by invoking the TKG Service declarative API is simple, flexible and pretty cool. You should bare in mind, that it is necessary to apply a Pod Security Policy first in order to deploy Pod’s on your TKG Cluster. Helm is powerful and can support you with defining, deploying and upgrading your applications fast and easily. BTW: Harbor supports Helm charts as well! Running Containers/ Pods natively alongside Virtual Machines in vSphere 7 is amazing and will support you by running any application on a single platform - vSphere. We’ve covered the topic, how you can add a certificate or a certificate bundle to your Supervisor Cluster in order to pull images from an external registry like e.g Harbor. Thank’s for reading ","date":"2020-07-21","objectID":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/:5:0","tags":["July2020","Harbor","Helm","Kubernetes","Tanzu","TKG","vSphere"],"title":"vSphere 7 with Kubernetes supercharged - Helm, Harbor and Tanzu Kubernetes Grid","uri":"/post/vsphere-7-with-kubernetes-supercharged-helm-harbor-tkg/"},{"categories":["VEBA","FaaS","Automation","VMware","Open Source"],"content":"A post about a community function contribution to the open source project VMware Event Broker Appliance to notify you if a ESXi host failure occured.","date":"2020-06-29","objectID":"/post/vsphere-ha-event-notification-function/","tags":["June2020","VEBA","FaaS"],"title":"VMware Event Broker Appliance - vSphere HA Event Notification Function","uri":"/post/vsphere-ha-event-notification-function/"},{"categories":["VEBA","FaaS","Automation","VMware","Open Source"],"content":"Community I recently had the pleasure to suppport a member out of the VMware Event Broker Appliance (VEBA) Slack Community with his function example contribution to the VEBA project. His name is Bob. He introduced his Powershell/ PowerCLI function on a remote session to me and I was immediately thrilled about what the function does. It covers a use case that one of my customers brought up some time ago when I first introduced VEBA to them. Bob’s engagement was really contagious and getting work done was quite fun. All credits goes out to Bob! ","date":"2020-06-29","objectID":"/post/vsphere-ha-event-notification-function/:1:0","tags":["June2020","VEBA","FaaS"],"title":"VMware Event Broker Appliance - vSphere HA Event Notification Function","uri":"/post/vsphere-ha-event-notification-function/"},{"categories":["VEBA","FaaS","Automation","VMware","Open Source"],"content":"Code Bob developed a script some time ago, which functionality basically is, to send out a notification via Email after a vSphere HA event occurred. This Email will have the affected host mentioned as well as all affected VMs which has been restarted through vSphere HA. The problem was, that the execution of the script was a manual task every time a ESXi host outage took place…at least until he became aware of VEBA. Quote \"…I knew VEBA would be a prime place to attempt to do this for me.\" Then he started digging into it. There is really good material available on “Getting started with VEBA” like for example this Series by Patrick Kremer as well as articles and documentation on “Writing your own function” like e.g. Writing your first Serverless Function by Partheeban Kandasamy (PK) and also here at the Documentation section at VEBA’s homepage. You don’t have to reinvent the wheel! If you have great code, like Bob, take it, make your hands dirty and write your first function in any of your preferred programming language. It is not that complicated and I promise you the joy of success when your first function does exactly what you built it for. If you don’t have own code available, start looking here: VMware {code} Sample Exchange. The community is diligent and willing to share. ","date":"2020-06-29","objectID":"/post/vsphere-ha-event-notification-function/:2:0","tags":["June2020","VEBA","FaaS"],"title":"VMware Event Broker Appliance - vSphere HA Event Notification Function","uri":"/post/vsphere-ha-event-notification-function/"},{"categories":["VEBA","FaaS","Automation","VMware","Open Source"],"content":"Contribution You are probably wondering when I finally introduce the function, right? The reason for my long introduction in this post is that I want to encourage you to start building your own function(s) to unlock the potential of event-driven automation in your data center. Like Bob successfully did. 😁 Here we go and waiting for it to get merged: VMware Event Broker Appliance PR #183 - Add function HA Restarted VMs Email Notification I had to share my excitment on Twitter when I first contributed to the project. It’s a great and unique feeling and even better when the PR got merged. Another moment for my \"I'll never forget this box\". 1st (last week) the feeling of having contributed to an #OpenSource project, my 1st PR (ever) was merged 🥳and then on top of that, a ded. coding session with no one else than @lamw today (PR#2 😬) 😀. Thank you so much! #VEBA — Robert Guske (@vmw_rguske) January 13, 2020 ","date":"2020-06-29","objectID":"/post/vsphere-ha-event-notification-function/:3:0","tags":["June2020","VEBA","FaaS"],"title":"VMware Event Broker Appliance - vSphere HA Event Notification Function","uri":"/post/vsphere-ha-event-notification-function/"},{"categories":["VEBA","FaaS","Automation","VMware","Open Source"],"content":"The HA Restart Function I’m not going into the details on how to deploy the function, because it works basically like the vSphere Datastore Usage Email Notification function which deployment is well explained step-by-step here: vCenter Event Broker Appliance – Part IX – Deploying the Datastore Usage Email sample script in VMC. Exactly two files are important to deploy the function. The vcconfig-ha-restarted-vms.json and the stack.yml. You have to fill out all the fields in the vcconfig-ha-restarted-vms.json file to have the required information like e.g. for the vCenter Server as well as for the SMTP server stored as a Kubernetes Secret. { \"VC\" : \"vcsa.jarvis.lab\", \"VC_USERNAME\" : \"vebauser@jarvis.lab\", \"VC_PASSWORD\" : \"VMware1!\", \"SMTP_SERVER\" : \"smtp.gmail.com\", \"SMTP_PORT\" : \"587\", \"SMTP_USERNAME\" : \"unknownidentity@gmail.com\", \"SMTP_PASSWORD\" : \"securepwd\", \"EMAIL_TO\": [\"unknownidentity@gmail.com\"], \"EMAIL_FROM\" : \"Jarvis Lab\" } vcconfig-ha-restarted-vms.json example The below listed stack.yml file describes on which vCenter Event the function will be invoked, where the function image is stored and on which VEBA instance the function will finally run. topic: com.vmware.vc.HA.ClusterFailoverActionCompletedEvent image: harbor.jarvis.lab/veba/veba-powercli-ha-restarted-vms:latest gateway: https://veba041.jarvis.lab version:1.0provider:name:openfaasgateway:https://veba041.jarvis.labfunctions:powercli-ha-restarted-vms:lang:powerclihandler:./handlerimage:harbor.jarvis.lab/veba/veba-powercli-ha-restarted-vms:latestenvironment:write_debug:trueread_debug:truefunction_debug:falsesecrets:- vcconfig-ha-restarted-vmsannotations:topic:com.vmware.vc.HA.ClusterFailoverActionCompletedEvent stack.yml example I have built the function from scratch and pushed it into my local container registry Harbor. If you would like to read more about “How to push your function images to Harbor”, take a look here: Using Harbor with the VMware Event Broker Appliance. After successfully deploying the function to my VEBA the next step is to simulate a host outage to trigger the aforementioned vCenter HA event. ","date":"2020-06-29","objectID":"/post/vsphere-ha-event-notification-function/:4:0","tags":["June2020","VEBA","FaaS"],"title":"VMware Event Broker Appliance - vSphere HA Event Notification Function","uri":"/post/vsphere-ha-event-notification-function/"},{"categories":["VEBA","FaaS","Automation","VMware","Open Source"],"content":"ESXi Kernel Panic command I’m using a nested vSphere Cluster in my Homelab to simulate the outage. Connect to your nested ESXi host via ssh and execute the following command to perform a ESXi Kernel Panic. Please use this command with caution! Danger vsish -e set /reliability/crashMe/Panic 1 And here it goes! The below screenshot from the H5 vSphere client shows the affected host (top left: nesxi67-02.jarvis.lab) as well as the occurred vCenter event which triggers our function. A more detailed view can be done via a terminal. Below you can see the deployed ha-restarted-vms function running as a Kubernetes POD on VEBA (lower left), the ESXi Kernel Panic command (lower right) and the invocation of the HA Restart Function triggered by the occurred vCenter HA event (top right). ","date":"2020-06-29","objectID":"/post/vsphere-ha-event-notification-function/:5:0","tags":["June2020","VEBA","FaaS"],"title":"VMware Event Broker Appliance - vSphere HA Event Notification Function","uri":"/post/vsphere-ha-event-notification-function/"},{"categories":["VEBA","FaaS","Automation","VMware","Open Source"],"content":"You have mail! If everything operates as expected, you should have received an email according to the following screenshot of the one I received. Thanks Bob! Keep up the great work. ","date":"2020-06-29","objectID":"/post/vsphere-ha-event-notification-function/:6:0","tags":["June2020","VEBA","FaaS"],"title":"VMware Event Broker Appliance - vSphere HA Event Notification Function","uri":"/post/vsphere-ha-event-notification-function/"},{"categories":["Monitoring","VEBA","FaaS","Open Source","Cloud Native","VMware"],"content":"This post covers how to monitor the VMware Event Broker Appliance (VM), plus it's Kubernetes components like the Control Plane (Master), the Node(s) as well as the Pods by using vRealize Operations Manager.","date":"2020-05-12","objectID":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/","tags":["May2020","VEBA","Monitoring","vRealize Operations Manager"],"title":"Monitoring the VMware Event Broker Appliance with vRealize Operations Manager","uri":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/"},{"categories":["Monitoring","VEBA","FaaS","Open Source","Cloud Native","VMware"],"content":"Since the VMware Event Broker Appliance (VEBA) is reaching more and more folks out there and the interest is growing steadily, it is important to validate the integration with other solutions (preferably VMware). At least that’s what I thought 😉. Monitoring in terms of utilization and performance captured my interest. So, how can I monitor not only the appliance (virtual machine) but also the Kubernetes components like the Control Plane (Master), the Node(s) as well as the Pods? Furthermore, not only the workload is of interest but also the dependencies of the individual components to each other are interesting. Beyond that and because I’m just getting started, I would also like to have the ability to generate alarms if a desired state is no longer fulfilled. VMware’s vRealize Operations Manager (vROps) and it’s Management Pack for Container Monitoring helped me quickly with my needs. Let me demonstrate you how we can prepare VEBA to send the desired data (metrics) to vROPs and how we import a custom dashboard that provides you with plenty of interesting and useful information. ","date":"2020-05-12","objectID":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/:0:0","tags":["May2020","VEBA","Monitoring","vRealize Operations Manager"],"title":"Monitoring the VMware Event Broker Appliance with vRealize Operations Manager","uri":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/"},{"categories":["Monitoring","VEBA","FaaS","Open Source","Cloud Native","VMware"],"content":"Leveraging Google cAdvisor Repository: https://github.com/google/cadvisor cAdvisor (Container Advisor) will be the component which we will make use of to get resource usage and performance data provided for vROps. It is super easy to deploy and you will see first results quickly. Let me show you how. I’m going to start an nginx container from which we will grab the data from after deploying cAdvisor. docker run --name vmware-nginx -p 8181:80 vmwarecna/nginx The nginx webserver container is running and listening on port 8181 for HTTP requests. Now let’s deploy cAdvisor in it’s latest version also as a docker container (for now). Export the desired version as an environment variable. We will use this variable ($VERSION) for the container deployment. VERSION=v0.35.0 The following shows how the container deployment looks like and important for this example is the usage of the docker option --publish=8080:8080 which will expose the port through which we can reach the cAdvisor website. sudo docker run \\ --volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:ro \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --volume=/dev/disk/:/dev/disk:ro \\ --publish=8080:8080 \\ --detach=true \\ --name=cadvisor \\ gcr.io/google-containers/cadvisor:$VERSION docker ps will show us if both containers are up and running as well as if they are listening on the specified ports: CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ff0d3fad427d gcr.io/google-containers/cadvisor:v0.35.0 \"/usr/bin/cadvisor -…\" 3 minutes ago Up 3 minutes (healthy) 0.0.0.0:8080-\u003e8080/tcp cadvisor 48da3118416f vmwarecna/nginx \"nginx -g 'daemon of…\" 6 minutes ago Up 6 minutes 443/tcp, 0.0.0.0:8181-\u003e80/tcp vmware-nginx Both are up and we will reach the cAdvisor page via localhost:8080. Navigate to the container, in my case the nginx webserver, which performance data we want to see. Here we go: Figure I: cAdvisor provides performance data\" Figure I: cAdvisor provides performance data This is the kind of data that vROps can put into a good looking shape. Now let’s move on to deploy cAdvisor on VEBA. ","date":"2020-05-12","objectID":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/:1:0","tags":["May2020","VEBA","Monitoring","vRealize Operations Manager"],"title":"Monitoring the VMware Event Broker Appliance with vRealize Operations Manager","uri":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/"},{"categories":["Monitoring","VEBA","FaaS","Open Source","Cloud Native","VMware"],"content":"Start the sshd daemon on VEBA To deploy cAdvisor on VEBA, we have to establish a ssh connection to it because we will run the deployment steps directly from within the appliance. The sshd daemon is stopped by default. To start it, open a virtual machine console session (Web or VMware Remote Console) through the vSphere Client and login with your configured root credentials. Start the sshd daemon by executing systemctl start sshd and validate if it’s running properly with systemctl status sshd. Figure II: PhotonOS - start sshd daemon\" Figure II: PhotonOS - start sshd daemon ","date":"2020-05-12","objectID":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/:1:1","tags":["May2020","VEBA","Monitoring","vRealize Operations Manager"],"title":"Monitoring the VMware Event Broker Appliance with vRealize Operations Manager","uri":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/"},{"categories":["Monitoring","VEBA","FaaS","Open Source","Cloud Native","VMware"],"content":"Deploy the cAdvisor as a Kubernetes DaemonSet Environment with internet access Now it’s time to deploy the cAdvisor as a Kubernetes DaemonSet onto VEBA. I’ve used the yaml specifications from the cAdvisor repository which are located in /deploy/kubernetes/base and modified them a bit for my needs. We will have three files at the end, because we are using kustomize in order to create and deploy the namespace and the daemonset for cAdvisor. Establish a ssh connection to your VEBA appliance and change into the /tmp directory. I have set up a repository on Github where I’ve stored the necessary files for the deployment. From here (/tmp) we will clone the repository and deploy the daemonset to the Kubernetes Master subsequently. Run git clone https://github.com/rguske/monitoring-veba.git \u0026\u0026 kubectl apply -k monitoring-veba/. Environment without internet access For air-gapped environments, three simple steps are neccessary: create a new directory in /tmp -\u003e e.g. mkdir /tmp/monitoring-veba create the following three specification files inside the newly created directory: vim kustomization.yaml apiVersion:kustomize.config.k8s.io/v1beta1kind:Kustomizationnamespace:vropscommonLabels:app:vrops-cadvisorresources:- daemonset.yaml- namespace.yaml vim namespace.yaml apiVersion:v1kind:Namespacemetadata:name:vrops vim daemonset.yaml apiVersion:apps/v1kind:DaemonSetmetadata:name:cadvisornamespace:vropsspec:selector:matchLabels:name:cadvisortemplate:metadata:labels:name:cadvisorspec:containers:- name:cadvisorimage:k8s.gcr.io/cadvisor:v0.35.0resources:requests:memory:200Micpu:150mlimits:memory:2000Micpu:300mvolumeMounts:- name:rootfsmountPath:/rootfsreadOnly:true- name:var-runmountPath:/var/runreadOnly:true- name:sysmountPath:/sysreadOnly:true- name:dockermountPath:/var/lib/dockerreadOnly:true- name:diskmountPath:/dev/diskreadOnly:trueports:- name:httpcontainerPort:31194hostPort:31194protocol:TCPsecurityContext:capabilities:drop:- ALLadd:- NET_BIND_SERVICEargs:- --port=31194- --profiling- --housekeeping_interval=1sautomountServiceAccountToken:falseterminationGracePeriodSeconds:30volumes:- name:rootfshostPath:path:/- name:var-runhostPath:path:/var/run- name:syshostPath:path:/sys- name:dockerhostPath:path:/var/lib/docker- name:diskhostPath:path:/dev/disk By executing kubectl apply -k . from within the new folder (/tmp/monitoring-veba), the daemonset will be deployed in a new namespace called vrops. ","date":"2020-05-12","objectID":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/:1:2","tags":["May2020","VEBA","Monitoring","vRealize Operations Manager"],"title":"Monitoring the VMware Event Broker Appliance with vRealize Operations Manager","uri":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/"},{"categories":["Monitoring","VEBA","FaaS","Open Source","Cloud Native","VMware"],"content":"Validate the cAdvisor deployment To ensure that our procedure was correct, we will check the deployment of the pod. root@veba040 [ /tmp/monitoring-veba ]# kubectl -n vrops get pods NAME READY STATUS RESTARTS AGE cadvisor-ccgcx 1/1 Running 0 4m STATUS Running is the desired state which will fulfill our requirements to continue with the import of the vROps Management Pack for Container Monitoring as well as with the configuration of the Kubernetes Adapter which is available after a successful import. ","date":"2020-05-12","objectID":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/:1:3","tags":["May2020","VEBA","Monitoring","vRealize Operations Manager"],"title":"Monitoring the VMware Event Broker Appliance with vRealize Operations Manager","uri":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/"},{"categories":["Monitoring","VEBA","FaaS","Open Source","Cloud Native","VMware"],"content":"A Dashboard to monitor the VMware Event Broker Appliance Before we continue with the configuration steps in vRealize Operations Manager, I wanted to give you a little sneak peek what you can expect by the end. Figure III: vROps Dashboard to monitor VEBA\" Figure III: vROps Dashboard to monitor VEBA ","date":"2020-05-12","objectID":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/:2:0","tags":["May2020","VEBA","Monitoring","vRealize Operations Manager"],"title":"Monitoring the VMware Event Broker Appliance with vRealize Operations Manager","uri":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/"},{"categories":["Monitoring","VEBA","FaaS","Open Source","Cloud Native","VMware"],"content":"Step 1. Import vROps Management Pack for Container Monitoring To enable the capability to monitor Kubernetes Clusters in vRealize Operations Manager, it’s necessary to import a Management Pack for it, which is available for free at the VMware Marketplace. Download the Mangement Pack for Container Monitoring Note: vRealize Operations Management Pack for Container Monitoring 1.4.2 and above will only be interoperable with vRealize Operations Enterprise edition. The older versions of Management Pack continue to work with vRealize Operations Advanced and Enterprise editions. Import the Management Pack in vROps as follows: Go to Administration -\u003e Repository and select ADD/UPGRADE Browse to the folder which contains the vmware-MPforContainerMonitoring-1.4.3-15987816.pak file for the Management Pack and upload it up to vROps It will appear after the installation Configure the Kubernetes Adapter to establish a connection to your VEBA node(s) Info: If you are running more than one VEBA appliance, then you have to repeat the next steps for each instance separately. Go to Other Accounts and select the Kubernetes Adapter Enter the IP address of your VEBA instance followed by port 6443 e.g.https://10.10.14.4:6443 Select DaemonSet as your cAdvisor Service Enter your specified cAdvisor Port 31194 was defined above Add new Credentials for your VEBA instance We need to configure Client Certificate Auth as credential kind. To get the data for Certificate Authority Data, Client Certificate Data as well as for the Client Key Data we need to ssh again into our VEBA appliance. Execute the following to get the data: grep 'client-certificate-data\\|client-key-data\\|certificate-authority-data' ~/.kube/config This is how the output should look like: Figure IV: Client Certificate Auth data grep’ed from kube config\" Figure IV: Client Certificate Auth data grep’ed from kube config Back in vROps, fill out the blank fields as required (Proxy only if needed). Figure V: Client Certificate Auth Credentials config in vROps\" Figure V: Client Certificate Auth Credentials config in vROps The last configuration at this point is to provide the FQDN of your vCenter Server under Advanced Settings. This will give us the possibility to include the virtual machine object itself and not only the Kubernetes components in the dashboard later on. Figure VI: Account Information for VEBA\" Figure VI: Account Information for VEBA Attention: Please do not forget to stop ssh again after finishing the above steps. systemctl stop sshd ","date":"2020-05-12","objectID":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/:2:1","tags":["May2020","VEBA","Monitoring","vRealize Operations Manager"],"title":"Monitoring the VMware Event Broker Appliance with vRealize Operations Manager","uri":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/"},{"categories":["Monitoring","VEBA","FaaS","Open Source","Cloud Native","VMware"],"content":"Step 2. Preparations for the dashboard Before we can import the dashboard and the corresponding views, we need to configure two simple steps first. The creation of a Custom Metric Configuration and a Dynamic Custom Group. VEBA Node Utilization Metric Configuration Go to Administration -\u003e Configuration -\u003e Metric Configurations -\u003e ReskndMetric and select ADD. Enter the name veba_node_utilization and paste the following configuration into the right opened field: \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cAdapterKinds\u003e \u003cAdapterKind adapterKindKey=\"KubernetesAdapter\"\u003e \u003cResourceKind resourceKindKey=\"K8S-Minion\"\u003e \u003cMetric attrkey=\"Cpu|UsagePct|Total\" label=\"CPU Usage Total\" unit=\"%\" yellow=\"80\" orange=\"90\" red=\"99\" link=\"\"/\u003e \u003cMetric attrkey=\"Memory|UsagePct\" label=\"Memory Usage\" unit=\"%\" yellow=\"80\" orange=\"90\" red=\"99\" link=\"\"/\u003e \u003cMetric attrkey=\"Network|Interface:eth0|ReadMb\" label=\"eth0 read\" unit=\"KB\" yellow=\"\" orange=\"\" red=\"\" link=\"\"/\u003e \u003cMetric attrkey=\"Network|Interface:eth0|WriteMb\" label=\"eth0 write\" unit=\"KB\" yellow=\"\" orange=\"\" red=\"\" link=\"\"/\u003e \u003c/ResourceKind\u003e \u003c/AdapterKind\u003e \u003c/AdapterKinds\u003e VEBA openfaas-fn namespace Custom Group The dashboard includes a widget which will display all deployed functions and it’s utilization in terms of CPU and Memory (limit, current, average and maximum). Figure VII: vROps Widget to show function utilization\" Figure VII: vROps Widget to show function utilization I configured a Dynamic Custom Group in vROps to realize this. Like the above step for the VEBA Node Utilization Metric Configuration, this Dynamic Custom Group has to be configured before we import the dashboard. Otherwise, the widget(s) will be empty. Navigate to Administration -\u003e Configuration -\u003e and create a new Group Type called VEBA. The Custom Group can be configured under Environment -\u003e Groups and Applications -\u003e Custom Group. Select the before created Group Type VEBA and click ADD. Configure the Custom Group as shown in Figure VIII: Figure VIII: Dynamic Custom Group for ƒ(x) in namespace openfaas-fn\" Figure VIII: Dynamic Custom Group for ƒ(x) in namespace openfaas-fn ","date":"2020-05-12","objectID":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/:2:2","tags":["May2020","VEBA","Monitoring","vRealize Operations Manager"],"title":"Monitoring the VMware Event Broker Appliance with vRealize Operations Manager","uri":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/"},{"categories":["Monitoring","VEBA","FaaS","Open Source","Cloud Native","VMware"],"content":"Step 3. Import the Dashboard and it’s Views The Dashboard is available for download on VMware {code} Unpack the zip file and import the views first. Dashboards -\u003e select any of the views, click on ACTIONS and select Manage Views. The Import button will appear after clicking on the three points next to the ADD button. Go to the folder where you’ve unpacked the dashboard.zip and select the views.zip. This will import the Views. Similar procedure applies for the dashboard itself. Dashboards -\u003e ACTIONS -\u003e Manage Dashboards -\u003e three points -\u003e Import and this time, select the dashboard.zip. Here we go! Figure IX: vROps Dashboard to monitor VEBA\" Figure IX: vROps Dashboard to monitor VEBA Thanks for reading.","date":"2020-05-12","objectID":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/:2:3","tags":["May2020","VEBA","Monitoring","vRealize Operations Manager"],"title":"Monitoring the VMware Event Broker Appliance with vRealize Operations Manager","uri":"/post/monitoring-the-vmware-event-broker-appliance-with-vrealize-operations-manager/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"…and thus also the Enterprise PKS Management Console (EPMC), which is now inferring named as Tanzu Kubernetes Grid Integrated Edition Management Console (TKGIMC). Quote \"…VMware is announcing important milestones for key products in the VMware Tanzu portfolio, which brings consistency to building, running, and managing code. VMware is the only company that addresses the challenges of application modernization from both the application and the infrastructure perspective.\" Source: https://tanzu.vmware.com/content/blog/consistency-matters-tanzu-application-service-kubernetes Referring to this 👆, this rebranding was necessary to unify VMware’s Kubernetes offerings under the Tanzu umbrella. April 16th, version 1.7 of the production-grade Kubernetes-based container solution VMware Tanzu Kubernetes Grid Integrated Edition as well as the VMware Tanzu Kubernetes Grid Integrated Edition Management Console went GA and I had the pleasure to introduce the updates for TKGIMC on VMware’s official Tanzu Blog: Tanzu Kubernetes Grid Integrated Edition Management Console v.1.7: Improved Support for Multi-cluster Kubernetes Deployments Enjoy! Thanks ","date":"2020-05-01","objectID":"/post/tanzu-kubernetes-grid-integrated-edition-management-console-1-7/:0:0","tags":["April2020","Kubernetes","PKS","EPMC","TKG","TKGIMC"],"title":"VMware Enterprise PKS got rebranded VMware Tanzu Kubernetes Grid Integrated Edition","uri":"/post/tanzu-kubernetes-grid-integrated-edition-management-console-1-7/"},{"categories":["VEBA","FaaS","VMware","Cloud Native"],"content":"Instead of pulling function images from Docker Hub, I'm going to explorer the use of the enterprise container image registry Harbor this time.","date":"2020-03-23","objectID":"/post/using-harbor-with-the-vcenter-event-broker-appliance/","tags":["March2020","VEBA","Harbor","FaaS"],"title":"Using Harbor with the VMware Event Broker Appliance","uri":"/post/using-harbor-with-the-vcenter-event-broker-appliance/"},{"categories":["VEBA","FaaS","VMware","Cloud Native"],"content":"The vCenter Event Broker Appliance (VEBA) is still one of my favorite open source projects these days and it is evolving rapidly and continuously through the great work of the two main contributors Michael Gasch and William Lam as well as through the valuable feedback from the community. I´m very proud to be part of the “inner circle” of folks who meet on a regular basis to discuss everything #VEBA and the keyword here is also feedback. Version 0.3 was recently with big updates announced which can be viewed via the following link: Big updates to the vCenter Event Broker Appliance (VEBA) Fling Just to name a few updates: VMware Event Router implementation AWS EventBridge support New event playload structure - cloudevents.io ","date":"2020-03-23","objectID":"/post/using-harbor-with-the-vcenter-event-broker-appliance/:0:0","tags":["March2020","VEBA","Harbor","FaaS"],"title":"Using Harbor with the VMware Event Broker Appliance","uri":"/post/using-harbor-with-the-vcenter-event-broker-appliance/"},{"categories":["VEBA","FaaS","VMware","Cloud Native"],"content":"Pulling images from Harbor fails I was still testing the new version in my lab but this time I wanted to add another great open source project as a playmate to the round, the enterprise container image registry Harbor. VEBA ist using OpenFaaS® as the built-in event processor and you define the function configuration in a YAML file, the stack.yml. Figure I: Original repository, image name and tag\" Figure I: Original repository, image name and tag A closer look into the stack.yml file shows us that the official VMware repository is used. We only see the repository name vmware/, the image name veba-powercli-tagging as well as the tag :latest. This means that when no registry is configured at this point, the Docker default applies, which is then Docker Hub. Instead of pulling the appropriate image(s) from Docker Hub, I wanted to use Harbor this time. In order to implement this, I have to replace the original image specification in the stack.yml and point to my Harbor instance, the corresponding project and the image with tag. Figure II: Modified repository, image name and tag\" Figure II: Modified repository, image name and tag ","date":"2020-03-23","objectID":"/post/using-harbor-with-the-vcenter-event-broker-appliance/:1:0","tags":["March2020","VEBA","Harbor","FaaS"],"title":"Using Harbor with the VMware Event Broker Appliance","uri":"/post/using-harbor-with-the-vcenter-event-broker-appliance/"},{"categories":["VEBA","FaaS","VMware","Cloud Native"],"content":"Create a project in Harbor Before we continue with the deployment of a function and what I´ve observed when VEBA is trying to pull the images from Harbor, I´d like to give you a short intoduction “How to create a project in Harbor”. Login with admin, select Projects and give it a name. Figure III: New project\" Figure III: New project Go to Members and add a new User to the project. Figure IV: Add a user\" Figure IV: Add a user The last step for now is to mark the project as Public so it is accessible to everyone and no docker login is required before. I also enable the Automatically scan image on push option, to let Clair immediately scan images when they are pushed. Figure V: Project configuration page\" Figure V: Project configuration page ","date":"2020-03-23","objectID":"/post/using-harbor-with-the-vcenter-event-broker-appliance/:2:0","tags":["March2020","VEBA","Harbor","FaaS"],"title":"Using Harbor with the VMware Event Broker Appliance","uri":"/post/using-harbor-with-the-vcenter-event-broker-appliance/"},{"categories":["VEBA","FaaS","VMware","Cloud Native"],"content":"Optional: Authentiction with LDAP The following settings are not required but I wanted to authenticate Active-Directory users and groups to my project(s). I think it could be helpful to see a working configuration. Figure VI: Harbor LDAP Authentication configuration\" Figure VI: Harbor LDAP Authentication configuration If everything is configured correctly, you should be able to add one or more AD groups to your project. Figure VII: Harbor LDAP Authentication configuration\" Figure VII: Harbor LDAP Authentication configuration ","date":"2020-03-23","objectID":"/post/using-harbor-with-the-vcenter-event-broker-appliance/:3:0","tags":["March2020","VEBA","Harbor","FaaS"],"title":"Using Harbor with the VMware Event Broker Appliance","uri":"/post/using-harbor-with-the-vcenter-event-broker-appliance/"},{"categories":["VEBA","FaaS","VMware","Cloud Native"],"content":"Push an image to Harbor For this demonstration I´m going to pick the vSphere Tagging Function example. Let´s create the veba-powercli-tagging image from the Dockerfile which is located in the following directory: vcenter-event-broker-appliance/examples/powercli/tagging/template/powercli/. We can build, tag and push the image in two ways: ","date":"2020-03-23","objectID":"/post/using-harbor-with-the-vcenter-event-broker-appliance/:4:0","tags":["March2020","VEBA","Harbor","FaaS"],"title":"Using Harbor with the VMware Event Broker Appliance","uri":"/post/using-harbor-with-the-vcenter-event-broker-appliance/"},{"categories":["VEBA","FaaS","VMware","Cloud Native"],"content":"The Docker way - docker cli Change into the directory were the Dockerfile is located, create a fresh new image out of it and directly assign it with a tag subsequently: cd ~/vcenter-event-broker-appliance/examples/powercli/tagging/template/powercli/ docker build -t harbor.jarvis.lab/veba/veba-powercli-tagging:latest . docker images REPOSITORY TAG IMAGE ID CREATED SIZE harbor.jarvis.lab/veba/veba-powercli-tagging latest ab7d0277d326 15 seconds ago 368MB Login into Harbor with an assigned user or a user of an assigned group: docker login -u rguske harbor.jarvis.lab/veba After a successful login, push the image to your project: docker push harbor.jarvis.lab/veba/veba-powercli-tagging:latest ","date":"2020-03-23","objectID":"/post/using-harbor-with-the-vcenter-event-broker-appliance/:4:1","tags":["March2020","VEBA","Harbor","FaaS"],"title":"Using Harbor with the VMware Event Broker Appliance","uri":"/post/using-harbor-with-the-vcenter-event-broker-appliance/"},{"categories":["VEBA","FaaS","VMware","Cloud Native"],"content":"The OpenFaaS way - faas-cli The use of the faas-cli makes the above described steps a little bit more comfortable and by the end, the faas-cli is using the native docker commands as well. To build as well as to tag the image, we just need to execute faas-cli build -f stack.yml. This command will pick the image specification from the stack.yml and will hand it over automatically. Same result here: REPOSITORY TAG IMAGE ID CREATED SIZE harbor.jarvis.lab/veba/veba-powercli-tagging latest 3520bc9c8b85 32 seconds ago 368MB You still need to login to Harbor if you want to execute the push command, otherwise you will receive the following error: denied: requested access to the resource is denied unauthorized: authentication required 2020/03/23 17:48:45 ERROR - Could not execute command: [docker push harbor.jarvis.lab/veba/veba-powercli-tagging:latest] Login and execute faas-cli push -f stack.yml. The image will be pushed into your project. Figure VIII: Pushed image to the new project\" Figure VIII: Pushed image to the new project ","date":"2020-03-23","objectID":"/post/using-harbor-with-the-vcenter-event-broker-appliance/:4:2","tags":["March2020","VEBA","Harbor","FaaS"],"title":"Using Harbor with the VMware Event Broker Appliance","uri":"/post/using-harbor-with-the-vcenter-event-broker-appliance/"},{"categories":["VEBA","FaaS","VMware","Cloud Native"],"content":"Deploy a function to VEBA I don´t need to stress this topic in detail here because everything you need to be ready for take off 🚀 with VEBA is well documented on the Github page and if you miss something or need a more comprehensive insight on VEBA, visit this blog series by Patrick Kraemer. Assuming the corresponding image is available in Harbor and the stack.yml is adapted accordingly (see Figure II), you can proceed with the steps provided on Github (LINK). Update the stack.yml and the vc-tag-config.json with your environment information. I´m curious and therefore I established a ssh connection to my VEBA appliance. By executing kubectl -n openfaas-fn get pods, I can see how the deployment of the pod runs and the following is not what we want to see as a result: NAME READY STATUS RESTARTS AGE powercli-tag-5ff88775fb-6g92k 0/1 ImagePullBackOff 0 28s NAME READY STATUS RESTARTS AGE powercli-tag-5ff88775fb-6g92k 0/1 ErrImagePull 0 64s I validated this behavior by executing docker images and observed that no image has been downloaded from Harbor. Troubleshooting often means, if an automatic process fails, go the manual way step by step. Consequently, the next step is to download the image manually. docker pull harbor.jarvis.lab/veba/veba-powercli-tagging:latest Error response from daemon: Get https://harbor.jarvis.lab/v2/: x509: certificate signed by unknown authority Interesting! Failure “certificate signed by unknown authority” This is not based on the fact that I have not done a docker login before, as this is not necessary since we have made our project publicly available. Following the official Docker documentation, this behavior is expected: Verify repository client with certificates To solve this issue, I´ve created a little script which downloads the root certificate from Harbor, creates the relevant directories, puts the certificate into them and restarts the docker service. You can download the script HERE and copy it via scp into your VEBA appliance. scp ~/Downloads/docker_harbor_cert.sh root@veba030:/ Make it executable with chmod +x docker_harbor_cert.sh and run it. The other option without using scp is, to establish a ssh connection and to copy and execute the neccessary lines directly. ","date":"2020-03-23","objectID":"/post/using-harbor-with-the-vcenter-event-broker-appliance/:5:0","tags":["March2020","VEBA","Harbor","FaaS"],"title":"Using Harbor with the VMware Event Broker Appliance","uri":"/post/using-harbor-with-the-vcenter-event-broker-appliance/"},{"categories":["VEBA","FaaS","VMware","Cloud Native"],"content":"The script #! /bin/bash set -euo pipefail # Ask for the Harbor FQDN echo \"Enter the FQDN (e.g. harbor.domain.com) of your Harbor registry:\" read REGISTRY # Create folder for custom certificate as described in Docker docs https://docs.docker.com/engine/security/certificates/ mkdir -p /etc/docker/certs.d/$REGISTRY # Download Registry Root Certificate wget -O /etc/docker/certs.d/$REGISTRY/ca.crt https://$REGISTRY/api/v2.0/systeminfo/getcert --no-check-certificate # Restart Docker service systemctl restart docker VEBA is now able to pull and run the image from Harbor. kubectl -n openfaas-fn get pods NAME READY STATUS RESTARTS AGE powercli-tag-5ff88775fb-wqqqf 1/1 Running 0 84s Thanks for reading. ","date":"2020-03-23","objectID":"/post/using-harbor-with-the-vcenter-event-broker-appliance/:6:0","tags":["March2020","VEBA","Harbor","FaaS"],"title":"Using Harbor with the VMware Event Broker Appliance","uri":"/post/using-harbor-with-the-vcenter-event-broker-appliance/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"Part I - A guide how to setup a VMware Horizon Connection Server, the installation, configuration and preperation of the Linux distributions Ubuntu and CentOS and how you can turn them into a universal workbench for development and operations.","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"The Universal Workbench I often thought about writing down my experiences on the topics of Jumphosts, Shell-modifications and useful tools in a series of blog posts, but I hadn´t found the right moment, until I saw the following Tweet, where Alex asked his followers how they get access to a Linux system if they aren´t running Linux on their workstation (locally). Ubuntu 18.04 VDI desktop — Mark Brookfield (@virtualhobbit) December 28, 2019 Similar to Mark´s answer, I´m also using a VDI (Virtual-Desktop-Infrastructure) desktop for my purposes and I began writing this series…back in January…time passed by and due to other projects I worked on and which demanded a lot of my attention (VEBA 😁), it took a little bit longer to publish them. Thank You Big thanks to Gerrit Lehr, Hilko Lantinga and Alex Lopez for your valuable input and feedback. Having a suitable Jumphost, Remotehost, Development-workstation or however you like to call the system that you connect to, to have access to the appropriate tools and applications available, comes in very handy when you reach the limits or get stuck with your local machine. Examples: [Ops] Deploying an OVA (Open virtualization Appliance) which is located locally on your workstation into your remote Datacenter via VPN isn´t really a good idea. [Dev] I recently had to repeatedly create new versions of an OVA for testing purposes as changes were made to the code. Building the OVA takes time. And another very important aspect of having a “Developer-friendly” environment quickly available comes into play! Let´s consider the rise of Kubernetes for example. Organizations today will perhaps reach a point, where it will be necessary to employ external developers and engineers (e.g. SRE´s, PRE´s) and provide a environment to them, that is on the one hand organization compliant and on the other hand offers sufficient flexibility and appropriate tools to these developers and engineers. With this series of posts, I´d like to demonstrate how you can build such an environment for the described use-cases on your own and what´s necessary to offer it as a VDI desktop to your team or external employers. This will include the setup of a VMware Horizon Connection Server (broker for client connections), the installation, configuration and preperation of the Linux distributions Ubuntu and CentOS and how you can turn them into a universal workbench for development and operations. Please give 3 minutes of your precious time for the following recording to get an idea of what you can expect at the end of these three articles. Video I: SSO and HTML5 feature in VMware Horizon ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:1:0","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"My Mindnode Download on Figure I: Mindnode Development Desktop\" Figure I: Mindnode Development Desktop ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:2:0","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"1. Setup VMware Horizon Connection Server (v.7.11) The VMware Horizon Connection Server is the broker for your client connections which authenticates users through Windows Active Directory and directs the request to the appropriate virtual machine (Figure II). Figure II: VMware Horizon Client authenticating to a Connection Server\" Figure II: VMware Horizon Client authenticating to a Connection Server Source: Digital Workspace Tech Zone - VMware Workspace ONE and VMware Horizon Reference Architecture If all PREREQUISITES are given (Figure I: Mindnode Development Desktop), the installation itself is really quick and simple. Short overview Windows Server VM (W2k8R2 - W2k19) which is part of a MS Domain The Connection Server cannot be installed on a Domain Controller (e.g. for a PoC or a Homelab) Download and install the Connection Server v.7.11 installer (VMware-Horizon-Connection-Server-x86_64-7.11.0.exe / ~250 MB) Select the View Standard Server role Choose a User or a Group (local or AD) to authorize them for Horizon 7 administration That´s it roughly! Therefore, it´s not necessary for me to go into the details here and instead, I´d like to refer you to the following sources: VMware Docs Install Horizon Connection Server with a New Configuration - https://docs.vmware.com/en/VMware-Horizon-7/7.11/horizon-installation/GUID-9F93A59F-C35F-4388-B3D6-CE4F50D8BAFD.html Post by Carl Stalhood Install Horizon 7 Standard Connection Server by - https://www.carlstalhood.com/vmware-horizon-7-connection-server/ ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:3:0","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"2. Install a Linux-VM ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:4:0","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"2.1 Select a Linux Distribution In this post, I´ll concentrate just on Ubuntu 18.04 LTS and CentOS 8.0 but in case that this is not your pick, you´ll find all Linux distributions that are supported for Horizon Agent on page 14 of the official installation guide here:“Setting Up Horizon 7 for Linux Desktops”. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:4:1","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"2.2 Download Linux installation iso Ubuntu 18.04 LTS: https://ubuntu.com/download/desktop/ CentOS 8.0 (at this time only 8.0 is supported!): http://isoredirect.centos.org/centos/8/isos/x86_64/ ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:4:2","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"2.3 Create a new Virtual Machine Now that your iso is ready to be mounted let´s create a new Virtual Machine. Choose your distro from the list and configure your vHardware. I followed the official recommendation for “Improved video playback in a 2D desktop (page 23). Figure III: Linux Distribution \u0026 vHardware Settings\" Figure III: Linux Distribution \u0026 vHardware Settings For CentOS I chose the “Workstation” option. Minimal installation is sufficient for Ubuntu. Figure IV: Installation options CentOS 8.0 \u0026 Ubuntu LTS 18.04\" Figure IV: Installation options CentOS 8.0 \u0026 Ubuntu LTS 18.04 Compared to CentOS, it´s not possible under Ubuntu to make the system settings like e.g. the Network-settings or NTP during the initial phase of the installation. So make sure that you configure everything after Ubuntu is booted for the first time. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:4:3","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"2.4 NTP (Ubuntu) Proper network time synchronization is an essential part of monitoring a network and resolving issues within it! sudo apt install ntp systemctl status ntp timedatectl list-timezones timedatectl set-timezone Europe/Berlin Set your NTP server in the /etc/ntp.conf file. Chose an server pool closest to your location (https://www.ntppool.org/zone) and restart the ntp.service (systemctl restart ntp) after you´ve finished your configuration. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:4:4","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"2.5 SSH (Ubuntu) Activating SSH makes all the configurations, which will follow, a lot easier (copy \u0026 paste) in comparison to using the VMware Remote Console. OpenSSH has to be installed on Ubuntu: Ubuntu: sudo apt install openssh-server CentOS: OpenSSH server is installed and enabled by default. Check that the service is Active and running: systemctl status sshd.service ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:4:5","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"2.6 Packages and module dependencies We´re planning to join our system into a Active Directory Domain and also want to install the VMware Horizon View Agent, which is necessary for adding our machine to a Desktop-Pool later on. Before we can continue, it´s necessary to eliminate some dependencies by installing some packages upfront. Note: The following only applies to CentOS 8.0 with Horizon Agent version 7.11 Do not** run yum update! Normally, the first step I do after setting up a fresh Linux system, is the update of every currently installed package of it (apt update / yum update). You can run the update and upgrade “safely” for Ubuntu but NOT for CentOS 8.0. The VMware Horizon View Agent v.7.11 does not support gnome-shell version 3.3x. If you run the sudo yum update command, the gnome-shell will be updated and this leads to the following error: The version of gnome shell 3.32.2 isn't matched,\\n try to replace default version. Don't support 8.1.1911, disable sso “Disable SSO” isn´t what we want! I also tried to exclude the package update for the GNOME Shell by excluding it as well as Mutter from the yum.conf file… sudo vim /etc/yum.conf exclude=gnome* mutter* …but this will cause the system to boot with a black-screen. I got the internal feedback, that we´ll support CentOS 8.1 (gnome-shell 3.3x) with Horizon v.7.12. [2020-03-17] Update VMware Horizon 7.12 with CentOS 8.1 support released: Release Notes As already mentioned earlier, normally I update my freshly installed system immediately after the first boot. Let´s do this now! Ubuntu: sudo apt -y update \u0026\u0026 sudo apt -y upgrade \u0026\u0026 sudo reboot CentOS: sudo yum update Now that our system has a current state, we will install the needed packages which are necessary to proceed further. Ubuntu: sudo apt -y install open-vm-tools-desktop vim curl git build-essential realmd python python-dbus python-gobject CentOS: sudo dnf -y install sssd oddjob oddjob-mkhomedir adcli samba-common-tools ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:4:6","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"3 Single Sign-On Source: VMware Horizon 7: Setting Up Single Sign-On The Horizon Single Sign-On feature gives us the great ability to connect to our Linux system via the Horizon View Client (Video I \u0026 Figure II) with an, on the Desktop-Pool level, entitled Active Directory user. This also includes the creation of users home directory if the user logs in for the first time. We have to integrate our Desktop with an Active Directory first to make use of SSO. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:5:0","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"3.1 Integrating Ubuntu with Active Directory Source: VMware Horizon 7: PowerBroker Identity Services Open (PBISO) Ubuntu: Pbis-open Releases: https://github.com/BeyondTrust/pbis-open/releases Download and installation: wget https://github.com/BeyondTrust/pbis-open/releases/download/9.1.0/pbis-open-9.1.0.551.linux.x86_64.deb.sh chmod +x pbis-open-9.1.0.551.linux.x86_64.deb.sh sudo ./pbis-open-9.1.0.551.linux.x86_64.deb.sh Domain join: sudo domainjoin-cli --loglevel info join jarvis.lab admin@jarvis.lab Joining to AD Domain: jarvis.lab With Computer DNS Name: devdesk-ubuntu.jarvis.lab admin@JARVIS.LABs password: Warning: System restart required Your system has been configured to authenticate to Active Directory for the first time. It is recommended that you restart your system to ensure that all applications recognize the new settings. SUCCESS If you get the following error during the domain join… Error DNS_ERROR_BAD_PACKET [code 0x0000251e] A bad packet was received from a DNS server. Potentially the requested address does not exist. …then the following applies: “It’s not DNS”…“There’s no way it’s DNS”…It was DNS!. Make sure DNS is set up properly. Default configuration for domain users: sudo /opt/pbis/bin/config UserDomainPrefix jarvis sudo /opt/pbis/bin/config AssumeDefaultDomain true sudo /opt/pbis/bin/config LoginShellTemplate /bin/bash sudo /opt/pbis/bin/config HomeDirTemplate %H/%U Edit the /etc/pam.d/common-session file and replace the line session optional pam_lsass.so with session [success=ok default=ignore] pam_lsass.so, save and quite (wq) and reboot the system. Note: Updating the Horizon Agent will require that this setting must be performed again. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:5:1","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"3.2 Integrating CentOS 8 with Active Directory Source: VMware Horizon 7: Use the Realmd Join Solution for RHEL/CentOS 8.0 To have the ability to connect with an AD user to our desktop, we have to join an Active Directory domain. realmd will be the tool of our choice to join an Active Directory domain. It´s installed on CentOS by default. CentOS: sudo realm discover -vvv jarvis.lab * Resolving: _ldap._tcp.jarvis.lab * Performing LDAP DSE lookup on: 10.10.13.10 * Successfully discovered: jarvis.lab jarvis.lab type: kerberos realm-name: JARVIS.LAB domain-name: jarvis.lab configured: no server-software: active-directory client-software: sssd required-package: oddjob required-package: oddjob-mkhomedir required-package: sssd required-package: adcli required-package: samba-common-tools That looks g00d! So the next step is joining the discovered realm. Execute sudo realm join -v -U admin@JARVIS.LAB JARVIS.LAB and wait until your terminal shows you the result. The -v option will give you a verbose output. sudo realm join -v -U admin@JARVIS.LAB JARVIS.LAB Last to-do at this point is to edit the /etc/sssd/sssd.conf. Add ad_gpo_map_interactive = +gdm-vmwcred, simple_allow_groups = \"Your-AD-User-Group\" under the [domain/domain name] section and set the value for use_fully_qualified_names to = False. Example: sudo vim /etc/sssd/sssd.conf [sssd] domains = jarvis.lab config_file_version = 2 services = nss, pam [domain/jarvis.lab] ad_domain = jarvis.lab krb5_realm = JARVIS.LAB realmd_tags = manages-system joined-with-adcli cache_credentials = True id_provider = ad krb5_store_password_if_offline = True default_shell = /bin/bash ldap_id_mapping = True use_fully_qualified_names = False fallback_homedir = /home/%d/%u access_provider = ad ad_gpo_map_interactive = +gdm-vmwcred simple_allow_groups = labadmins@JARVIS.LAB We are going to use Gnome Classic for the SSO login for CentOS 8.0. Remove all the desktop startup files from the /usr/share/xsessions directory, except the one for Gnome Classic. cd /usr/share/xsessions/ ls -rtl -rw-r--r--. 1 root root 8471 May 15 2019 gnome-custom-session.desktop -rw-r--r--. 1 root root 1303 May 15 2019 gnome.desktop -rw-r--r--. 1 root root 1303 May 15 2019 gnome-xorg.desktop -rw-r--r--. 1 root root 1394 May 17 2019 gnome-classic.desktop sudo mkdir backup sudo mv *.desktop backup/ sudo mv backup/gnome-classic.desktop ./ Note: Updating the Horizon Agent will require that this setting must be performed again. Reboot the system. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:5:2","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"4 Install Horizon View Agent As a next step we´re going to install the VMware View Agent which must be installed on virtual machines that are managed by vCenter Server so that View Connection Server can communicate with them. Download: https://www.vmware.com/go/download-horizon Tip I´ve downloaded the Agent locally first and copied it then via scp into my VM(s). scp /Users/rguske/Downloads/VMware-horizonagent-linux-x86_64-7.11.0-15238356.tar.gz jarvis@devdesk-centos:/home/jarvis/Downloads Unpack the tarball, go into the unpacked directory and execute the install_viewagent.sh script. tar -zxvf VMware-horizonagent-linux-x86_64-7.11.0-15238356.tar.gz cd VMware-horizonagent-linux-x86_64-7.11.0-15238356/ ~/Downloads/VMware-horizonagent-linux-x86_64-7.11.0-15238356$ ./install_viewagent.sh -A yes Regarding the used command options and arguments - https://docs.vmware.com/en/VMware-Horizon-7/7.11/linux-desktops-setup/GUID-09A3F97C-47FE-4ABF-B68C-E42AE26632CC.html#GUID-09A3F97C-47FE-4ABF-B68C-E42AE26632CC Now set the correct SSO Desktop Type for the View Agent (see page 18 and 107). Run sudo vim /etc/vmware/viewagent-custom.conf and uncomment the necessary line for your desktop environment. Ubuntu 18.04 = SSODesktopType=UseGnomeUbuntu CentOS = SSODesktopType=UseGnomeClassic On the way down through the viewagent-custom.conf you came by a point where you can define a subnet for the View-Agent. If you like to install Docker on your desktop, as I would assume, you have to uncomment the example and replace the CIDR with yours. In my case: Subnet=10.10.13.0/26 Otherwise you won´t be able to connect to your desktop anymore after installing Docker. Save and exit the file with :wq. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:6:0","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"5 Create a Manual Desktop Pool Now that the work on the prerequisites for adding a VM to a Horizon Pool is complete, it´s time to setup a “Manual Desktop Pool” for our Desktop. Creating a Manual Desktop Pool is pretty straight forward and well described in VMware´s official documentation. Source: Create a Manual Desktop Pool for Linux Mandatory settings are: Default display protocol: VMware Blast Allow users to choose protocol: No 3D Renderer: Manage using vSphere Client for 2D or vDGA desktop and NVIDIA GRID vGPU for vGPU desktop By the end, it should look similar to what you can see on Figure V. Figure V: Manual desktop pool(s)\" Figure V: Manual desktop pool(s) Since you´ve added the Linux-VM to your new desktop pool, it´ll appear under Machines on the left site. This section will give you a quick overview with some helpful details, like the status and version of the View Agent as well as the DNS name of your system for example. BTW! Regarding DNS…avoid mistakes like the ones I did…😐…Figure VI. Figure VI: DNS Name = Shortname\" Figure VI: DNS Name = Shortname Let´s bring this into the correct form: sudo hostnamectl set-hostname devdesk-ubuntu.jarvis.lab Doublecheck if the changes got applied: hostname -f Way better! 👇 Figure VII: DNS Name = FQDN\" Figure VII: DNS Name = FQDN You should now be able to connect to your Desktop-Pool via the Horizon Client. Thanks for reading. Change Log: [2020-03-18]: Added VMware Horizon 7.12 announcement (CentOS 8.1 support) [2020-03-18]: Updated CentOS 8.0 \u0026 Horizon Agent 7.11 section; Updated Mindmap ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/:7:0","tags":["February2020","Linux","Horizon","VDI"],"title":"A Linux Development Desktop with VMware Horizon - Part I: Horizon","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-i-horizon/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"Part II - In this post I´m going to list a couple of applications which I´m using for my desktop and how you can easily install them from your shell.","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"Applications In this section I´m going to list a couple of applications which I´m using for my desktop and how you can easily install them from your shell. Of course, it´s not a must and it´s up to you which of them you´d like to install. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/:1:0","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"Google Chrome https://www.google.com/chrome/ Firefox is pre-installed on Ubuntu as well as on CentOS but I´m for years now with Chrome and I´m still satisfied. Ubuntu wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb -P ~/Downloads sudo dpkg -i ~/Downloads/google-chrome-stable_current_amd64.deb CentOS wget https://dl.google.com/linux/direct/google-chrome-stable_current_x86_64.rpm -P ~/Downloads sudo yum install ~/Downloads/google-chrome-stable_current_x86_64.rpm ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/:1:1","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"Snap (package manager) https://snapcraft.io/docs This package manager is pre-installed and ready for use on Ubuntu but not on CentOS. Nevertheless, we can install it subsequently via the Extra Packages for Enterprise Linux. Add the EPEL repository: sudo dnf install epel-release Install and enable snapd: sudo yum install snapd sudo systemctl enable --now snapd.socket sudo ln -s /var/lib/snapd/snap /snap The command line interface snap is ready for use: snap --help The snap command lets you install, configure, refresh and remove snaps. Snaps are packages that work across many different Linux distributions, enabling secure delivery and operation of the latest apps and utilities. Commands can be classified as follows: Basics: find, info, install, list, remove ...more: refresh, revert, switch, disable, enable History: changes, tasks, abort, watch Daemons: services, start, stop, restart, logs Commands: alias, aliases, unalias, prefer Configuration: get, set, unset, wait Account: login, logout, whoami Permissions: connections, interface, connect, disconnect Snapshots: saved, save, check-snapshot, restore, forget Other: version, warnings, okay, ack, known, model, create-cohort Development: run, pack, try, download, prepare-image ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/:1:2","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"Visual Studio Code (VSCode) https://code.visualstudio.com/ VSCode is a powerful open source cross-platform editor and definetely one of my absolute favorite tools so far and indispensable for our desktop. Option 1: Installation via snap sudo snap install code --classic Option 2: Step by Step Ubuntu Repository and key: curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor \u003e packages.microsoft.gpg sudo install -o root -g root -m 644 packages.microsoft.gpg /usr/share/keyrings/ sudo sh -c 'echo \"deb [arch=amd64 signed-by=/usr/share/keyrings/packages.microsoft.gpg] https://packages.microsoft.com/repos/vscode stable main\" \u003e /etc/apt/sources.list.d/vscode.list' Install: sudo apt-get install apt-transport-https \u0026\u0026 sudo apt-get update \u0026\u0026 sudo apt-get install code CentOS Repository and key: sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc sudo sh -c 'echo -e \"[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\" \u003e /etc/yum.repos.d/vscode.repo' Install: sudo dnf check-update sudo dnf install code ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/:1:3","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"[Extension] VSCode Settings sync https://marketplace.visualstudio.com/items?itemName=Shan.code-settings-sync VSCode has a REALLY rich extensibility model which gives you everything you need to work even more efficiently and faster. It´ll not be possible to avoid that more and more extensions and other niceties are added to your installation and as of today it´s not possible to have everything consistent across different systems or platforms through an Online-Account like you have e.g. for Google Chrome. Thus I´d like to recommend using the VSCode Settings sync extension for it. It uses your GitHub account (required) token and Gist, to give you the ability to upload as well as download your settings, snippets, themes and so forth. Just search for the extension at the marketplace, install it, select LOGIN WITH GITHUB (get redirected) and follow the instructions. Since you´ve created your Gist-ID you´re ready to go to upload as well as to download your settings. Shortcuts Upload Key : Shift + Alt + U Download Key : Shift + Alt + D (on macOS: Shift + Option + U / Shift + Option + D) Set VSCode as default editor for Git Open your terminal and execut: git config --global core.editor “code --wait” Source: https://git-scm.com/book/en/v2/Customizing-Git-Git-Configuration ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/:1:4","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"Remmina https://remmina.org/ Remmina is a Remote Desktop Client which supports multiple network protocols like e.g. RDP, VNC, SPICE, NX, XDMCP and SSH. sudo snap install remmina Figure I: Remmina - Remote Desktop Client\" Figure I: Remmina - Remote Desktop Client ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/:1:5","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"Content sharing I was looking for a workspace where I can share content, mainly files like scripts or specification-files (.yml, .json) across platforms. It´s possible by default to activate Online Accounts (Figure II) like e.g. Google or Nextcloud in both distributions through the System Settings. Figure II: Activate Online Accounts\" Figure II: Activate Online Accounts ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/:2:0","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"Dropbox for Linux https://www.dropbox.com/en/install-linux I decided to try Dropbox due to the availability of an already existing account. Ubuntu Dropbox Headless: cd ~ \u0026\u0026 wget -O - \"https://www.dropbox.com/download?plat=lnx.x86_64\" | tar xzf - Installing the Dropbox CLI: sudo wget -O /usr/local/bin/dropbox \"https://www.dropbox.com/download?dl=packages/dropbox.py\" sudo chmod +x /usr/local/bin/dropbox dropbox --help Dropbox command-line interface commands: autostart automatically start Dropbox at login exclude ignores/excludes a directory from syncing filestatus get current sync status of one or more files help provide help lansync enables or disables LAN sync ls list directory contents with current sync status proxy set proxy settings for Dropbox puburl get public url of a file in your Dropbox's public folder running return whether Dropbox is running sharelink get a shared link for a file in your Dropbox start start dropboxd status get current status of the dropboxd stop stop dropboxd throttle set bandwidth limits for Dropbox update download latest version of Dropbox version print version information for Dropbox Enable Dropbox to start automatically after every reboot: sudo vim /etc/systemd/system/dropbox.service [Unit] Description=Dropbox Service After=network.target [Service] ExecStart=/bin/sh -c '/usr/local/bin/dropbox start' ExecStop=/bin/sh -c '/usr/local/bin/dropbox stop' PIDFile=${HOME}/.dropbox/dropbox.pid User=jarvis Group=jarvis Type=forking Restart=on-failure RestartSec=5 StartLimitInterval=60s StartLimitBurst=3 [Install] WantedBy=multi-user.target Reload system daemon: sudo systemctl daemon-reload Enable the new Dropbox service: sudo systemctl enable dropbox Start the service: sudo systemctl start dropbox CentOS Dropbox Headless: cd ~ \u0026\u0026 wget -O - \"https://www.dropbox.com/download?plat=lnx.x86_64\" | tar xzf - sudo mkdir -p /opt/dropbox sudo cp -r .dropbox-dist/* /opt/dropbox By executing /opt/dropbox/dropboxd you should be forwarded to the Dropbox homepage where you have to login to authenticate the connection request. After this, quite the running Dropbox on your terminal with ctrl + c. Creating the init.d file: sudo curl -o /etc/init.d/dropbox https://gist.githubusercontent.com/thisismitch/6293d3f7f5fa37ca6eab/raw/2b326bf77368cbe5d01af21c623cd4dd75528c3d/dropbox Creating the systemd unit file: sudo curl -o /etc/systemd/system/dropbox.service https://gist.githubusercontent.com/thisismitch/6293d3f7f5fa37ca6eab/raw/99947e2ef986492fecbe1b7bfbaa303fefc42a62/dropbox.service Set the appropriate permissions for the newly created files: sudo chmod +x /etc/systemd/system/dropbox.service /etc/init.d/dropbox Add users to be able to run Dropbox: sudo vim /etc/sysconfig/dropbox DROPBOX_USERS=\"rguske\" Reload system daemon: sudo systemctl daemon-reload Enable the new Dropbox service: sudo systemctl enable dropbox Start the service: sudo systemctl start dropbox ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/:2:1","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"Installing the Dropbox CLI sudo wget -O /usr/local/bin/dropbox \"https://www.dropbox.com/download?dl=packages/dropbox.py\" sudo chmod +x /usr/local/bin/dropbox ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/:3:0","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"Docker Engine - Community Another must! Option 1: Installation via snap sudo snap install docker Option 2: Step by Step Ubuntu: sudo apt -y install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null sudo apt update sudo apt -y install docker-ce docker-ce-cli containerd.io Manage Docker as a non-root user The usergroup docker should be created after the installation. If not, run: sudo groupadd docker Add your $USER to the group by executing sudo usermod -aG docker $USER. CentOS: sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 sudo dnf install https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce docker-ce-cli sudo systemctl enable --now docker systemctl status docker.service sudo usermod -aG docker $USER After adding your user to the docker group, you have to logout and login first, so that your group membership is re-evaluated. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/:3:1","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"Docker Compose https://docs.docker.com/compose/ A tool for deploying multi-container applications which services are specified in a YAML file. sudo curl -L \"https://github.com/docker/compose/releases/download/1.25.3/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/:3:2","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"VMware Remote Console Download: https://www.vmware.com/go/download-vmrc ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/:3:3","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"VMware OVF Tool Download: https://code.vmware.com/web/tool/4.3.0/ovf These two binaries were downloaded locally (to my Notebook) first, then got packed into a zip archive and copied via scp to my desktop(s). scp ~/Downloads/tools.zip jarvis@devdesk-centos:/home/jarvis/Downloads Unpack afterwards: unzip tools.zip Note: Installing the OVF Tool for CentOS requires the installation of the ncurses-compat-libs package first. sudo dnf install ncurses-compat-libs Make them executable and start the installation: chmod +x VMware-ovftool-4.3.0-14746126-lin.x86_64.bundle sudo ./VMware-ovftool-4.3.0-14746126-lin.x86_64.bundle --console --required --eulas-agreed chmod +x VMware-Remote-Console-11.0.0-15201582.x86_64.bundle sudo ./VMware-Remote-Console-11.0.0-15201582.x86_64.bundle --console --required --eulas-agreed ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/:3:4","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware"],"content":"Enpass https://www.enpass.io/ A secure vault which is available for all mobile and desktop platforms. Ubuntu: sudo -i echo \"deb https://apt.enpass.io/ stable main\" \u003e \\/etc/apt/sources.list.d/enpass.list wget -O - https://apt.enpass.io/keys/enpass-linux.key | apt-key add - exit sudo apt update \u0026\u0026 sudo apt install enpass CentOS: cd /etc/yum.repos.d/ sudo wget https://yum.enpass.io/enpass-yum.repo sudo yum install enpass cd ~ Thanks for reading. Change Log: [2020-03-18]: Added Snap (package manager); Remmina (Remote Desktop Client); Section “Content sharing”; Dropbox client ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/:3:5","tags":["February2020","Linux","Horizon","VDI","Application"],"title":"A Linux Development Desktop with VMware Horizon - Part II: Applications ","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-ii-applications/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"Part III - A guide on the \"Highway\" to Shell.","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"High ⚡ Way to Shell The operationalization of platforms such as e.g. Kubernetes, or the use of tools for building services or applications such as Docker, requires in both cases the use of the command-line. There are dozens of great plugins, themes and extensions out there to pimp your shell so that it´ll help you to increase velocity as well as useability. Part III of my series is focused on the Shell and which “Highway” you can take to have the described tool available at the end. Figure I: Default Terminal appearance before tuning\" Figure I: Default Terminal appearance before tuning ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:1:0","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"1. ZSH aka Z shell http://www.zsh.org/ ZSH is a extended shell with lots of features, support for plugins and themes. Ubuntu: sudo apt install zsh CentOS: Should be installed by default. sudo yum install zsh ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:2:0","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"2. Oh My ZSH https://ohmyz.sh/ A community-driven framework for managing your Z shell configuration. Ubuntu: sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" CentOS: sh -c \"$(wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" Verify if ZSH is set as your new default Shell: echo $SHELL cat /etc/passwd | grep \"username\" ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:3:0","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"2.1 ZSH Syntax Highlighting https://github.com/zsh-users/zsh-syntax-highlighting This plugin enables highlighting of commands whilst they are typed and it´ll help you to avoid syntax erros before you run a command. CentOS \u0026 Ubuntu: Let´s clone the repository into our oh-my-zsh plugins directory: git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting Enable the plugin in your zsh config file (~/.zshrc) in the “plugins section”. plugins=(git pks zsh-syntax-highlighting) Restart your terminal. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:3:1","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"2.2 Install Powerline Fonts https://github.com/powerline/fonts Needed fonts for the Powerlevel9k theme (step 4.). Ubuntu: sudo apt install fonts-powerline CentOS: git clone https://github.com/powerline/fonts.git --depth=1 cd fonts ./install.sh cd .. rm -rf fonts ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:3:2","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"2.3 Powerlevel9k https://github.com/Powerlevel9k/powerlevel9k This theme will give your shell a new shine. CentOS \u0026 Ubuntu: git clone https://github.com/bhilburn/powerlevel9k.git ~/.oh-my-zsh/custom/themes/powerlevel9k vim ~/.zshrc Replace the default ZSH_THEME with the new Powerlevel9k theme: ZSH_THEME=\"powerlevel9k/powerlevel9k\" The Powerlevel9k theme gives you some pretty neat customization options which makes your terminal even more powerful. Here´s my configuration: POWERLEVEL9K_SHORTEN_DIR_LENGTH=3 POWERLEVEL9K_SHORTEN_DELIMITER=”” POWERLEVEL9K_PROMPT_ON_NEWLINE=true POWERLEVEL9K_SHORTEN_STRATEGY=”truncate_from_right” POWERLEVEL9K_TIME_BACKGROUND=blue POWERLEVEL9K_DATE_BACKGROUND=cyan POWERLEVEL9K_RIGHT_PROMPT_ELEMENTS=(kubecontext time date) POWERLEVEL9K_LEFT_PROMPT_ELEMENTS=(user dir vcs) And you should also paste the following variable to the top of your .zshrc file, otherwise you will get an error when using tmux later. export TERM=\"xterm-256color\" A logout or reboot of the system is necessary for the changes to take effect. The result: Figure II: ZSH with Powerlevel-9k Theme | l: Ubuntu r: CentOS*\" Figure II: ZSH with Powerlevel-9k Theme | l: Ubuntu r: CentOS* I love it! And it doesn´t only looks good, it also comes in very handy if you work e.g. with Git as you can see on Figure II for example. The right screenshot shows you that I am in a git branch called featurebranch and the color indicates me, that my workingtree is either clean and theres nothing to commit (green) or that new files were added and needs to be commited (orange). More! More! More! ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:3:3","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"3 Homebrew on Linux 🍺 https://docs.brew.sh/Homebrew-on-Linux Homebrew is a package manager (like Snappy) which simplifies the installation of software on your OS and should not be missing if you ask me. CentOS \u0026 Ubuntu: sh -c \"$(curl -fsSL https://raw.githubusercontent.com/Linuxbrew/install/master/install.sh)\" The installation-script will throw a Warning at the end, because it points out that /home/linuxbrew/.linuxbrew/bin isn´t in your PATH. Add the path to your .zshrc dot-file. vim ~/.zshrc export PATH=$PATH:\"/home/linuxbrew/.linuxbrew/bin\" source ~/.zshrc ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:4:0","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4 CLI´s and tools …I´m using and I´d like to recommend kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-linux Info A command line tool for controlling Kubernetes clusters. brew install kubectl ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:0","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.1 ZSH Autocompletion for kubectl https://v1-16.docs.kubernetes.io/docs/reference/kubectl/cheatsheet/#kubectl-autocomplete source \u003c(kubectl completion zsh) echo \"if [ $commands[kubectl] ]; then source \u003c(kubectl completion zsh); fi\" \u003e\u003e ~/.zshrc ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:1","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.2 Powershell https://snapcraft.io/powershell Info PowerShell is an cross-platform (Windows, Linux, and macOS) command-line shell for automation and configuration management. Installation via snap: sudo snap install powershell --classic ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:2","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.3 KinD - Kubernetes in Docker https://github.com/kubernetes-sigs/kind Info kind is a tool for running local Kubernetes clusters using Docker container “nodes”. Figure III: kind create cluster\" Figure III: kind create cluster ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:3","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.4 Octant https://github.com/vmware-tanzu/octant Info Octant is a tool for developers to understand how applications run on a Kubernetes cluster. Source: https://github.com/vmware-tanzu/octant ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:4","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.5 tmux https://github.com/tmux/tmux Info tmux is a terminal multiplexer. Enable Mouse-scrolling: Scrolling the terminal pages by using the mouse-wheel is natural for me and because it´s not enabled by default when using tmux I need to enable it. Settings like this for example can be applied while running tmux. Press ctrl + b and then type :set -g mouse on. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:5","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.6 bat https://github.com/sharkdp/bat Info bat is a cat clone with syntax highlighting and Git integration. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:6","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.7 glances https://github.com/nicolargo/glances Info glances is a cross-platform monitoring tool. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:7","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.8 htop https://github.com/hishamhm/htop Info htop is an interactive process viewer. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:8","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.9 ctop https://github.com/bcicen/ctop Info ctop provides a concise and condensed overview of real-time metrics for multiple containers: ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:9","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.10 prettyping https://github.com/denilsonsa/prettyping Info prettyping is a wrapper around the standard ping tool with the objective of making the output prettier, more colorful, more compact, and easier to read. And all of them can be easily installed via brew: brew install kind octant tmux bat glances htop ctop prettyping Addtionally to htop, ytop (former gotop) is a pretty neat process and system monitor. ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:10","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.11 ytop https://github.com/cjbassi/ytop brew tap cjbassi/ytop \u0026\u0026 brew install ytop Figure IV: tmux, htop, ctop, prettyping \u0026 ytop\" Figure IV: tmux, htop, ctop, prettyping \u0026 ytop ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:11","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.12 figlet http://www.figlet.org/ ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:12","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.13 lolcat https://github.com/busyloop/lolcat brew install figlet lolcat figlet cliFun | lolcat _ _ _____ ___| (_) ___| _ _ __ / __| | | |_ | | | | '_ \\ | (__| | | _|| |_| | | | | \\___|_|_|_| \\__,_|_| |_| ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:13","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.14 cowsay You should have a look at this post regarding cowsay 😄: “cowsay is the Most Important Unix-like Command Ever” brew install cowsay cowsay -f vader I am your...LOL! __________________ \u003c I am your...LOL! \u003e ------------------ \\ ,-^-. \\ !oYo! \\ /./=\\.\\______ ## )\\/\\ ||-----w|| || || Cowth Vader Last but not least! ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:14","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"4.15 Asciinema https://asciinema.org/ A perfect way how you can easily record your terminal sessions. brew install asciinema asciinema rec ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:5:15","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["Linux","VDI","Desktop","VMware","Terminal"],"content":"5 Shell Aliases Put the following aliases for the individual commands to the end of your .zshrc file tmux alias tmuxinit=\"tmux new-session -n init\" alias c=clear alias k=kubectl alias w='watch -n1' alias ping=prettyping alias cat=bat You´re good to go - Thanks for reading. Change Log: [2020-03-10]: Added Enable Mouse-scrolling for tmux; ZSH Syntax Highlighting; Powerlevel9k configuration [2020-03-10]: Updated Figure II [2020-03-18]: Added Glances \u0026 Powershell ","date":"2020-02-27","objectID":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/:6:0","tags":["February2020","Linux","Horizon","VDI","Tools","Terminal"],"title":"A Linux Development Desktop with VMware Horizon - Part III: Shell","uri":"/post/a-linux-development-desktop-with-vmware-horizon-part-iii-shell/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"The Enterprise PKS Management Console really improves the overall Day-1 and Day-2 experience by greatly simplifying the deployment process of Enterprise PKS (Day-1) and additionally providing a rich set of features around Visibility, User Management and Upgrades/Patches (Day-2).","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Info VMware Enterprise PKS got rebranded VMware Tanzu Kubernetes Grid Integrated Edition See: https://rguske.github.io/post/tanzu-kubernetes-grid-integrated-edition-management-console-1-7/ Version 1.5.0 of VMware´s turnkey solution VMware Enterprise PKS, which brings production-grade Kubernetes to enterprises, was released back in August this year. This release introduced the beta (v.0.9) of the VMware Enterprise PKS Management Console (EPMC) which offers an opinionated installation of Enterprise PKS. With the PKS minor release v.1.5.1, the EPMC went GA in version 1.0.0. Further improvements and enhancements were made in version 1.6 (*1), which went GA on the 26th of November, a few days after the release of Enterprise PKS v.1.6 and after testing it, I´d like to share my impressions with you through this post. Note *: Enterprise PKS Management Console version number equals now the version number of Enterprise PKS. Thanks Keith Lee for reviewing both articles. ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:0:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Let´s start with my conclusion first The Enterprise PKS Management Console makes the adjective TURNKEY, which VMware uses when introducing Enterprise PKS, really well-rounded. It really improves the overall Day-1 and Day-2 experience by greatly simplifying the deployment process of Enterprise PKS (Day-1) and additionally providing a rich set of features around Visibility, User Management and Upgrades/Patches (Day-2). ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:1:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"It´s an appliance EPMC will run as a Virtual Appliance in your Datacenter and can be downloaded through the provided link at the end of this post. It´s a 10,6 GB download and it´s that heavy because it´s all packaged together in one OVA, which in turn has the advantage, that you don´t have to work out the interoperability matrix. Included bits are: Pivotal Ops Manager BOSH PKS instance Harbor Stemcells When you think about Dark Site installations, it brings the benefit, that it´s just one download and you don´t have to care about the different pieces with the right version anymore. Version Details: Element Details EPMC version v1.6.0 Release date November 26, 2019 Installed Enterprise PKS version v1.6.0 Installed Ops Manager version v2.7.3 Installed Kubernetes version v1.15.5 Installed Harbor Registry version v1.9.3 Supported NSX-T versions v2.4.1, v2.4.2, 2.5.0 Source: Release Notes Enterprise PKS 1.6 Let´s have a look at the artifacts.yaml file to see the different component versions. ssh root@epmc.jarvis.lab root@epmc [ /storage/data/artifacts ]# ls artifacts.yaml harbor-container-registry ops-manager pivotal-container-service stemcells-ubuntu-xenial root@pksmgmt [ /storage/data/artifacts ]# cat artifacts.yaml opsman: name: ops-manager version: 2.7.3 path: /storage/data/artifacts/ops-manager/2.7.3/ops-manager-vsphere-2.7.3-build.208.ova harbor: name: harbor-container-registry version: 1.9.3-build.2 path: /storage/data/artifacts/harbor-container-registry/1.9.3-build.2/harbor-container-registry-1.9.3-build.2.pivotal pks: name: pivotal-container-service version: 1.6.0-build.17 path: /storage/data/artifacts/pivotal-container-service/1.6.0-build.17/pivotal-container-service-1.6.0-build.17.pivotal pks-stemcell: name: pks-stemcell version: \"456.51\" path: /storage/data/artifacts/stemcells-ubuntu-xenial/456.51/bosh-stemcell-456.51-vsphere-esxi-ubuntu-xenial-go_agent.tgz harbor-stemcell: name: harbor-stemcell version: \"456.51\" path: /storage/data/artifacts/stemcells-ubuntu-xenial/456.51/bosh-stemcell-456.51-vsphere-esxi-ubuntu-xenial-go_agent.tgz bosh: name: bosh version: NA path: NA root@epmc [ /storage/data/artifacts ]# Figure I: Enterprise PKS Management Console component versions ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:2:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Prerequisites As always, before we come to the exciting part, we have to deal with the requirements as well as the prerequisuits in advance. The requirements for the Appliance are 2 vCPUs, 8 GB of vRAM and 40 GB of Diskspace. Since the Appliance must reach the vCenter Server as well as the NSX Manager, it should be assigned to a network with connectivity to the same. Furthermore, we have to define two Availability-Zones. From a vSphere point of view, an AZ can be a Cluster, a Host-Group or a Resource Pool. One AZ is for the PKS Management Plane components and the other one (at least) for the Kubernetes cluster Nodes. I´m running just one vSphere-Cluster in my Homelab and I´m using Resource-Pools to provide the two AZ´s. IMPORTANT: The Resource-Pools should exist before starting with the PKS Configuration through the EPMC! Otherwise they will not show up if you configure them on-the-fly. Means, you have to restart from the beginning. Avoid this, trust me 😉. When it comes to Networking, EPMC offers three kinds of flavors: Bring-Your-Own-Topology (BYOT): Assumes that NSX-T is already preconfigured for the use of Ent. PKS. Flannel: EPMC will provision a Flannel container networking interface during the Ent. PKS deployment. and now my favorite… the Automated NAT Deployment to NSX-T Data Center: This option will deploy Ent. PKS fully automated and ready-for-use configured in NAT mode, which includes the creation and configuration of the T0, T1 routers as well as the necessary IP Blocks for the Kubernetes Nodes and PODs. End-to-End! As always, I recommend reading the Prerequisite-sections in the documentation: ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:3:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Prereq for vSphere https://docs.vmware.com/en/VMware-Enterprise-PKS/1.6/vmware-enterprise-pks-16/GUID-console-prereqs-vsphere.html ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:3:1","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Prereq for an Automated NAT Deployment https://docs.vmware.com/en/VMware-Enterprise-PKS/1.6/vmware-enterprise-pks-16/GUID-console-prereqs-nsxt-automatednat.html ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:3:2","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Nine simple steps After having finished the OVA deployment and the Appliance is successfully booted, you can reach the Login-page by entering the FQDN or the IP address into your Browser. BTW: If you want to re-ensure that the appliance configuration has been correctly submitted during OVA provisioning, simply connect via ssh to the appliance and check the environment file. root@epmc [ /etc/vmware ]# cat environment APPLIANCE_SERVICE_UID=10000 HOSTNAME=epmc.jarvis.lab IP_ADDRESS=10.10.14.3 FILESERVER_PORT=443 APPLIANCE_ID=2tsKp8PbqZeR6ON5G5IhNZJnknET4fA+JGXvl6xyHJE= APPLIANCE_TLS_CERT= APPLIANCE_TLS_PRIVATE_KEY= APPLIANCE_TLS_CA_CERT= APPLIANCE_PERMIT_ROOT_LOGIN=True NETWORK_FQDN=epmc.jarvis.lab NETWORK_IP0=10.10.14.3 NETWORK_NETMASK0=255.255.255.192 NETWORK_GATEWAY=10.10.14.62 NETWORK_DNS=10.10.13.10 10.10.13.62 NETWORK_SEARCHPATH=jarvis.lab LOGINSIGHT_HOST=10.10.13.5 LOGINSIGHT_PORT=9000 LOGINSIGHT_ID=039daa6c-3d83-46a8-8b44-db273e9d11a5 Figure II: Appliance configuration Login with the assigned credentials. Figure III: Enterprise PKS Management Console Login Page\" Figure III: Enterprise PKS Management Console Login Page On the next page click on Install to continue with the wizard-driven installation or on Upgrade to perform an upgrade of the EPMC (including all the new Enterprise PKS bits). Figure IV: Install or Upgrade Option\" Figure IV: Install or Upgrade Option The first cool capability already comes on the next page, because you can choose between Start Configuration, which lets the Deployment-Wizard followed by, or Import Configuration File were we can upload an existing YAML configuration file from a previous deployment. Figure V: Start or Import a Configuration\" Figure V: Start or Import a Configuration ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:4:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Step 1: vCenter Account Step One is the connection establishment to our vCenter Server which will be our deployment target. Just enter the FQDN/ IP, provide valid credentials of a user with the appropriate vSphere administrator permissions, wait for the validation and you will recognize that the Datacenter field will be auto-populated. Figure VI: vCenter Server Configuration\" Figure VI: vCenter Server Configuration ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:4:1","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Step 2: Networking I´m not going to explain all three options in detail here. What I am very curious and excited about was the Automated NAT deployment option, which is an End-to-End PKS deployment in NAT mode (T0, T1, IP blocks) including NSX Certificates generation. If you choose this deployment-method too and in case you have configured an NSX-T Edge for the use of Enterprise PKS with NAT rules before, make sure you clean them out to prevent any conflicts, because EPMC will automatically create all NAT rules for you. Proof-of-Concept Deployments: The well documented prereqs for NSX-T describing how VMware recommends an Enterprise-ready NSX-T deployment with the necessary pieces deployed in High-Availability mode. Luckily, these prereqs are not “hard coded” and thus it doesn´t apply for Proof-of-Concept deployments. With regard to, one NSX Manager node and one Edge Node is sufficient. Uplink Network Like you´ve already done in the previous step for the vCenter Server before, just do the same here and provide the credentials for the administrative user. The fields for the NSX-T Edges will be auto-populated too. Select the one you intend to use and specify the appropriate Uplink Network specifications. Figure VII: NSX Manager and PKS Edge configuration\" Figure VII: NSX Manager and PKS Edge configuration Network Resources In the Network Resources section, specify the different subnets as well as network services like DNS and NTP for the following resources: Deployment: Basically, the PKS Management Plane components like OpsManager, BOSH, the PKS instance as well as Harbor. POD IP Block: Used by the Kubernetes PODs. IMPORTNAT! Maximum suffix is 24. Node IP Block: Used by the Kubernetes Master, and Worker-Nodes. IMPORTNAT! Maximum suffix of 22. Floating IPs: Used for SNAT IP addresses whenever a Namespace is created (NAT mode) as well as for Load Balancers which will be automatically provisioned by NSX-T, including the load balancer fronting the PKS API server and load balancers for pod ingress. I use self-signed certificates in my environment and for this reason I´ve toggled the radio button Disable SSL certificates verification to allow unsecured connections to the NSX Manager. At this point, I´d also like to point out to the documentation regarding the automated generation of certificates: Info If NSX-T Data Center uses custom certificates and you do not provide the CA certificate for NSX Manager, Enterprise PKS Management Console automatically generates one and registers it with NSX Manager. This can cause other services that are integrated with NSX Manager not to function correctly. Source: https://docs.vmware.com/en/VMware-Enterprise-PKS/1.6/vmware-enterprise-pks-16/GUID-console-deploy-ent-pks-wizard.html Figure VIII: Deployment, POD IP Block, Node IP Block and Floating IP range configuration\" Figure VIII: Deployment, POD IP Block, Node IP Block and Floating IP range configuration ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:4:2","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Step 3: Identity Specify in this Step how you would like to add Users and Groups to Enterprise PKS. EPMC offers three option here: Create Users and Groups and assign roles to those which will be stored into the UAA database: Figure IX: Identity configuration - UAA\" Figure IX: Identity configuration - UAA or Add an External LDAP Server and define the User search base and the Group search base: Figure X: Identity configuration - LDAP\" Figure X: Identity configuration - LDAP or you can use a SAML Identity Provider like e.g. Okta or Azure Active Directory To finish this step, enter the address for the PKS API Server like pks.jarvis.lab in my case. Optionally, you can use OpenID Connect to authenticate users who access Kubernetes clusters with kubectl by using e.g. Okta as an external IDP. This will give administrators the ability to grant access on an namespace or cluster-wide level to Kubernetes end users. ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:4:3","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Step 4: Availability Zones Availability Zones (AZ) are one of four layers of fault tolerance in VMware Enterprise PKS. As a design-recommendation, you should separate the PKS Management Plane from the Kubernetes cluster. This can easily be done by tagging the first AZ as the Management AZ. Click on Save Availability Zone and add another one for the Kubernetes cluster(s) with the recommendation to deploy more than one, for high-availability purposes. Figure XI: Availability Design for VMware Enterprise PKS\" Figure XI: Availability Design for VMware Enterprise PKS Source: VMware VVD5.1 | Availability Design for Enterprise PKS with NSX-T Workload Domains As mentioned at the beginning of this post (Prerequisites), I created two Resource Pools and gave them the appropriate description. az-mgmt and az-prod-1. Figure XII: Availability Zones\" Figure XII: Availability Zones ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:4:4","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Step 5: Resources \u0026 Storage As you can see on the screenshot Figure XII, we have to configure three Resources now. Resources in this step is meant to be a) the storage for the service virtual machines which are as needed for compilation purposes (during installation, upgrade, and operation) by Ent. PKS, called Ephemeral Storage as well as b) the Kubernetes Persistent Volume Storage were all the vmdks will be stored for the Kubernetes persistent volumes (stateful applications) AND optionally c) the Permanent Storage for Enterprise PKS data. If you let this option default (radio switch off), the same selected datastore for Ephemeral Storage will be used for it. Figure XIII: Resources \u0026 Storage Configuration\" Figure XIII: Resources \u0026 Storage Configuration ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:4:5","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Step 6: Plans Plans are configuration sets defining the sizes as well as the deployment target (AZ) of the Kubernetes clusters. The EPMC provides by default three “T-Shirt sizes”: Small, Medium and Large. I won´t go into details regarding the other per plan available options here, however I´d like to recommend reading the documentation (Resources section). Figure XIV: Specification - PKS Plan Small\" Figure XIV: Specification - PKS Plan Small Figure XV: PKS Plans as T-Shirt sizes\" Figure XV: PKS Plans as T-Shirt sizes ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:4:6","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Step 7: Integrations The integration of vRealize Operations Manager, vRealize Log Insight and Wavefront for monitoring, logging and enterprise observability can be all together done in Step 7. To monitor your Kubernetes clusters with vROps, it is necessary to import the Management Pack for Container Monitoring. A description of how to import the Management Pack as well as some more details on how it works (cAdvisor), can be found through this POST by my friend Alex. Figure XVI: Specify logging, monitoring and other integrations\" Figure XVI: Specify logging, monitoring and other integrations New in EPMC version 1.1.0 is the ability to attach a by PKS created Kubernetes cluster to VMware Tanzu Mission Control (TMC) 🚀. Tanzu Mission Control provides you the ability to handle multi-cluster Kubernetes deployments across multiple clouds with operational consistency. More to find here: https://blogs.vmware.com/cloudnative/2019/08/26/vmware-tanzu-mission-control/ Figure XVII: Integration into Tanzu Mission Control\" Figure XVII: Integration into Tanzu Mission Control ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:4:7","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Step 8: Harbor Running production grade Kubernetes in your organization requires the use of a container image registry as well. Harbor is where your container images found a place which they can call “Home”. Configure it in Step 8. Figure XVIII: Harbor Enterprise Container Registry configuration\" Figure XVIII: Harbor Enterprise Container Registry configuration ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:4:8","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Step 9: CEIP Please stay a moment here and read the following blog post from Pivotal regarding the Enhanced Telemetry for PKS before you proceed further. https://content.pivotal.io/blog/announcing-enhanced-telemetry-for-pks Also: https://docs-pcf-staging.cfapps.io/pks/1-6/telemetry.html#sample-reports Figure XIX: Harbor Enterprise Container Registry configuration\" Figure XIX: Harbor Enterprise Container Registry configuration Validate your deployment config and click on Generate Configuration. Figure XX: Generate configuration\" Figure XX: Generate configuration ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:4:9","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Let the magic happen You can see now that every configuration we made in the past 9 steps, is available and still editable in an in-browser text editor. Export the configuration as an YAML file to have it availavle for further deployments (see Figure V: Start or Import an Configuration). Click APPLY CONFIGURATION Figure XXI: Configuration as YAML editable and exportable\" Figure XXI: Configuration as YAML editable and exportable Observe the Deployment status and rejoice the green hooks. Figure XXII: Deployment status page\" Figure XXII: Deployment status page ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:4:10","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Troubleshooting If you are running into troubles with the Enterprise PKS Management Console, you can connect to it via ssh (root) and execute the following command to obtain the server log: Either: journalctl -u pks-mgmt-server \u003e server.log and than cat server.log Or: journalctl -u pks-mgmt-server.service -f to see more details like the following indicates: Nov 27 21:51:50 epmc.jarvis.lab pks-mgmt-server[939]: time=\"2019-11-27T21:51:50Z\" level=info msg=\"Update for deployment 08fd9eeb-2ada- 42c0-8d41-dba9a1b9a2e6 - Deploy harbor-container-registry 1.9.3-build.2: state( running) configuring Harbor product on ops_manager\\n\" Nov 27 21:51:50 epmc.jarvis.lab pks-mgmt-server[939]: configuring product... Nov 27 21:51:55 epmc.jarvis.lab pks-mgmt-server[939]: setting up networkfinished setting up networksetting propertiesfinished setting propertiesapplying resource configuration for the following jobs:errands are not provided, nothing to do here Nov 27 21:51:56 epmc.jarvis.lab pks-mgmt-server[939]: time=\"2019-11-27T21:51:56Z\" level=info msg=\"Update for environment, _harbor.depl oyment_config : **********\" Nov 27 21:51:56 epmc.jarvis.lab pks-mgmt-server[939]: finished configuring producttime=\"2019-11-27T21:51:56Z\" level=info msg=\"Update for deployment 08fd9eeb-2ada-42c0-8d41-dba9a1b9a2e6 - Deploy harbor-container-registry 1.9.3-build.2: state( running) installing Harbor product on ops_manager\\n\" Nov 27 21:51:57 epmc.jarvis.lab pks-mgmt-server[939]: attempting to apply changes to the targeted Ops Manager Nov 27 21:52:02 epmc.jarvis.lab pks-mgmt-server[939]: time=\"2019-11-27T21:52:02Z\" level=info msg=\"Update for deployment 08fd9eeb-2ada-42c0-8d41-dba9a1b9a2e6 - Deploy harbor-container-registry 1.9.3-build.2: state( running) Installing BOSH\\n\" Nov 27 21:52:38 epmc.jarvis.lab pks-mgmt-server[939]: time=\"2019-11-27T21:52:38Z\" level=info msg=\"Update for deployment 08fd9eeb-2ada-42c0-8d41-dba9a1b9a2e6 - Deploy harbor-container-registry 1.9.3-build.2: state( running) Putting Tile Credentials into CredHub\\n\" Nov 27 21:52:50 epmc.jarvis.lab pks-mgmt-server[939]: time=\"2019-11-27T21:52:50Z\" level=info msg=\"Update for deployment 08fd9eeb-2ada-42c0-8d41-dba9a1b9a2e6 - Deploy harbor-container-registry 1.9.3-build.2: state( running) Updating runtime configs for harbor-container-registry\\n\" Nov 27 21:54:33 epmc.jarvis.lab pks-mgmt-server[939]: time=\"2019-11-27T21:54:33Z\" level=info msg=\"Update for deployment 08fd9eeb-2ada-42c0-8d41-dba9a1b9a2e6 - Deploy harbor-container-registry 1.9.3-build.2: state( running) Installing VMware Harbor Registry\\n\" Nov 27 22:08:57 epmc.jarvis.lab pks-mgmt-server[939]: time=\"2019-11-27T22:08:57Z\" level=info msg=\"Update for deployment 08fd9eeb-2ada-42c0-8d41-dba9a1b9a2e6 - Deploy harbor-container-registry 1.9.3-build.2: state( running) Running errand smoke-testing for VMware Harbor Registry\\n\" Nov 27 22:11:38 epmc.jarvis.lab pks-mgmt-server[939]: time=\"2019-11-27T22:11:38Z\" level=info msg=\"Failed to read channel\\n\" Nov 27 22:11:38 epmc.jarvis.lab pks-mgmt-server[939]: time=\"2019-11-27T22:11:38Z\" level=info msg=\"Update for deployment 08fd9eeb-2ada-42c0-8d41-dba9a1b9a2e6 - Deploy harbor-container-registry 1.9.3-build.2: state( running) Cleaning up BOSH director\\n\" Nov 27 22:11:38 epmc.jarvis.lab pks-mgmt-server[939]: time=\"2019-11-27T22:11:38Z\" level=info msg=\"Update for environment, _harbor.is_deployed : true\" Nov 27 22:11:38 epmc.jarvis.lab pks-mgmt-server[939]: time=\"2019-11-27T22:11:38Z\" level=info msg=\"Update for deployment 08fd9eeb-2ada-42c0-8d41-dba9a1b9a2e6 - Deploy harbor-container-registry 1.9.3-build.2: state( running) checking harbor liveness\\n\" Nov 27 22:11:41 epmc.jarvis.lab pks-mgmt-server[939]: time=\"2019-11-27T22:11:41Z\" level=info msg=\"Update for deployment 08fd9eeb-2ada-42c0-8d41-dba9a1b9a2e6 - Deploy harbor-container-registry 1.9.3-build.2: state( success) finished deploying Harbor\\n\" When it comes to BOSH realted issues, also have a look at this post by my colleague Keith Lee: PKS Troubleshooting – Part 1:","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:5:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Resources Site URL VMware Docs EPMC https://docs.vmware.com/en/VMware-Enterprise-PKS/1.6/vmware-enterprise-pks-16/GUID-console-console-index.html Pivotal Docs EPMC http://docs-pcf-staging.cfapps.io/pks/1-6/console/deploy-console-ova.html VMware Release Notes https://docs.vmware.com/en/VMware-Enterprise-PKS/1.6/rn/VMware-Enterprise-PKS-16-Release-Notes.html VMware VVD 5.1 - PKS https://docs.vmware.com/en/VMware-Validated-Design/5.1/sddc-architecture-and-design-for-vmware-enterprise-pks-with-vmware-nsx-t-workload-domains/ EPMC YAML Configuration import https://docs.vmware.com/en/VMware-Enterprise-PKS/1.6/vmware-enterprise-pks-16/GUID-console-deploy-ent-pks-yaml.html VMware Docs - PKS Control Plane https://docs.vmware.com/en/VMware-Enterprise-PKS/1.6/vmware-enterprise-pks-16/GUID-control-plane.html Download via VMware https://my.vmware.com/web/vmware/info/slug/infrastructure_operations_management/vmware_enterprise_pks/1_6 Download via Pivotal https://network.pivotal.io/products/pivotal-container-service Download Pivotal CLI https://network.pivotal.io/products/pivotal-container-service ","date":"2019-11-27","objectID":"/post/pks-management-console-day-1/:6:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 1","uri":"/post/pks-management-console-day-1/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"I covered the Day-1 capabilities of the new version of the Enterprise PKS Management Console (EPMC) in my previous post. Let´s have a look at what´s in for Day-2.","date":"2019-11-27","objectID":"/post/pks-management-console-day-2/","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 2","uri":"/post/pks-management-console-day-2/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"In the previous blog post we went through the Day-1 capabilities of the new version of the Enterprise PKS Management Console (EPMC). Let´s have a look at what´s in for Day-2. ","date":"2019-11-27","objectID":"/post/pks-management-console-day-2/:0:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 2","uri":"/post/pks-management-console-day-2/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Summary Page The summary page of your Enterprise PKS deployment will appear after clicking the Enterprise PKS button at the top left corner. This page will give you an detailed overview of all the PKS Management Plane components including important data like, Service State, Version Numbers, IP addresses and also Hyperlinks either to the relevant Virtual Machine in vSphere or to the Website of an Management-Component like, NSX-T or the Pivotal Ops Manager. Figure I: Enterprise PKS deployment summary\" Figure I: Enterprise PKS deployment summary ","date":"2019-11-27","objectID":"/post/pks-management-console-day-2/:1:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 2","uri":"/post/pks-management-console-day-2/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Identity Management Do you remember the following? uaac user add jarvis --emails jarvis@jarvis.lab -p Password0815 With EPMC you can simply add users and groups through the Identity Management tab. Click on Add Users and fill out the required fields. In this version of EPMC, you cannot add individual LDAP or SAML users. Instead of, you have to use the Groups tab for LDAP or SAML authentication. Note This release of Enterprise PKS Management Console does not support assigning roles to individual LDAP or SAML users. To assign roles to LDAP or SAML users, use user groups. Source: https://docs.vmware.com/en/VMware-Enterprise-PKS/1.6/vmware-enterprise-pks-16/GUID-console-console-identity-management.html Figure II: Identity Management\" Figure II: Identity Management There are two roles which can be assigned. The Cluster Manager, to manage the lifecycle of the individual Kubernetes Clusters and the PKS Administrator, which manage the Enterprise PKS infrastructure. Depending on what you have configured in Step 3 (Identity) during your deployment configuration, you can enter the distinguished name of an existing LDAP group (cn=,ou=,dc=) or the name of your SAML identity provider group. Figure III: Identity Management\" Figure III: Identity Management ","date":"2019-11-27","objectID":"/post/pks-management-console-day-2/:2:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 2","uri":"/post/pks-management-console-day-2/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Deployment Metadata This section is kind of an “Documentation Page” for your Enterprise PKS deployment. You will find all the important Metadata beginning with IP-addresses, FQDNs, credentials (username \u0026 password), certificates up to the Enterprise PKS UAA Encryption Passphrase. Figure IV: Deployment Metadata\" Figure IV: Deployment Metadata Just as an example what´s also in this section. You can use the EPMC as a Jumphost for making use of the BOSH CLI to operationalise PKS. The necessary environment variables, which have to be exported, can be found in the Metadata section under the entry BOSH CLI invocation from console appliance, as you can see on Figure IV. Once you have exported the environment variables, you can issue commands like: bosh vms to get the state of your deployment or bosh tasks --recent=20 to see the last 20 recent tasks of BOSH. Figure V: ssh´d into EPMC | export env variables (Metadata Section) | show deployment state\" Figure V: ssh´d into EPMC | export env variables (Metadata Section) | show deployment state ","date":"2019-11-27","objectID":"/post/pks-management-console-day-2/:3:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 2","uri":"/post/pks-management-console-day-2/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Kubernetes cluster creation To create a new Kubernetes cluster in Enterprise PKS, you have to make use of the PKS CLI. Login into the PKS API Endpoint with a previously created user, which is in my example the svcpksmgr to which I have assigned the Cluster Manager role, and execute the following: pks login -a pks.jarvis.lab -u svcpksmgr -p VMware1! --ca-cert pks.crt* Tip The PKS certificate can be found via the Metadata tab. After a successful login, create a new Kubernetes cluster in the desired “T-Shirt size” (--plan). pks create-cluster k8s-small --external-hostname k8s-small.jarvis.lab --plan small You can use watch -n 2 pks cluster k8s-small to watch the creation-progress of your new K8s-cluster. Figure VI: Kubernetes cluster deployment\" Figure VI: Kubernetes cluster deployment ","date":"2019-11-27","objectID":"/post/pks-management-console-day-2/:4:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 2","uri":"/post/pks-management-console-day-2/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Kubernetes cluster information Again! The Enterprise PKS Management Console brings tons of benefits to operators, by beeing the single pane of glass for your Enterprise PKS/ your Kubernetes environment. The following screenshot gives you an idea what I mean. Figure VII: Kubernetes cluster Overview\" Figure VII: Kubernetes cluster Overview You can see, that besides the creation date of your cluster, the version of it, the count of Master, Worker and Namespaces OR Storage related information, it also shows you all relevant details regarding Networking like: the assigned Network Profile, the connected T0 and T1-Router, the Logical Switch, Load Balancer information as well as Node and POD networking data. By selecting Open Kubernetes Dashboard or Access Cluster, a new window appears in both cases with the necessary command-lines in it, which you can either use to get access to the K8s-Dashboard or the Cluster. Otherwise, you can pass it on to the colleague who is eagerly waiting for it. Figure VIII: Access Kubernetes Cluster and Dashboard\" Figure VIII: Access Kubernetes Cluster and Dashboard If you have multiple Kubernetes clusters across multiple Availability Zones deployed, you will get the right synopsis on the Nodes tab. Figure IX: Kubernetes Master \u0026 Worker\" Figure IX: Kubernetes Master \u0026 Worker Having an understanding of how Enterprise PKS works with NSX-T is crucial. I don´t want to stress this out here, but as a reminder: Note Each Logical Switch will have a Tier1 Logical Router as a Default Gateway. For the Tier1-Cluster Router, the first available subnet from Nodes-IP-Block will be used with the first IP address as the Default Gateway. For all Kubernetes Name Spaces, a subnet will be derived from the Pods-IP-Block A SNAT entry will be configured in the Tier0 Logical Router for each Name Space using the IP-Pool-VIPs since it is the Routable one in the enterprise network. Figure X: Logical diagram for created Logical Routers and Switches\" Figure X: Logical diagram for created Logical Routers and Switches Source: https://blogs.vmware.com/networkvirtualization/2019/06/kubernetes-and-vmware-enterprise-pks-networking-security-operations-with-nsx-t-data-center.html/ The Namespace tab provides you the Hyperlinks to the NSX-T Logical Switches (POD Network) as well as the T1-Router were all the Kubernetes Namespaces are connected to. Figure XI: Kubernetes Namespaces\" Figure XI: Kubernetes Namespaces ","date":"2019-11-27","objectID":"/post/pks-management-console-day-2/:5:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 2","uri":"/post/pks-management-console-day-2/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Upgrade Enterprise PKS + Kubernetes Upgrading your VMware Enterprise PKS Management Plane components as well as your Kubernetes cluster(s) can also be done via the EPMC. If a new version of PKS is available, you´ll be notified by a message at the top. But before you can run the upgrade, you have to migrate your deployment to the newer version of the EPMC first. Therefore: Download the new version via https://downloads.vmware.com Deploy the OVA If you deployed your old EPMC with a static IP and you would like to re-use it for the new version, just Shutdown the old appliance, reconfigure the vApp with a temporary IP address and Power on the appliance again (this should be done before rolling out the new one 😉) Login in into the new EPMC and select this time Upgrade. See Figure IV: Install or Upgrade Option Figure XII: Upgrade Enterprise PKS Management Console\" Figure XII: Upgrade Enterprise PKS Management Console BTW: As you can see on the screenshot above and below, I´ve duplicated the tab of my browser to highlight a pretty neat feature of the EPMC. By clicking the ? icon, which can be found at the top bar as well as in some of the sections, a Contextual help will appear. ","date":"2019-11-27","objectID":"/post/pks-management-console-day-2/:6:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 2","uri":"/post/pks-management-console-day-2/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"VMware Enterprise PKS Component Patches Unfortunately, this section currently applies to the Enterprise PKS Management Console only. It´s planned to have the ability to patch the PKS Management Plane components in a future release too. For now applies, if an patch for the EPMC is available, you will be notified via a message at the top bar: There are new patches to the VMware Enterprise PKS components. Figure XIII: VMware Enterprise PKS Component Patches\" Figure XIII: VMware Enterprise PKS Component Patches Thanks for reading. ","date":"2019-11-27","objectID":"/post/pks-management-console-day-2/:7:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 2","uri":"/post/pks-management-console-day-2/"},{"categories":["VMware","Kubernetes","Cloud Native"],"content":"Resources Site URL VMware Docs EPMC https://docs.vmware.com/en/VMware-Enterprise-PKS/1.6/vmware-enterprise-pks-16/GUID-console-console-index.html Pivotal Docs EPMC http://docs-pcf-staging.cfapps.io/pks/1-6/console/deploy-console-ova.html VMware Release Notes https://docs.vmware.com/en/VMware-Enterprise-PKS/1.6/rn/VMware-Enterprise-PKS-16-Release-Notes.html VMware VVD 5.1 - PKS https://docs.vmware.com/en/VMware-Validated-Design/5.1/sddc-architecture-and-design-for-vmware-enterprise-pks-with-vmware-nsx-t-workload-domains/ EPMC YAML Configuration import https://docs.vmware.com/en/VMware-Enterprise-PKS/1.6/vmware-enterprise-pks-16/GUID-console-deploy-ent-pks-yaml.html VMware Docs - PKS Control Plane https://docs.vmware.com/en/VMware-Enterprise-PKS/1.6/vmware-enterprise-pks-16/GUID-control-plane.html Download via VMware https://my.vmware.com/web/vmware/info/slug/infrastructure_operations_management/vmware_enterprise_pks/1_6 Download via Pivotal https://network.pivotal.io/products/pivotal-container-service Download Pivotal CLI https://network.pivotal.io/products/pivotal-container-service ","date":"2019-11-27","objectID":"/post/pks-management-console-day-2/:8:0","tags":["November2019","Kubernetes","PKS","EPMC","vSphere","TKG"],"title":"VMware Enterprise PKS Management Console (EPMC) - Day 2","uri":"/post/pks-management-console-day-2/"},{"categories":["VMware"],"content":"This post documents a large set of existing Microsites from VMware.","date":"2019-09-24","objectID":"/post/vmware-microsites-you-should-bookmark/","tags":["September2019","VMware"],"title":"VMware Microsites You should bookmark","uri":"/post/vmware-microsites-you-should-bookmark/"},{"categories":["VMware"],"content":"The idea behind this post or collection of links was born as follows: Quote Me: …it´s on vSphere Central. Customer: On which site? Me: vSphere Central! Never heard of it? Customer: I know Storagehub, but I´ve never heard of vSphere Central. This is basically how the conversation with the customer went and I told him that I will provide a collection of all the VMware Microsites I know, together in a post. Info What is a Microsite? Microsites are typically used in web design to add a specialized group of information either editorial or commercial. Such sites may be linked in to a main site or not or taken completely off a site’s server when the site is used for a temporary purpose. The main distinction of a microsite versus its parent site is its purpose and specific cohesiveness as compared to the microsite’s broader overall parent website. Source: https://en.wikipedia.org/wiki/Microsite I will keep this site up-to-date as good as I can. If you have any other site(s) to add, please put it/ them in the comments below 👇. ","date":"2019-09-24","objectID":"/post/vmware-microsites-you-should-bookmark/:0:0","tags":["September2019","VMware"],"title":"VMware Microsites You should bookmark","uri":"/post/vmware-microsites-you-should-bookmark/"},{"categories":["VMware"],"content":"List of Microsites [Updated: January 2022] ","date":"2019-09-24","objectID":"/post/vmware-microsites-you-should-bookmark/:1:0","tags":["September2019","VMware"],"title":"VMware Microsites You should bookmark","uri":"/post/vmware-microsites-you-should-bookmark/"},{"categories":["VMware"],"content":"General resources Site URL Link VMware Careers https://careers.vmware.com/ Open Find your Digital Transformation Path https://pathfinder.vmware.com/ Open VMware Documentation https://docs.vmware.com/ Open VMware Solution Download https://downloads.vmware.com/ Open VMware Knowledge Base https://kb.vmware.com/ Open VMware Technical Papers https://www.vmware.com/techpapers.html Open VMware Developers https://developer.vmware.com/ Open VMware Product Interoperability Matrix https://interopmatrix.vmware.com/ Open VMware Product Lifecycle Matrix https://lifecycle.vmware.com/ Open VMware Configuration Maximums https://configmax.vmware.com/ Open Ports and Protocols https://ports.vmware.com/ Open VMware Solution Walkthrough https://featurewalkthrough.vmware.com/ Open VMware {code} https://code.vmware.com/ Open VMware Marketplace https://marketplace.cloud.vmware.com/ Open vExpert Portal https://vexpert.vmware.com/ Open VMware Certified Design Experts https://vcdx.vmware.com/ Open VMware Blogs https://blogs.vmware.com/ Open Office of the CTO Blog https://octo.vmware.com/ Open VMware TAM Service Portal https://tamservices.vmware.com/ Open ","date":"2019-09-24","objectID":"/post/vmware-microsites-you-should-bookmark/:1:1","tags":["September2019","VMware"],"title":"VMware Microsites You should bookmark","uri":"/post/vmware-microsites-you-should-bookmark/"},{"categories":["VMware"],"content":"VMware Multi-Cloud Site URL Link VMware Core Tech Zone https://core.vmware.com/ Open VMware Cloud Services https://cloud.vmware.com/ Open VMware Cloud Provider https://cloudsolutions.vmware.com/ Open VMware Cloud Services Status Page https://status.vmware-services.io/ Open VMware Cloud Services - GovCloud Status Page https://status.gov.vmware-services.io/ Open ","date":"2019-09-24","objectID":"/post/vmware-microsites-you-should-bookmark/:1:2","tags":["September2019","VMware"],"title":"VMware Microsites You should bookmark","uri":"/post/vmware-microsites-you-should-bookmark/"},{"categories":["VMware"],"content":"VMware Application Modernization Site URL Link VMware Application Modernization https://tanzu.vmware.com/ Open App Modernization https://www.vmware.com/uk/app-modernization.html Open ","date":"2019-09-24","objectID":"/post/vmware-microsites-you-should-bookmark/:1:3","tags":["September2019","VMware"],"title":"VMware Microsites You should bookmark","uri":"/post/vmware-microsites-you-should-bookmark/"},{"categories":["VMware"],"content":"VMware vSAN Site URL Link VMware Core Tech Zone https://core.vmware.com/ Open vSAN Ready Node Sizer https://vsansizer.vmware.com/ Open ","date":"2019-09-24","objectID":"/post/vmware-microsites-you-should-bookmark/:1:4","tags":["September2019","VMware"],"title":"VMware Microsites You should bookmark","uri":"/post/vmware-microsites-you-should-bookmark/"},{"categories":["VMware"],"content":"VMware vRealize Site URL Link vRealize Product Walkthroughs https://vrealize.vmware.com/ Open vRealize Operations Manager Sizing https://vropssizer.vmware.com/ Open vRealize Log Insight Sizing Calculator https://vrlisizer.vmware.com/ Open Learn Cloud Assembly https://learncloudassembly.github.io/ Open Learn Service Broker https://learnservicebroker.github.io/ Open Learn SaltStack Config https://learnsaltstackconfig.github.io/ Open ","date":"2019-09-24","objectID":"/post/vmware-microsites-you-should-bookmark/:1:5","tags":["September2019","VMware"],"title":"VMware Microsites You should bookmark","uri":"/post/vmware-microsites-you-should-bookmark/"},{"categories":["VMware"],"content":"VMware Education Site URL Link VMware Education https://mylearn.vmware.com/ Open VMware Hands-on Labs https://labs.hol.vmware.com/ Open VMware Customer Connect Learning https://learning.customerconnect.vmware.com Open VMware Tanzu Developer Center Hand-On Workshops https://tanzu.vmware.com/developer/workshops/ Open VMware Tanzu ModernApps Ninja https://modernapps.ninja/ Open Kubernetes Academy by VMware https://kubernetes.academy/ Open VMware Podcasts on Soundcloud https://soundcloud.com/vmware Open Der deutschsprachige VMware Podcast https://der-deutschsprachige-vmware-podcast-v2.zencast.website/ Open ","date":"2019-09-24","objectID":"/post/vmware-microsites-you-should-bookmark/:1:6","tags":["September2019","VMware"],"title":"VMware Microsites You should bookmark","uri":"/post/vmware-microsites-you-should-bookmark/"},{"categories":["VMware"],"content":"VMware Digital Workspace (EUC) Site URL Link VMware Digital Workspace Tech Zone https://techzone.vmware.com/ Open VMware WorkspaceONE Status Page https://status.workspaceone.com/ Open ","date":"2019-09-24","objectID":"/post/vmware-microsites-you-should-bookmark/:1:7","tags":["September2019","VMware"],"title":"VMware Microsites You should bookmark","uri":"/post/vmware-microsites-you-should-bookmark/"},{"categories":["VMware"],"content":"More VMware Sites [Updated: August 2021] Site URL Link VMware TAM Lab https://www.youtube.com/c/VMwareTAMLab Open VMware Tanzu Mission Control https://k8s.vmware.com/tanzu-mission-control/ Open VMworld On-Demand Video Library https://videos.vmworld.com/ Open VMware Open Source https://github.com/vmware Open User Group Communities https://community.vmug.com/ Open VMworld Homepage https://vmworld.com/ Open Change Log: [2022-01-07]: added VMware Careers [2022-01-07]: added VMware Technical Papers [2022-01-07]: added Learn Cloud Assembly [2022-01-07]: added Learn Service Broker [2022-01-07]: added Learn SaltStack Config [2021-08-25]: added VMware Developers [2021-08-25]: added VMware Product Lifecycle Matrix [2021-08-25]: added VMware Product Interoperability Matrix [2021-04-26]: added VMware TAM Lab YouTube channel [2021-04-26]: added VMware Podcasts on Soundcloud [2021-04-26]: added Der deutschsprachige VMware Podcast [2021-04-26]: added VMware Tanzu ModernApps Ninja [2021-04-26]: added VMware Tanzu Developer Center Hand-On Workshops [2021-04-26]: added VMware Customer Connect Learning [2021-04-26]: added vRealize Log Insight Sizing Calculator [2021-04-26]: added VMware Cloud Services Status Page [2021-04-26]: added VMware Cloud Services - GovCloud Status Page [2021-04-26]: added VMware WorkspaceONE Status Page [2021-02-12]: added VMware Tanzu Developer Center Hand-On Workshops [2020-10-07]: added VMware Core Tech Zone [2020-03-13]: added App Modernization in a Multi-Cloud World [2020-03-13]: added VMware Tanzu site ","date":"2019-09-24","objectID":"/post/vmware-microsites-you-should-bookmark/:2:0","tags":["September2019","VMware"],"title":"VMware Microsites You should bookmark","uri":"/post/vmware-microsites-you-should-bookmark/"},{"categories":["Container","VMware","Automation"],"content":"A guide through the various steps to consume vic-machine and every specific action of it in a simplified fashion AND have it running in a container pulled out of Harbor or from another repository of your choice.","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"Introduction The deployment of a Virtual Container Host (VCH) can be done in various ways. One easy way is to use the vSphere-Client (H5) plugin which gives you the ability to configure all necessary parameters e.g. for network, storage or deployment target through a step-by-step deployment wizard. The far more efficient variant in my opinion is making use of the vic-machine utility, a Command Line Interface which is part of the vSphere Integrated Containers Engine bundle. Why do I think it is way better? Because it´s reproducible, independent from a browser, automatable as well as better documentable. You can find some of my vic-machine action examples in my Github repo here: https://github.com/rguske/vic-machine I had a lot of VMware-internal and public facing presentations as well as many demos on VIC over the past months and all based, more or less, on the following vic-machine arguments: rguske@rguske-a01\u003e ./vic-machine-darwin create \\ ### vic-machine binary + action --name vch-example \\ ### Name of the Virtual Conatiner Host --syslog-address udp://10.10.10.10:514 \\ ### Syslog server --target vcenter.example.org/Datacenter \\ ### vSphere Datacenter --user administrator@vsphere.local \\ ### User with appropriate privileges --compute-resource /Datacenter/host/Cluster \\ ### vSphere Cluster or single ESXi Host --image-store vsanDatastore \\ ### Datastore to deposite the images --volume-store vsanDatastore/volumes:default \\ ### Datastore for the Docker Volumes (VMDKs) --bridge-network ls-vic-bridge \\ ### vDS dPort Group for the Container Bridge Network --bridge-network-range 172.16.0.0/12 \\ ### Network range for the Bridge Network --public-network ls-vic-public \\ ### vDS dPort Group for the Public Network --dns-server 10.10.10.254 \\ ### DNS Server address for the Public Network --public-network-ip=10.10.10.11/24 \\ ### Static IP address for the Virtual Container Host --public-network-gateway 10.10.10.254 \\ ### Gateway for the Public network --tls-cname=vch-example.example.org \\ ### Certificate Common Name (CN) for the VCH --certificate-key-size 3072 \\ ### Key size for the auto-generated certificates --organization example.org \\ ### Certificate Organization (O) field --registry-ca=/folder path/ca.crt \\ ### Path to the VIC (Harbor) ca.crt file --thumbprint 4C:92:D7:FC:F8:8D:5A:9F:.... \\ ### vCenter Server Thumbprint --affinity-vm-group \\ ### Automatically creates a VM/Host Group (cVM/ VCH Group) --ops-user vic-devops-admin@example.org \\ ### Defines the user which is used by the VCH to interact with vSphere --ops-grant-perms ### Automatically grants the necessary vSphere permissions Constantly recurring executions should be automated or at least easier to handle …and thus, I was looking for a way how to apply this for vic-machine. ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:1:0","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"Credits goes out to Ben Ben Corrie - the father of project vSphere Integrated Containers Engine - developed a hack back in 2017 which was born out of his own frustrations with vic-machine, because it takes a lot of arguments for each action like create, configure or debug for example. He mentioned this in his video from VMworld 2017 👇 With this post I´d like to guide you through the various steps to consume vic-machine and every specific action of it in a simplified fashion AND have it running in a container pulled out of Harbor or from another repository of your choice. To have Harbor as a cloud native registry deeply coupled and running inside of VIC is simply great. If you haven´t heard of Harbor yet, check it out: https://goharbor.io/ A short look into the innards of the VIC appliance shows us that all components of Harbor, including Clair for vulnerability scanning as well as Notary for image signing, are running as a container inside of Harbor: docker ps -a | grep goharbor Figure I: VIC components running as containers\" Figure I: VIC components running as containers ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:2:0","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"1. The Hack The prerequisites for this hack are that you have Git as well as Docker installed on your local machine or on a Jumphost. ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:3:0","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"1.1 Download or Clone Bens Github repository called bensdoings git clone https://github.com/corrieb/bensdoings.git (into a folder of your choice) ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:3:1","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"1.2 Create a new folder in which we´ll subsequently copy all necessary files into mkdir ~/vic-dev ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:3:2","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"1.3 Copy the files highlighted in green and folder (1) out of the downloaded/-cloned repo /bensdoings/vic-machine/example-complete-empty.json, example-complete.json as well as example-simple.json /bensdoings/vic-machine/build/1.3.1/build.sh and push.sh /bensdoings/vic-machine/vic-machine-base/OVA/Dockerfile /bensdoings/vic-machine/actions …and paste them into the newly created folder vic-dev. It should look like the following: rguske@rguske-a01\u003e ~/_DEV/Lab/vic-dev\u003e tree -L 2 . ├── Dockerfile ├── actions │ ├── 1.1 │ ├── 1.2 │ ├── Dockerfile.create │ ├── Dockerfile.debug │ ├── Dockerfile.delete │ ├── Dockerfile.direct │ ├── Dockerfile.dumpargs │ ├── Dockerfile.firewall-allow │ ├── Dockerfile.firewall-deny │ ├── Dockerfile.inspect │ ├── Dockerfile.ls │ ├── Dockerfile.rollback │ ├── Dockerfile.thumbprint │ ├── Dockerfile.upgrade │ └── README.md ├── build.sh ├── example-complete-empty.json ├── example-complete.json ├── example-simple.json ├── push.sh └── vic-machine ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:3:3","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"2. Create a new project in Harbor The target is to have all vic-machine actions available as a container-image and to pull as well as to run it from our own registry. Open the VIC Management-Portal via port 8282 (https://vic01.jarvis.lab:8282) and enter your login credentials in case you weren´t automatically forwarded via Single Sign-on from an existing vSphere Web-Client session. Now let´s create a project in Harbor called e.g. vic-machine in which we will later push our images to. Administration –\u003e Projects –\u003e + PROJECT If you want to make all repositories (images) of that project public, select the Public check box like I did. Figure II: New Project in Harbor\" Figure II: New Project in Harbor As a next step we will assign a new user to our project. Administration –\u003e Projects –\u003e vic-machine –\u003e Members –\u003e + ADD I´ve added my local user admin@jarvis.lab to the project and assigned the role Developer to it. In addition to the view access rights, the “Developer” role has the permissions to: Provision containers Push images into registries Create and import templates Figure III: User or Group entitlement to a Project in Harbor\" Figure III: User or Group entitlement to a Project in Harbor Under Administration –\u003e Projects –\u003e vic-machine –\u003e Configuration I´ve checked the box “Automatically scan images on push” to enable the automatically scanning for vulnerabilities of images while pushing them to the project. Figure IV: Vulnerability scanning automatically on push\" Figure IV: Vulnerability scanning automatically on push ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:4:0","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"3. The vic-machine utility running inside a container The basis of a docker image is the Dockerfile. Open the copied Dockerfile and replace the URL in the Dockerfile with the appropriate version of the VIC instance you´re running, to curl the VIC Engine bundle tarball from the appliance into the docker image during the building process. To avoid typos with the URL you can simply browse to the VIC appliance page (port 9443), right click the DOWNLOAD button which can be found under the section Deploy a VCH and choose Copy Link Address. Modify the section like below: ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:5:0","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"3.1 Dockerfile FROMdebian:jessieRUN apt-get update \u0026\u0026 apt-get install -y jq ca-certificates curl tar bcRUN mkdir /vic \\ \u0026\u0026 curl -k -L https://vic01.jarvis.lab:9443/files/vic_v1.5.2.tar.gz | tar xz -C /vic \\ \u0026\u0026 cp /vic/vic/vic-machine-linux /vic \\ \u0026\u0026 cp /vic/vic/*.iso /vic \\ \u0026\u0026 rm -fr /vic/vicCMD [\"/bin/bash\"] Save and close the Dockerfile. ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:5:1","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"3.2 Every vic-machine action as a docker image The build.sh script will build all docker images for each vic-machine action for us. IMPORTANT: It´s necessary to have the Dockerfile prepared before executing the build script! Because “docker build -t vic-machine-base .” will start the build process, which creates the docker image as well as tag (-t) the image as vic-machine-base. The image includes the vic-machine binary (for Linux), the bootstrap.iso as well the appliance.iso. See also: rguske@rguske-a01\u003e ~/_DEV\u003e docker run -it --rm vic-machine-base:latest /bin/bash root@5d76a7b7973f:/# cd /vic root@5d76a7b7973f:/vic# ls -rtl -rwxr-xr-x 1 root root 19623264 Jul 16 20:51 vic-machine-linux -rw-r--r-- 1 root root 41091072 Jul 16 20:51 bootstrap.iso -rw-r--r-- 1 root root 91029504 Jul 16 20:51 appliance.iso Now moving forward to the build script itself. I´ve modified the script from the original one just a little bit for my own needs. I´d like to thank Alexander Dess at this point who reminded me that I should simply stick to the K.I.S.S. principle. Stop making things more complicated 😉. ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:6:0","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"3.3 build.sh #!/bin/bash set -e ### Harbor FQDN and project REPO_NAME=\"vic01.jarvis.lab:443/vic-machine\" ### Folder (with version) were the JSON keys for each vic-machine action are deposited. You can define your own JSON schema by simply modifying those files. MAP_VERSION=\"1.2\" ### Define your own version for the docker images VERSION=\"1.0\" actions=( \"create\" \"debug\" \"delete\" \"inspect\" \"ls\" \"rollback\" \"upgrade\" \"thumbprint\" \"firewall-allow\" \"firewall-deny\" \"dumpargs\" \"direct\" ) ### Docker command to build a docker image (from Dockerfile above) and to tag it with the given name. docker build -t vic-machine-base . ### Will change the directory to actions/1.2 cd actions/$MAP_VERSION ### Will copy the new created Dockerfiles cp ../Dockerfile* . for i in \"${actions[@]}\" do ### Build a docker image out of every newly created Dockerfile and tag it with the repo-name as well as version number docker build -f Dockerfile.$i -t $REPO_NAME/vic-machine-$i:$VERSION . done ### Remove all Dockerfiles rm Dockerfile* Execute sh ./build.sh and validate that everything went well by executing the command docker images | grep vic. You should see the newly created images listed: rguske@rguske-a01\u003e ~/_DEV\u003e docker images | grep vic vic01.jarvis.lab:443/vic-machine/vic-machine-direct 1.0 6434b9f67b34 About an hour ago 305MB vic01.jarvis.lab:443/vic-machine/vic-machine-dumpargs 1.0 65a84b73ea6d About an hour ago 305MB vic01.jarvis.lab:443/vic-machine/vic-machine-firewall-deny 1.0 4280647bb27f About an hour ago 305MB vic01.jarvis.lab:443/vic-machine/vic-machine-firewall-allow 1.0 ef69a80ff285 About an hour ago 305MB vic01.jarvis.lab:443/vic-machine/vic-machine-thumbprint 1.0 cba88f5aeec8 About an hour ago 305MB vic01.jarvis.lab:443/vic-machine/vic-machine-upgrade 1.0 6eca536b5221 About an hour ago 305MB vic01.jarvis.lab:443/vic-machine/vic-machine-rollback 1.0 85b65b17df10 About an hour ago 305MB vic01.jarvis.lab:443/vic-machine/vic-machine-ls 1.0 57496ec4fdd9 About an hour ago 305MB vic01.jarvis.lab:443/vic-machine/vic-machine-inspect 1.0 47eaedc40980 About an hour ago 305MB vic01.jarvis.lab:443/vic-machine/vic-machine-delete 1.0 e24ee592ab15 About an hour ago 305MB vic01.jarvis.lab:443/vic-machine/vic-machine-debug 1.0 306ba819b1d4 About an hour ago 305MB vic01.jarvis.lab:443/vic-machine/vic-machine-create 1.0 e567f6eff231 About an hour ago 305MB vic-machine-base latest 9434fc83969b About an hour ago 305MB ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:6:1","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"3.4 Pushing the docker images to Harbor The next step after creating a lot of new docker images is to have them available and well guarded in Harbor. We´ll make use of the push.sh script from our folder which will save us the work of handling each image separately. Let´s have a look into it as well. ### !/bin/bash set -e ### Push to our Harbor instance REPO_NAME=\"vic01.jarvis.lab:443/vic-machine\" ### Docker images version VERSION=\"1.0\" actions=( \"create\" \"debug\" \"delete\" \"inspect\" \"ls\" \"rollback\" \"upgrade\" \"thumbprint\" \"firewall-allow\" \"firewall-deny\" \"dumpargs\" \"direct\" ) for i in \"${actions[@]}\" do ### Tagging the images with the give version as well as with \"latest\" ### Pushing them with both versions into Harbor docker tag $REPO_NAME/vic-machine-$i:$VERSION $REPO_NAME/vic-machine-$i:latest docker push $REPO_NAME/vic-machine-$i:$VERSION docker push $REPO_NAME/vic-machine-$i:latest done Just replace the repository URL like we already did in the build script. Before you execute the script, you have to login into Harbor with the user which we´ve added to the project in step 4. Execute: docker login vic01.jarvis.lab:443 -u admin@jarvis.lab Password: ******** Login Succeeded “Login succeeded” is always pleasing to see and since we´re done with that run sh ./push.sh from your shell. If you have the membership and role correctly configured, you should see under Internal Repositories all vic-machine-(action) images. Figure V: vic-machine-(action) images deposited under Internal (Project) Repositories\" Figure V: vic-machine-(action) images deposited under Internal (Project) Repositories ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:7:0","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"4. Configure the config.json file for the VCH deployment Everything seems to be well prepared so far and we´re approaching the end of our efforts to deploy a VCH in a “simplified fashion”. The intention is to make use of the vic-machine script, which in turn needs a JSON manifest as input to all commands lying in a subdirectory of our vic-dev folder. Before turning to the config.json file I don’t want to embezzle this impressive piece of work. The vic-machine script: #!/bin/bash # This script is designed to simplify vic-machine usage, but using a single JSON manifest as input to all commands # The expected layout is that a particular vic-machine configuration will be in a subdirectory of the current dir # In that subdirectory should be a config.json file which conforms to the format in example.json # That subdirectory is then both the input and working directory of vic-machine # As such, generated certificates and log files will be written to that sub directory as root user # Note that if you don't specify an admin password in the JSON, you will be prompted each time - this is why the script needs -it # consume images from dockerhub # REPO_NAME=\"bensdoings\" # consume images from a local registry REPO_NAME=\"vic01.jarvis.lab:443/vic-machine\" usage() { echo \"Usage: vic-machine.sh \u003cversion\u003e \u003cconfig-dir\u003e \u003caction\u003e\" echo \" Where version is \\\"latest\\\" or \u003cversion number\u003e-\u003coptional rc\u003e eg. 1.0.0-rc5\" echo \" Where action is one of: create | delete | inspect | ls | debug | upgrade | rollback | thumbprint | firewall-allow | firewall-deny | dumpargs\" echo \" Use dumpargs action to show the args the script generates for vic-machine-create\" exit 1 } if [ $# -le 2 ]; then usage; fi case \"$3\" in create|delete|inspect|ls|debug|upgrade|rollback|thumbprint|firewall-allow|firewall-deny|dumpargs) ;; *) usage ;; esac host_config_path=$(pwd)/$2 if [ ! -f \"$host_config_path/config.json\" ]; then echo \"Config file not found at $host_config_path/config.json\" exit 1 fi docker_volume_path=/config # Check to see whether the combination of version and action actually exist docker pull $REPO_NAME/vic-machine-$3:$1 \u003e /dev/null 2\u003e\u00261 if [ $? != 0 ]; then echo \"Action \\\"$3\\\" or version \\\"$1\\\" not found\" exit 1 fi sudo docker run -it --rm -v $host_config_path:$docker_volume_path $REPO_NAME/vic-machine-$3:$1 | sed \"s|$docker_volume_path/|$host_config_path/|\" ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:8:0","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"4.1. Preparing the config.json file Create a new folder in your main folder (vic-dev), give it the name of your VCH for example and also create the config.json file. mkdir vch1 \u0026\u0026 cd vch1 \u0026\u0026 touch config.json Here is an example of my deployment. vch1/config.json: { \"version\": \"1.2\", \"vch\": { \"name\": \"vch1\", \"compute\": { \"vsphere_resource\": \"Stark\", \"memory\": { \"limit_mb\": \"8192\", \"reservation_mb\": \"512\", \"shares\": \"normal\" } }, \"network\": { \"bridge\": { \"port_group\": \"ls-vic-bridge\", \"ip_range\": \"172.16.0.0/12\" }, \"public\": { \"port_group\": \"ls-vic-public\", \"vch_static_ip\": \"192.168.2.192/24\", \"vch_gateway\": \"192.168.2.254\" }, \"container\": [ { \"port_group\": \"ls-vic-container\", \"firewall\": \"Open\" }, { \"port_group\": \"ls-vic-container\", \"alias\": \"container-net\", \"ip_range\": \"192.168.3.100/24\", \"gateway\": \"192.168.3.254/24\", \"dns\": \"192.168.100.254\", \"firewall\": \"Published\" } ], \"dns_server\": \"192.168.100.250\", \"asymmetric_routes\": false }, \"storage\": { \"image_store\": \"nfs_ds\", \"volume_stores\": [\"vsanDatastore/volumes:default\"], \"base_image_size\": \"8GB\", \"volumes_rm_on_delete\": true }, \"auth\": { \"cert_path\": \"vch1\", \"keygen\": { \"cert_size\": \"2048\", \"tls_cname\": \"*.jarvis.lab\", \"organization\": [\"Jarvis Lab\"] } }, \"endpoint_vm\": { \"vcpus\": \"1\", \"memory_mb\": \"2048\", \"syslog_address\": \"tcp://192.168.100.6:514\" }, \"container\": { \"name_convention\": \"{name}-{id}\" }, \"debug\": { \"enable_ssh\": true, \"root_password\": \"*********\" } }, \"vcenter\": { \"address_datacenter\": \"192.168.100.1/Malibu\", \"credentials\": { \"thumbprint\": \"4C:92:D7:FC:F8:8D:5A:....\", \"admin_user\": \"administrator@jarvis.lab\", \"admin_password\": \"*******\" } }, \"registry\": { \"insecure_registries\": [\"192.168.100.50\"] }, \"vic_machine\": { \"debug\": \"0\", \"timeout\": \"60s\" } } Important You have to define the version at the beginning which will point to the folder with the declared version number in the actions folder. Remember: rguske@rguske-a01\u003e ~/_DEV/Lab/vic-dev/actions\u003e tree -L 2 . ├── 1.1 │ ├── map-create.json │ ├── map-debug.json │ ├── map-delete.json │ ├── map-firewall.json │ ├── map-inspect.json │ ├── map-ls.json │ ├── map-thumbprint.json │ ├── map-upgrade.json │ ├── parse.sh │ └── validate.sh ├── 1.2 │ ├── map-create.json │ ├── map-debug.json │ ├── map-delete.json │ ├── map-firewall.json │ ├── map-inspect.json │ ├── map-ls.json │ ├── map-thumbprint.json │ ├── map-upgrade.json │ ├── parse.sh │ └── validate.sh Pick two identical files e.g. map-create.json from the folder 1.1 and 1.2 and compare them against each other to see the differences and how important it is to set the right version. The cool thing is that this also means that you can define your own JSON schema by simply modifying those files. Depending on your requirements (VCH Networking, Log aggregation, Debug-Level and so on), you can build your own config.json from the examples (example-simple.json, example-complete.json \u0026 example-complete-empty.json) which we´ve copied in step 3. If you now execute “sh ./vic-machine 1.0 (or latest) vch1 create” and run a watch perhaps on container instances spinning up locally and also a watch on the folder vch1 itself, you´ll see that the newly created image vic-machine-create:1.0 will be used for having the vic-machine utility on board which will then receive the parsed arguments from the config.json and starts deploying the Virtual Container Host to your target. By using e.g my deployment-example, you´ll see that a new folder with the given name of the VCH will be created, containing the VCH certificates, which can be used to add the VCH to a project for example as well as a vic-machine and vpxd logfile, which is really nice to have. Figure VI: vic-machine script with a watch on docker ps -a as well as ls -rtl\" Figure VI: vic-machine script with a watch on docker ps -a as well as ls -rtl Let´s give it a try - I hope you´ll find it helpful. ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:8:1","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"5 (addition): Adding a VCH to your project As mentioned a few lines above, we can use the created certificates during the deployment, to add a Virtual Container Host to our project(s). Just go to: Administration –\u003e Identity Management –\u003e Credentials –\u003e + CREDENTIAL Choose Certificate, enter a name and use the cert.pem for the Public certificate and the key.pem for the Private certificate. SAVE Figure VII: Using certificates for VCH authentication\" Figure VII: Using certificates for VCH authentication The VCH can be added to your project under: Administration –\u003e Projects –\u003e Your Project (vic-machine) –\u003e Infrastructure –\u003e + CONTAINER HOST Fill out the required fields and select under Credentials your previously created credentials and press SAVE. Figure VIII: Adding a Virtual Container Host to a Project\" Figure VIII: Adding a Virtual Container Host to a Project The result should look like this if everything went well. Figure IX: Virtual Container Host assigned to a Project\" Figure IX: Virtual Container Host assigned to a Project ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:9:0","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["Container","VMware","Automation"],"content":"Resources Article Link [@github/corrieb] Bens Github repo vic-machine https://github.com/corrieb/bensdoings/tree/master/vic-machine [@youtube] VMworld 2017 CNA2547BU - VIC Deep Dive https://youtu.be/AD7CD8Haqdc [@github/rguske] Roberts Github repo vic-machine action examples https://github.com/rguske/vic-machine [@harbor] Harbor Official Website https://goharbor.io/ [@github/coreos] Clair Github repo https://github.com/coreos/clair [@github/theupdateframework] Notary Github repo https://github.com/theupdateframework/notary [@docker] Dockerfile reference https://docs.docker.com/engine/reference/builder/ ","date":"2019-07-04","objectID":"/post/docker-run-vic-machine/:10:0","tags":["July2019","VIC","Docker","Hack","Harbor","vSphere"],"title":"docker run vic-machine","uri":"/post/docker-run-vic-machine/"},{"categories":["FaaS","Kubernetes","Open Source","Cloud Native","Automation","VMware"],"content":"A time pre project VMware Event Broker Appliance","date":"2019-02-14","objectID":"/post/event-driven-interactions-with-vsphere-using-functions-as-a-service/","tags":["February2019","vSphere","Kubernetes","FaaS","Event-driven","OSS"],"title":"Event-driven interactions with vSphere using Functions as a Service","uri":"/post/event-driven-interactions-with-vsphere-using-functions-as-a-service/"},{"categories":["FaaS","Kubernetes","Open Source","Cloud Native","Automation","VMware"],"content":"Note I am very happy to inform you that the below described idea has become a VMware Fling (completely OpenSource). Official website of VMware Event Broker Appliance: https://vmweventbroker.io/ When was the last time that you were so much taken by a topic or better by a combination of several, that just by thinking about it your face is filled with enthusiasm? For me, it’s only been a few days ago and it was in one of these afterwork-calls with my well appreciated colleague and friend Michael Gasch where we talked about things we were working on. I had just published my recent post on “Monitoring Container VMs with vRealize Operations Manager” were I described how I built a dashboard for \u0026 with vRealize Operations Manager to monitor the utilization and performance for two of the major objects from vSphere Integrated Containers, the Virtual Container Host (VCH) and the Container-VMs (cVM). Michael told me that he had spent some time in digging deeper and deeper into the topic Functions as a Service… knative might be the future of multi-cloud FaaS abstraction. Right now I still prefer OpenFaaS for it's simplicity (in the context of my work at VMware). It just works... — Michael Gasch (@embano1) October 25, 2018 which was far away from being experienced by myself, but he got me right away from the very beginning of his telling. For those of you who want to know more about the topic but haven’t put your nose into it yet, like me before, you´ll find some links at the Resources section at the buttom. I´m not going to explain what a Function/ FaaS is all about, instead of it Michael and I thought that this post should serve as an introduction of a project Michael is working on as well as to demonstrate how “just” two components are turning your vSphere environment into a Event-driven construct based on your annotations. Before I´m going to start with the details and what should already be up and running, I`d like to quote the following description to explain what it means when it comes to the topic “Functions vs. Containers”. Info Container: Create a container which has all the required (Application) dependencies pre-installed, put your application code inside of it and run it everywhere the container runtime is installed. FaaS: Applications get split up into different functionalities (or services), which are in turn triggered by events. You upload your function code and attach an event source to it. Source: Serverless (FaaS) vs. Containers - when to pick which? ","date":"2019-02-14","objectID":"/post/event-driven-interactions-with-vsphere-using-functions-as-a-service/:0:0","tags":["February2019","vSphere","Kubernetes","FaaS","Event-driven","OSS"],"title":"Event-driven interactions with vSphere using Functions as a Service","uri":"/post/event-driven-interactions-with-vsphere-using-functions-as-a-service/"},{"categories":["FaaS","Kubernetes","Open Source","Cloud Native","Automation","VMware"],"content":"Tagging vSphere objects by using a ƒ(x) At the beginning I´ve mentioned that my last post was on the topic “Monitoring Container VMs with vRealize Operations Manager” and that I had to built a dashboard to do so. I´m using a widget in this dashboard called Object List to have the ability to select a specific Virtual Container Host, the Resource Pool not the Docker end-point VM(!), to display the utilization in terms of Limits \u0026 Reservations (CPU \u0026 Memory) of it. While I was building this dashboard, one challange for me was, to find a way on which criteria these Resource-Pools will join a Dynamic Custom Group in vRealize Operations Manager (to appear automatically in the dashboard/ in the object list). Long story short, I decided to use vSphere Tags as a Membership-Criteria. But assigning a vSphere Tag is still a manually task (*1) and this is the point were Michaels function pytagfn or gotagfn, depending on the language you prefer, as well as the vCenter Connector comes into play. *1 A Feature-Request for using the vic-machine utility to assign a vSphere-Tag during the VCH deployment is already open on Github: https://github.com/vmware/vic/issues/8446 The following flowchart will give you a high-level overview what it´s all about and what Michael and I will demo you in the Video below. Figure I: Flowchart vSphere Tag assignment\" Figure I: Flowchart vSphere Tag assignment vCenter-Connector is listening to the event-stream coming from the vCenter Server and talks to the OpenFaaS-Gateway to retrieve a list of functions which are interested in incoming events. pytag-fn will get invoked by a specific event (e.g. resource.pool.created). vSphere Tag assignment through the triggered function. vRealize Operations Manager is polling data from vCenter Server in certain intervals. Resource Pool is added to the Dynamic Custom Group and is available in the dashboard/ object-list. ","date":"2019-02-14","objectID":"/post/event-driven-interactions-with-vsphere-using-functions-as-a-service/:1:0","tags":["February2019","vSphere","Kubernetes","FaaS","Event-driven","OSS"],"title":"Event-driven interactions with vSphere using Functions as a Service","uri":"/post/event-driven-interactions-with-vsphere-using-functions-as-a-service/"},{"categories":["FaaS","Kubernetes","Open Source","Cloud Native","Automation","VMware"],"content":"The recording ","date":"2019-02-14","objectID":"/post/event-driven-interactions-with-vsphere-using-functions-as-a-service/:2:0","tags":["February2019","vSphere","Kubernetes","FaaS","Event-driven","OSS"],"title":"Event-driven interactions with vSphere using Functions as a Service","uri":"/post/event-driven-interactions-with-vsphere-using-functions-as-a-service/"},{"categories":["FaaS","Kubernetes","Open Source","Cloud Native","Automation","VMware"],"content":"What you need | Pre-reqs All you need is OpenFaaS running in Kubernetes. As shown in the flowchart as well as having been mentioned in the recording, I´m running VMware´s Enterprise-grade Kubernetes solution VMware PKS in my #Homelab in order to make all necessary pieces available. Say \"Hello\" to the newest member in my #Homelab: #VMware #PKS 😎 Next step for tonight: Registry replication between my @project_harbor instance, which is running within #vSphereIntegratedContainers and PKS. #Kubernetes #containers pic.twitter.com/VW2abqBHSN — Robert Guske (@vmw_rguske) January 27, 2019 Another pretty cool alternative way to run OpenFaaS on k8s is KinD which is a new tool from the Kubernetes community named Kubernetes in Docker. Figure II: kind logo\" Figure II: kind logo Alex Ellis has already written a nice blog-post on it: “Get started with OpenFaaS and KinD”. Check it out! ","date":"2019-02-14","objectID":"/post/event-driven-interactions-with-vsphere-using-functions-as-a-service/:3:0","tags":["February2019","vSphere","Kubernetes","FaaS","Event-driven","OSS"],"title":"Event-driven interactions with vSphere using Functions as a Service","uri":"/post/event-driven-interactions-with-vsphere-using-functions-as-a-service/"},{"categories":["FaaS","Kubernetes","Open Source","Cloud Native","Automation","VMware"],"content":"Install OpenFaaS on Kubernetes Installing OpenFaaS on Kubernetes is quite easy by using the “OpenFaaS-on-Kubernetes” Edition faas-netes. Go through the documentation and you´ll have it installed in a couple of steps. OpenFaaS - Serverless Functions Made Simple OpenFaaS Docs - Deployment guide for Kubernetes Github Repository - faas-netes Having followed the instructions in the documentation, you will end up with the following components installed: Helm with the Helm client (helm) and the Helm server (Tiller - running in the namespace kube-system). OpenFaaS on Kubernetes deployed into the namespace openfaas The faas-cli via e.g. brew or curl Github Repo vCenter Connector As of writing this post: The demo is based on a pending pull request (PR) in the vcenter-connector Github repository, which is currently under review. The PR, including all the details and files changed, can be found here: https://github.com/openfaas-incubator/vcenter-connector/pull/11 Github Repo pytag-fn or Github Repo gotag-fn I hope you enjoyed reading and liked the recording. ","date":"2019-02-14","objectID":"/post/event-driven-interactions-with-vsphere-using-functions-as-a-service/:4:0","tags":["February2019","vSphere","Kubernetes","FaaS","Event-driven","OSS"],"title":"Event-driven interactions with vSphere using Functions as a Service","uri":"/post/event-driven-interactions-with-vsphere-using-functions-as-a-service/"},{"categories":["FaaS","Kubernetes","Open Source","Cloud Native","Automation","VMware"],"content":"Resources Article Link [@openfaaS] Deployment guide for Kubernetes https://docs.openfaas.com/deployment/kubernetes/ [@github/openfaas-incubator] Github repo vcenter-connector https://github.com/openfaas-incubator/vcenter-connector [github/embano1] Github repo pytagfn https://github.com/embano1/pytagfn [github/embano1] Github repo gotagfn https://github.com/embano1/pytagfn [@blog.alexellis.io] Get started with OpenFaaS and KinD https://blog.alexellis.io/get-started-with-openfaas-and-kind/ [@mgasch.com] Events, the DNA of Kubernetes https://www.mgasch.com/post/k8sevents/ [@virtuallyghetto.com] Getting started with VMware Pivotal Container Service (PKS) Part 1: Overview https://www.virtuallyghetto.com/2018/03/getting-started-with-vmware-pivotal-container-service-pks-part-1-overview.html [@keithlee.ie] PKS NSX-T Home Lab – Part 10: Install Ops Man and BOSH http://keithlee.ie/2018/12/09/pks-nsx-t-home-lab-part-10-install-ops-man-and-bosh/ [@beyondelastic.com] VMware PKS 1.3 What’s New https://beyondelastic.com/2019/01/24/vmware-pks-1-3-whats-new/ [@blogs.vmware.com] Dispatch to support Knative and Joins Cloud-Native Apps BU https://blogs.vmware.com/cloudnative/2018/10/25/dispatch-team-joins-cloud-native-apps-bu/ [@thehumblelab.com] Getting Started with VMware Dispatch on Kubernetes https://www.thehumblelab.com/getting-started-with-vmware-dispatch-on-kubernetes/ [@serverless.com] Serverless (FaaS) vs. Containers https://serverless.com/blog/serverless-faas-vs-containers/ [@medium.com] Search query “Functions as a Service” https://medium.com/search?q=functions%20as%20a%20service [@cloud.vmware.com] VMware PKS Landing Page https://cloud.vmware.com/vmware-pks [@pivotal.io] Pivotal PKS Landing Page https://pivotal.io/platform/pivotal-container-service ","date":"2019-02-14","objectID":"/post/event-driven-interactions-with-vsphere-using-functions-as-a-service/:5:0","tags":["February2019","vSphere","Kubernetes","FaaS","Event-driven","OSS"],"title":"Event-driven interactions with vSphere using Functions as a Service","uri":"/post/event-driven-interactions-with-vsphere-using-functions-as-a-service/"},{"categories":["Monitoring","VMware","Container"],"content":"Read how to use a custom vRealize Operations Manager dashboard to monitor vSphere Integrated Containers (Container-VMs).","date":"2019-01-21","objectID":"/post/monitoring-container-vms-with-vrealize-operations-manager/","tags":["January2019","VIC","Docker","vSphere","Monitoring","vRealize Operations Manager"],"title":"Monitoring Container VMs with vRealize Operations Manager","uri":"/post/monitoring-container-vms-with-vrealize-operations-manager/"},{"categories":["Monitoring","VMware","Container"],"content":"Based on talks with customers who already made their experiences running containerized applications in test as well as in production and who are using vSphere Integrated Containers for their way to go, I decided to build a dashboard to monitor those workloads (Container-VMs) with VMware´s Cloud Management Solution, vRealize Operations Manager (vR Ops). Figure I: VIC monitoring high level overview\" Figure I: VIC monitoring high level overview ","date":"2019-01-21","objectID":"/post/monitoring-container-vms-with-vrealize-operations-manager/:0:0","tags":["January2019","VIC","Docker","vSphere","Monitoring","vRealize Operations Manager"],"title":"Monitoring Container VMs with vRealize Operations Manager","uri":"/post/monitoring-container-vms-with-vrealize-operations-manager/"},{"categories":["Monitoring","VMware","Container"],"content":"The Dashboard Treating Containers as “First-Class Citizens” is what vSphere Integrated Containers can offer to IT-(Dev)Ops Teams through instantiating a Container-Image as a Virtual Machine. Quote “Therefore you don´t have to build out a separate, tailored infrastructure stack and can continue to leverage existing Scalability, Security and Monitoring capabilities.” Source: VMware vSphere Integrated Containers: Introduction Figure II: VIC Dashboard\" Figure II: VIC Dashboard I´ve splitted the dashboard into two parts because I wanted to get as most as valuable content displayed on the screen without scrolloing down the page (depending on your resolution 😉). The upper part gives you more details about the utilization and properties of the Virtual Container Host itself. Thus resource pool as well as the Virtual Container Host VM (Docker endpoint) are covered by the upper part. Figure III: VIC Dashboard - upper part\" Figure III: VIC Dashboard - upper part The lower part is focused on the performance of each Container-VM (by selecting the cVM) running inside the resource pool, and by the end of the day, on the service which will be provided by the container. Figure IV: VIC Dashboard - lower part\" Figure IV: VIC Dashboard - lower part I also considered to build out each part in a separate dashboard, so one for the Virtual Container Host(s) and another one for the Container-VM(s), but nevertheless I decided to go with the “All-in-One” variant. ","date":"2019-01-21","objectID":"/post/monitoring-container-vms-with-vrealize-operations-manager/:1:0","tags":["January2019","VIC","Docker","vSphere","Monitoring","vRealize Operations Manager"],"title":"Monitoring Container VMs with vRealize Operations Manager","uri":"/post/monitoring-container-vms-with-vrealize-operations-manager/"},{"categories":["Monitoring","VMware","Container"],"content":"Download via VMware {code} If you like the dashboard and you are interested to monitor your containerized applications in terms of performance and utilization, you can download it through the following link: https://code.vmware.com/samples?id=5283 But before you can make use of it you have to do some preperations first. ","date":"2019-01-21","objectID":"/post/monitoring-container-vms-with-vrealize-operations-manager/:1:1","tags":["January2019","VIC","Docker","vSphere","Monitoring","vRealize Operations Manager"],"title":"Monitoring Container VMs with vRealize Operations Manager","uri":"/post/monitoring-container-vms-with-vrealize-operations-manager/"},{"categories":["Monitoring","VMware","Container"],"content":"Things you have to do first before the import In vSphere Create a new vSphere-Tag Open up the vSphere-Client and create a new vSphere-Tag Category as well as a vSphere-Tag called Virtual Container Host and assign the Tag to every Virtual Container Host Resource Pool(!). Figure V: vSphere Resource Pool\" Figure V: vSphere Resource Pool I´m going to explain the reason why we need this in a moment. In vROps Create a Group Type to categorize objects in vR Ops. Create a Dynamic Custom Group for dynamic assignment of Virtual Container Hosts through a membership criteria. Create two new Metric Configurations to define a specific set of metrics for the VCH as well as for the cVM. Import two new Views (VCH properties \u0026 cVM properties) Let´s start with #1, the creation of a Group Type, a superior group for our Dynamic Custom Group which we´ll configure in step #2. When you´re already logged in into vR Ops go to Administration and select Group Types which is under Configuration on the left side. Create a new one and call it vSphere Integrated Containers for example. Figure VI: vROps - Create new Group Type\" Figure VI: vROps - Create new Group Type It is followed by the mentioned Dynamic Custom Group which is not found under Administration but under Environment and then Groups and Applications. Create a new one and call it e.g. Virtual Container Hosts. Figure VII: vROps - Create new Custom Group\" Figure VII: vROps - Create new Custom Group One striking aspect about Dynamic Custom Groups is the fact that we can define the Membership based on a specific criteria. During my first tests, I´ve defined the membership based on the Prefix of a Virtual Container Host name like **vch-**app01 for example. Asking me naming conventions is an important topic and every organization has its own, so it won´t fit for everyone and therefore I had to find another criteria to let the dynamic assignment to this group happen. I thought vSphere-Tags is a perfect alternative as a criteria. Consequently, we have to configure it for the Object Type Resource Pool like the screenshot below shows. Properties –\u003e Summary|vSphere Tag is Virtual Container Host Figure VIII: vROps - Dynamic Custom Group\" Figure VIII: vROps - Dynamic Custom Group Hit Preview to see if it works. Figure IX: vROps - vSphere Tag as a Membership Criteria\" Figure IX: vROps - vSphere Tag as a Membership Criteria The next step, step #3, is the configuration of a specific metric set which will only show us what we´ll see in our dashboard reagrding the Virtual Container Host Resource Pool as well as the Container-VM. To configure those, go to Administration and Configuration again and select this time Metric Configurations. Select the folder ReskndMetric and hit the plus sign above to create a new one. I´ve named the first of my two configurations vch_resource_utilization. Of course you can title it as you like but we have to keep in mind that we have to select this one after the dashboard import. Figure X: vROps - Metric Configuration VCH\" Figure X: vROps - Metric Configuration VCH After assigning the name, please copy the following xml code and paste it into your new metric configuration. \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cAdapterKinds\u003e \u003cAdapterKind adapterKindKey=\"VMWARE\"\u003e \u003cResourceKind resourceKindKey=\"ResourcePool\"\u003e \u003cMetric attrkey=\"summary|total_number_vms\" label=\"# cVM\" unit=\"cVM\" yellow=\"\" orange=\"\" red=\"\" link=\"\"/\u003e \u003cMetric attrkey=\"summary|number_running_vms\" label=\"# running cVM\" unit=\"cVM\" yellow=\"\" orange=\"\" red=\"\" link=\"\"/\u003e \u003cMetric attrkey=\"cpu|corecount_provisioned\" label=\"# CPU Cores\" unit=\"cores\" yellow=\"\" orange=\"\" red=\"\" link=\"\" isProperty=\"true\"/\u003e \u003cMetric attrkey=\"config|cpuAllocation|limit\" label=\"CPU Limit\" unit=\"MHz\" yellow=\"\" orange=\"\" red=\"\" link=\"\" isProperty=\"true\"/\u003e \u003cMetric attrkey=\"config|cpuAllocation|reservation\" label=\"CPU Reservation\" unit=\"GHz\" yellow=\"\" orange=\"\" red=\"\" link=\"\" isProperty=\"true\"/\u003e \u003cMetric attrkey=\"config|memoryAllocation|limit\" label=\"Mem Limit\" u","date":"2019-01-21","objectID":"/post/monitoring-container-vms-with-vrealize-operations-manager/:1:2","tags":["January2019","VIC","Docker","vSphere","Monitoring","vRealize Operations Manager"],"title":"Monitoring Container VMs with vRealize Operations Manager","uri":"/post/monitoring-container-vms-with-vrealize-operations-manager/"},{"categories":["Monitoring","VMware","Container"],"content":"Conclusion vRealize Operations Manager has evolved greatly over time and building custom dashboards according to your own ideas and demands is quite fun. Let´s just take the new Dashboard Canvas which was shipped with version 7.0 as an example. It´s not all about how cool it looks like but rather than the simplicity it brings during the creation of an dashboard. Figure XIII: vROps - Widget Interactions\" Figure XIII: vROps - Widget Interactions Here are two posts I would like to recommend to read with regard to: Dashboards Made Easy with vRealize Operations Bigger, Stronger, Faster Dashboards using vRealize Operations 7.0 The #vCommunity is more active than ever on sharing great work and if you like to make use of what the #vCommunity already cooked or even better if you like to share your own creations, please go here: https://vrealize.vmware.com/sample-exchange/ and give it a shout-out on Twitter. Our great Product-Manager´s will hear you 😎. Super excited to see how the #vROps community has grown over the past few years. I remember the lack of blogs and Ops experts. It is great to see this achievement of community driven ops content. To the #OpsCommunity.. Cheers.. @VMware #vExpert @vRealizeOps @vmwarecloudmgmt https://t.co/AEc6qgZ0Ep — Sunny Dua (@Sunny_Dua) January 5, 2019 And in case you are already a vR Ops-Ninja and you are not on this list https://twitter.com/johnddias/lists/vrops-friends1 reach out to John Dias and he´ll put your name on the list as well. I just created a list! It is short but growing. If anyone is missing that should be there let me know. https://t.co/0eVPbQJCwq — John Dias 🔭 (@johnddias) November 30, 2018 Figure XIV: vROps Ninja\" Figure XIV: vROps Ninja ","date":"2019-01-21","objectID":"/post/monitoring-container-vms-with-vrealize-operations-manager/:2:0","tags":["January2019","VIC","Docker","vSphere","Monitoring","vRealize Operations Manager"],"title":"Monitoring Container VMs with vRealize Operations Manager","uri":"/post/monitoring-container-vms-with-vrealize-operations-manager/"},{"categories":["Monitoring","VMware","Container"],"content":"Resources Article Link [@VMware Blog] Dashboards Made Easy with vRealize Operations https://blogs.vmware.com/management/2018/10/dashboards-made-easy-with-vrealize-operations.html [@VMware Blog] Bigger, Stronger, Faster Dashboards using vRealize Operations 7.0 https://blogs.vmware.com/management/2018/10/bigger-stronger-faster-dashboards-using-vrealize-operations-7-0.html [@VMware Blog] VMware´s Cloud Management Blog https://blogs.vmware.com/management/ [@VMware Blog] VMware´s Cloud-Native Blog https://blogs.vmware.com/cloudnative ","date":"2019-01-21","objectID":"/post/monitoring-container-vms-with-vrealize-operations-manager/:3:0","tags":["January2019","VIC","Docker","vSphere","Monitoring","vRealize Operations Manager"],"title":"Monitoring Container VMs with vRealize Operations Manager","uri":"/post/monitoring-container-vms-with-vrealize-operations-manager/"},{"categories":["Blogging"],"content":"Inspired by a recent post from my friend, (Go)Hugo and Git mentor Michael Gasch 😉, where he describes how he fixed the Hugo Shortcode for Twitter to suppress the default thread view, it came to my mind immediately that I faced a similar issue when I started blogging with the Shortcode which embeds a Youtube-Video into your post. See how Michael fixed the normal behavior for the Twitter Shortcode {{\u003c tweet \u003e}} here… I had to fix the @GoHugoIO Twitter shortcode to suppress the thread view as needed. Here's a short writeup: https://t.co/ls6HVEgrBI #hugo #twitter — Michael Gasch (@embano1) December 7, 2018 …and add it to your Shortcode folder to make advantages from. ","date":"2018-12-09","objectID":"/post/youtube-hugo-shortcode-workaround-when-youtube-wont-work/:0:0","tags":["December2018","Hugo","Shortcode","Blogging"],"title":"Youtube Hugo-Shortcode workaround - when {{\u003c youtube \u003e}} won´t work","uri":"/post/youtube-hugo-shortcode-workaround-when-youtube-wont-work/"},{"categories":["Blogging"],"content":"When {{\u003c youtube \u003e}} won´t work for your theme Attention This applies only to the singleViewStyle = \"casper\" configuration in your config.toml. With singleViewStyle = \"caspertwo\" the shortcode {{\u003c youtube \u003e}} works. Besides embedding a tweet into your post e.g. to give a statement more expression/ more emphasis, or to comment a topic related tweet, or simply because it´s pretty cool, a Youtube-Video is also a good alternative. I´m running the Casper-Two Theme for my Blog and back when I wrote \"Sometimes you gotta run before you can walk\", I absolutely wanted to have the IRON MAN I First Armor test Video included and it didn´t work straight from the start. ","date":"2018-12-09","objectID":"/post/youtube-hugo-shortcode-workaround-when-youtube-wont-work/:1:0","tags":["December2018","Hugo","Shortcode","Blogging"],"title":"Youtube Hugo-Shortcode workaround - when {{\u003c youtube \u003e}} won´t work","uri":"/post/youtube-hugo-shortcode-workaround-when-youtube-wont-work/"},{"categories":["Blogging"],"content":"Let me show you what I mean I´ll add the video “Shortcodes | Hugo - Static Site Generator | Tutorial 9” here at this point of the post by using {{\u003c youtube 2xkNJL4gJ9E \u003e}} in my post *.md file. 👇 Normally you would see the mentioned Youtube video here ☝️ but it seems that the Shortcode won´t work for my chosen theme. ","date":"2018-12-09","objectID":"/post/youtube-hugo-shortcode-workaround-when-youtube-wont-work/:1:1","tags":["December2018","Hugo","Shortcode","Blogging"],"title":"Youtube Hugo-Shortcode workaround - when {{\u003c youtube \u003e}} won´t work","uri":"/post/youtube-hugo-shortcode-workaround-when-youtube-wont-work/"},{"categories":["Blogging"],"content":"You are not alone The best thing to do first, from my point of view, is not to google the (mis-)behavior, but rather go to the Github-Repo from your theme and have a look at the Issues. If there exists one, comment…if not…raise one. So I had a look at the issues for Casper-Two and indeed, there´s already one open which covers this issue. I commented this as well and a really short period of time later, Salar Rahmanian had the code snippet for it. This is simply great and one of thousands of things I´m really impressed with regarding Github and the Community. ","date":"2018-12-09","objectID":"/post/youtube-hugo-shortcode-workaround-when-youtube-wont-work/:2:0","tags":["December2018","Hugo","Shortcode","Blogging"],"title":"Youtube Hugo-Shortcode workaround - when {{\u003c youtube \u003e}} won´t work","uri":"/post/youtube-hugo-shortcode-workaround-when-youtube-wont-work/"},{"categories":["Blogging"],"content":"The Code As Michael already described it in his post, we have to create a new empty file and this time we´ll call it yt.html. cd \u003cHUGO_BLOG_ROOT\u003e touch layouts/shortcodes/yt.html Put the following lines of code into the yt.html file and save it. \u003ciframe src=\"https://www.youtube.com/embed/{{ index .Params 0 }}?start={{ index .Params 1 }}\" style=\"position: absolute; top: 0; left: 0; width: 560; height: 315;\" allowfullscreen frameborder=\"0\" title=\"YouTube Video\"\u003e \u003c/iframe\u003e This time I´ll make use of our new Shortcode {{\u003c yt \u003e}} instead of {{\u003c youtube \u003e}} et voilà… …here we go 👍. Thanks again Salar! If you´re facing the issue that your youtube video is not showing up by using the @GoHugoIO shortcode {{\u003c youtube w7Ft2ymGmfc \u003e}} with your chosen theme, you should try this great workaround https://t.co/u6yLzQWf7n by @SalarRahmanian, which works perfectly for me.Thx a lot Salar! — Robert Guske (@vmw_rguske) August 1, 2018 Getting more interest in building your own Shortcodes? Go here: Create Your Own Shortcodes “You can extend Hugo’s built-in shortcodes by creating your own using the same templating syntax as that for single and list pages.\" Thanks for reading. ","date":"2018-12-09","objectID":"/post/youtube-hugo-shortcode-workaround-when-youtube-wont-work/:3:0","tags":["December2018","Hugo","Shortcode","Blogging"],"title":"Youtube Hugo-Shortcode workaround - when {{\u003c youtube \u003e}} won´t work","uri":"/post/youtube-hugo-shortcode-workaround-when-youtube-wont-work/"},{"categories":["VMware","Container"],"content":"A how-to guide to upgrade VIC from one major version to another.","date":"2018-10-19","objectID":"/post/upgrade-vmware-vsphere-integrated-containers/","tags":["October2018","VIC","Container","VMware","vSphere","Docker","Upgrade"],"title":"Upgrade vSphere Integrated Containers","uri":"/post/upgrade-vmware-vsphere-integrated-containers/"},{"categories":["VMware","Container"],"content":"I like upgrades! Because mostly they bring cool new features with them or eliminate disturbing issues. In this post I´d like to guide you through the upgrade process of VMware vSphere Integrated Containers from version v1.3.1 to v1.4.1 which is a major upgrade and also to v1.4.3 which is a minor upgrade. Thanks Tom Schwaller for reviewing this article. First of all and this applies to all upgrades, we have to check the compatibility and dependencys with our infrastructure components. This is for instance our vCenter Server where we have dependencies through the VIC vSphere Client Plug-In! VIC version 1.4.0 is e.g. compatible with vCenter Server version 6.0.0 U2 until 6.7.0! VIC verison 1.3.0 in contrast to leaves version 6.0.0 U2 as well as 6.5.0. See VMware Product Interoperability Matrices. Of course the same applies to the upgrade path of the solution itself. You can check the Upgrade Path for every specific VMware solution also over the VMware Product Interoperability Matrices. You´ll find the available upgrade paths for VIC HERE . Let´s now take a look at the interoperability for vSphere Integrated Containers: We can see that an upgrade from version 1.3.1 to 1.4(.x) is supported and this is for us the final sign to start. The upgrade itself can be splitted into four phases. VIC Appliance rollout in the desired and supported version Download of the newest VIC Engine Bundle (vic-machine) VIC vSphere Client Plug-In upgrade Upgrade of the Virtual Container Host(s) Before we initiate the deployment of the new VIC Appliance, we´ll validate our used versions. For the VIC Appliance itself we´ll simply start the VMware Remote Console which will then show us the currently used version. Version (Github versioning): 1.3.1 (Tag) 3409 (Build) 132fb13 (last Commit) Next, we´ll check the version of our running Virtual Container Host(s) by using the command docker info. As described in my previous post vSphere Integrated Containers Part IV: docker run a Container-VM we can export our VCH through it´s IP-Address and port via export DOCKER_HOST=192.168.100.220:2376. After we´ve exported it we´ll fire up docker --tls info what will show us the following output: Containers: 7 Running: 7 Paused: 0 Stopped: 0 Images: 5 Server Version: v1.3.1-16055-afdab46 Storage Driver: vSphere Integrated Containers v1.3.1-16055-afdab46 Backend Engine VolumeStores: default vSphere Integrated Containers v1.3.1-16055-afdab46 Backend Engine: RUNNING VCH CPU limit: 22800 MHz VCH memory limit: 30.49 GiB VCH CPU usage: 79 MHz VCH memory usage: 1.637 GiB VMware Product: VMware vCenter Server VMware OS: linux-x64 VMware OS version: 6.7.0 Registry Whitelist Mode: disabled. All registry access allowed. Plugins: Volume: vsphere Network: bridge container-net Log: Swarm: inactive Operating System: linux-x64 OSType: linux-x64 Architecture: x86_64 CPUs: 22800 Total Memory: 30.49GiB ID: vSphere Integrated Containers Docker Root Dir: Debug Mode (client): false Debug Mode (server): false Registry: registry.hub.docker.com Experimental: false Live Restore Enabled: false The server version which is running within our VCH is v1.3.1-16055-afdab46. Another way to get the desired info is to make use of the vic-machine command line utility with the option inspect: ./vic-machine-darwin inspect \\ --target lab-vcsa67-001.lab.jarvis.local/Datacenter-South \\ --user adm.jarvis@LAB.JARVIS.LOCAL \\ --thumbprint 4F:D3:9B:50:00:31:D9:84:9D:DA:CF:57:21:D6:0D:11:89:78:97:26 \\ --name vch-elk The output will also show us the VCH version and the Installer version from which the VCH got deployed as well… INFO[0000] vSphere password for adm.jarvis@LAB.JARVIS.LOCAL: INFO[0004] ### Inspecting VCH #### INFO[0005] Validating target INFO[0005] INFO[0005] VCH ID: VirtualMachine:vm-1190 INFO[0005] INFO[0005] Installer version: v1.3.1-16055-afdab46 INFO[0005] VCH version: v1.3.1-16055-afdab46 INFO[0005] INFO[0005] VCH upgrade status: INFO[0005] Installer has same version as VCH INFO[0005] No upgrade available w","date":"2018-10-19","objectID":"/post/upgrade-vmware-vsphere-integrated-containers/:0:0","tags":["October2018","VIC","Container","VMware","vSphere","Docker","Upgrade"],"title":"Upgrade vSphere Integrated Containers","uri":"/post/upgrade-vmware-vsphere-integrated-containers/"},{"categories":["VMware","Container"],"content":"Phase I ","date":"2018-10-19","objectID":"/post/upgrade-vmware-vsphere-integrated-containers/:1:0","tags":["October2018","VIC","Container","VMware","vSphere","Docker","Upgrade"],"title":"Upgrade vSphere Integrated Containers","uri":"/post/upgrade-vmware-vsphere-integrated-containers/"},{"categories":["VMware","Container"],"content":"VIC Appliance Upgrade v1.3.1 to 1.4.1 I suppose that you are familiar with the VIC Appliance deployment because this post described the upgrade of an already deployed VIC appliance but in case not, you can go here first. Download the latest bits through this link: www.vmware.com/go/download-vic. Start the deployment and be aware of the fact, that this is NOT an Inplace-Upgrade and therefore we have to enter a new IP address as well as FQDN. If you wish to reuse the originally IP address after the upgrade, reconfigure the old appliance to use a temporary IP address before you start the upgrade. VIC Appliance –\u003e Edit Settings –\u003e vApp Options At this point I´d like to quote the following astracts from the Upgrade section of the official documentation page: Important The upgrade process copies data from the old appliance to the new appliance. Consequently, if you deployed the appliances to a cluster, the virtual disks for the two appliances must be located in the same datastore cluster. \u0026 After the new appliance has initialized, do not go to the Getting Started page of the appliance. Logging in to the Getting Started page for the first time initializes the appliance. Initialization is only applicable to new installations and causes upgraded appliances not to function correctly. \u0026 You cannot upgrade between untagged, open source builds of the same release like 1.4.3 open source build to 1.4.3 official build for example. After the deployment of the new VIC Appliance we´ll establish a ssh connection to it. ssh root@192.168.100.161 The authenticity of host '192.168.100.161 (192.168.100.161)' can't be established. ECDSA key fingerprint is SHA256:iVIdb5Sv1pxUsiqelop7DVubJcvjuHHbPSJWBg3No1g. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added '192.168.100.161' (ECDSA) to the list of known hosts. ########################################################################## ## SSH access to the vSphere Integrated Containers Appliance can be ## ## used in exceptional cases that cannot be handled through standard ## ## remote management or CLI tools. This is primarily intended for use ## ## in break-fix scenarios, under the guidance of VMware GSS. ## ########################################################################## Password: ########################################################################## ## SSH access to the vSphere Integrated Containers Appliance can be ## ## used in exceptional cases that cannot be handled through standard ## ## remote management or CLI tools. This is primarily intended for use ## ## in break-fix scenarios, under the guidance of VMware GSS. ## ########################################################################## Use cd /etc/vmware/upgrade to go into the directory which contains the ./upgrade.sh script and execute it. You´ll be promped to enter the neccassary information. As an alternative you can also specify the neccassary arguments upfront like this: ./upgrade.sh --target 192.168.178.72 \\ ### vCenter Server --username 'administrator@jarvis.local' \\ ### User with appropriate permissions --password '*********' \\ ### Password --fingerprint '192.168.178.72 4F:D3:9B...' \\ ### vCenter Server Fingerprint --dc Datacenter-North \\ ### vSphere Datacenter --embedded-psc \\ ### Platform Service Controller (--embedded-psc or --external-psc) --appliance-target 192.168.100.160 \\ ### VIC Appliance where we´d upgrade from --appliance-username root \\ ### VIC Appliance root user --appliance-password VMware1234!?! \\ ### root user password --appliance-version v1.3.1 \\ ### VIC Appliance version where we come from --destroy ### Destroy the the old appliance after the upgrade is finished. See: Specify Command Line Options During Appliance Upgrade The first time I ran the upgrade it ended up in an error which was related to the “? letter” which I´ve used in my password. root@vic02 [ /etc/vmware/upgrade ]# ./upgrade.sh ------------------------------- VIC Appliance Upgrade to v1.4.1 -----------","date":"2018-10-19","objectID":"/post/upgrade-vmware-vsphere-integrated-containers/:1:1","tags":["October2018","VIC","Container","VMware","vSphere","Docker","Upgrade"],"title":"Upgrade vSphere Integrated Containers","uri":"/post/upgrade-vmware-vsphere-integrated-containers/"},{"categories":["VMware","Container"],"content":"Phase II ","date":"2018-10-19","objectID":"/post/upgrade-vmware-vsphere-integrated-containers/:2:0","tags":["October2018","VIC","Container","VMware","vSphere","Docker","Upgrade"],"title":"Upgrade vSphere Integrated Containers","uri":"/post/upgrade-vmware-vsphere-integrated-containers/"},{"categories":["VMware","Container"],"content":"Download of the newest VIC Engine Bundle (vic-machine) In phase two we´ll download the VIC Engine Bundle which includes the following: Scripts for the VIC vSphere Client Plug-In like install, upgrade, or remove The latest version of the vic-machine utility I´ve already covered this topic in my previous post vSphere Integrated Containers Part III: Deployment of a Virtual Container Host right at the beginning. Open the VIC Getting Started page by entering your VIC Address with port 9443 into a browser and find the download-button under the point Infrastructure Deployment Tools. Otherwise you can go directly to: https://your VIC Appliance address:9443/files/ ","date":"2018-10-19","objectID":"/post/upgrade-vmware-vsphere-integrated-containers/:2:1","tags":["October2018","VIC","Container","VMware","vSphere","Docker","Upgrade"],"title":"Upgrade vSphere Integrated Containers","uri":"/post/upgrade-vmware-vsphere-integrated-containers/"},{"categories":["VMware","Container"],"content":"Phase III ","date":"2018-10-19","objectID":"/post/upgrade-vmware-vsphere-integrated-containers/:3:0","tags":["October2018","VIC","Container","VMware","vSphere","Docker","Upgrade"],"title":"Upgrade vSphere Integrated Containers","uri":"/post/upgrade-vmware-vsphere-integrated-containers/"},{"categories":["VMware","Container"],"content":"VIC vSphere Client Plug-In upgrade Now that we have the new VIC Enginge Bundle available we´ll make use of the Upgrade Script for the VIC vSphere Client Plug-In. Note This applies only until version VIC 1.4.1! Use a terminal and you´ll find the script deposited in the path /vic141/ui/vcsa on your local computer or remote execution host (wherever you´ve downloaded it). total 72 drwxr-xr-x@ 6 rguske staff 192 Aug 16 23:22 . drwxr-xr-x@ 8 rguske staff 256 Jul 3 17:37 .. -rw-r--r--@ 1 rguske staff 937 Aug 16 23:22 configs -rwxr-xr-x@ 1 rguske staff 10364 Jun 22 22:28 install.sh -rwxr-xr-x@ 1 rguske staff 6002 Jun 22 22:28 uninstall.sh -rwxr-xr-x@ 1 rguske staff 12059 Jun 22 22:28 upgrade.sh Execute ./upgrade.sh so that the script starts prompting you after neccessary information regarding the vCenter Server target and user credentials. This is how it will look like: ------------------------------------------------------------- This script will upgrade vSphere Integrated Containers plugin for vSphere Client (HTML) and vSphere Web Client (Flex). Please provide connection information to the vCenter Server. ------------------------------------------------------------- Enter FQDN or IP to target vCenter Server: lab-vcsa67-001.lab.jarvis.local Enter your vCenter Administrator Username: administrator@jarvis.local Enter your vCenter Administrator Password: SHA-1 key fingerprint of host 'lab-vcsa67-001.lab.jarvis.local' is '4F:D3:9B:50:00:31:D9:84:9D:DA:CF:57:21:D6:0D:11:89:78:97:26' Are you sure you trust the authenticity of this host (yes/no)? yes ------------------------------------------------------------- Checking existing plugins... ------------------------------------------------------------- Plugin with key 'com.vmware.vic' is already registered with VC. (Version: 1.3.1.687) Plugin with key 'com.vmware.vic.ui' is already registered with VC (Version: 1.3.1.687) The version you are about to install is '1.4.1.1262'. Are you sure you want to continue (yes/no)? yes ------------------------------------------------------------- Preparing to upgrade vCenter Extension vSphere Integrated Containers-FlexClient... ------------------------------------------------------------- Aug 29 2018 23:12:07.794+02:00 INFO ### Installing UI Plugin #### Aug 29 2018 23:12:07.919+02:00 INFO Removing existing plugin to force install Aug 29 2018 23:12:08.164+02:00 INFO Removed existing plugin Aug 29 2018 23:12:08.164+02:00 INFO Installing plugin Aug 29 2018 23:12:08.484+02:00 INFO Installed UI plugin ------------------------------------------------------------- Preparing to upgrade vCenter Extension vSphere Integrated Containers-H5Client... ------------------------------------------------------------- Aug 29 2018 23:12:08.535+02:00 INFO ### Installing UI Plugin #### Aug 29 2018 23:12:08.664+02:00 INFO Removing existing plugin to force install Aug 29 2018 23:12:08.730+02:00 INFO Removed existing plugin Aug 29 2018 23:12:08.730+02:00 INFO Installing plugin Aug 29 2018 23:12:08.865+02:00 INFO Installed UI plugin Aug 29 2018 23:12:09.230+02:00 WARN ignoring potential product VM vm-321: not powered on Aug 29 2018 23:12:09.237+02:00 INFO Found 1 VM(s) tagged as OVA Aug 29 2018 23:12:09.247+02:00 INFO Attempting to configure ManagedByInfo Aug 29 2018 23:12:09.485+02:00 INFO Successfully configured ManagedByInfo If all the entered information were correct and most of the time it depends on the correct user privileges, it will output… -------------------------------------------------------------- Upgrade successful. Restart the vSphere Client services. All vSphere Client users must log out and log back in again to see the vSphere Integrated Containersplug-in. Exited successfully Finally and as requested in the last lines of the above output, restart the vSphere Client services on your vCenter Server Appliance. service-control --stop vsphere-ui \u0026\u0026 service-control --start vsphere-ui service-control --stop vsphere-client \u0026\u0026 service-control --start vsphere-client In case yo","date":"2018-10-19","objectID":"/post/upgrade-vmware-vsphere-integrated-containers/:3:1","tags":["October2018","VIC","Container","VMware","vSphere","Docker","Upgrade"],"title":"Upgrade vSphere Integrated Containers","uri":"/post/upgrade-vmware-vsphere-integrated-containers/"},{"categories":["VMware","Container"],"content":"Upgrading v1.4.1 to v1.4.3 This phase (II) of the VIC Upgrade-process is now obsolete with version 1.4.3. The upgrade of the vSphere (H5) Plug-In is now integrated into the whole process. As you can see through the following output, the question after the VIC UI Plugin upgrade appeared when you start the VIC Appliance upgrade: Upgrade VIC UI Plugin? (y/n): and of course I answered it with y (=yes)…what else?! root@vic01 [ /etc/vmware/upgrade ]# ./upgrade.sh ------------------------------- VIC Appliance Upgrade to v1.4.3 ------------------------------- 2018-10-09 21:43:15 [=] Values containing $ (dollar sign), ` (backquote), \\' (single quote), \" (double quote), and \\ (backslash) will not be substituted properly. 2018-10-09 21:43:15 [=] Change any input (passwords) containing these values before running this script. Enter vCenter Server FQDN or IP: lab-vcsa67-001.lab.jarvis.local Enter vCenter Administrator Username: admin@jarvis.local Enter vCenter Administrator Password: If using an external PSC, enter the FQDN of the PSC instance (leave blank otherwise): If using an external PSC, enter the PSC Admin Domain (leave blank otherwise): Please verify the vCenter IP and TLS fingerprint: lab-vcsa67-001.lab.jarvis.local 4F:D3:9B:50:00:31:D9:84:9D:DA:CF:57:21:D6:0D:11:89:78:97:26 Is the fingerprint correct? (y/n): y Enter vCenter Datacenter of the old VIC appliance: Datacenter-North Enter old VIC appliance IP: 192.168.100.161 Enter old VIC appliance username: root Upgrade VIC UI Plugin? (y/n): y 2018-10-09 21:43:58 [=] ------------------------- Starting upgrade 2018-10-09 21:43:15 +0000 UTC 2018-10-09 21:44:00 [=] Please enter the VIC appliance password for root@192.168.100.161 The authenticity of host '192.168.100.161 (192.168.100.161)' can't be established. ECDSA key fingerprint is SHA256:MTj3Ak65Uem4uiipiygyOpVJuik7iawz09hFsz7tSM4. Are you sure you want to continue connecting (yes/no)? yes Password: 2018-10-09 21:44:09 [=] 2018-10-09 21:44:09 [=] Detected old appliance's version as v1.4.1. 2018-10-09 21:44:09 [=] If the old appliance's version is not detected correctly, please enter \"n\" to abort the upgrade and contact VMware support. 2018-10-09 21:44:09 [=] 2018-10-09 21:44:09 [=] Do you wish to proceed with upgrade? [y/n] y 2018-10-09 21:44:17 [=] Continuing with upgrade 2018-10-09 21:44:17 [=] 2018-10-09 21:44:19 [=] Waiting for old VIC appliance to power off 2018-10-09 21:44:35 [=] Waiting for old VIC appliance to power off 2018-10-09 21:44:50 [=] Waiting for old VIC appliance to power off 2018-10-09 21:45:05 [=] Waiting for old VIC appliance to power off 2018-10-09 21:45:20 [=] Waiting for old VIC appliance to power off 2018-10-09 21:45:39 [=] Migrating old disks to new VIC appliance... 2018-10-09 21:45:40 [=] Copying old data disk. Please wait. 2018-10-09 21:45:45 [=] Copying old database disk. Please wait. 2018-10-09 21:45:51 [=] Copying old log disk. Please wait. 2018-10-09 21:45:59 [=] Finished attaching migrated disks to new VIC appliance 2018-10-09 21:45:59 [=] Preparing upgrade environment 2018-10-09 21:45:59 [=] Disabling and stopping Admiral and Harbor 2018-10-09 21:45:59 [=] Registering the appliance in PSC 2018-10-09 21:46:00 [=] Waiting for register appliance... 2018-10-09 21:46:39 [=] Finished preparing upgrade environment 2018-10-09 21:46:39 [=] ------------------------- Starting VIC UI Plugin Upgrade 2018-10-09 21:43:15 +0000 UTC 2018-10-09 21:46:41 [=] ------------------------- Starting Admiral Upgrade 2018-10-09 21:43:15 +0000 UTC 2018-10-09 21:46:41 [=] Performing pre-upgrade checks 2018-10-09 21:46:41 [=] Starting Admiral upgrade 2018-10-09 21:47:22 [=] Updating Admiral configuration 2018-10-09 21:47:23 [=] Restarting Admiral 2018-10-09 21:48:12 [=] ------------------------- Starting Harbor Upgrade 2018-10-09 21:43:15 +0000 UTC 2018-10-09 21:48:12 [=] Performing pre-upgrade checks 2018-10-09 21:48:12 [=] Starting Harbor upgrade 2018-10-09 21:48:12 [=] [=] Shutting down Harbor 2018-10-09 21:48:12 [=] [=] Migra","date":"2018-10-19","objectID":"/post/upgrade-vmware-vsphere-integrated-containers/:4:0","tags":["October2018","VIC","Container","VMware","vSphere","Docker","Upgrade"],"title":"Upgrade vSphere Integrated Containers","uri":"/post/upgrade-vmware-vsphere-integrated-containers/"},{"categories":["VMware","Container"],"content":"Phase IV ","date":"2018-10-19","objectID":"/post/upgrade-vmware-vsphere-integrated-containers/:5:0","tags":["October2018","VIC","Container","VMware","vSphere","Docker","Upgrade"],"title":"Upgrade vSphere Integrated Containers","uri":"/post/upgrade-vmware-vsphere-integrated-containers/"},{"categories":["VMware","Container"],"content":"Upgrade of the Virtual Container Host(s) Last but not least we´ll upgrade our Virtual Container Host(s). This will also be done by using the vic-machine utility with the option upgrade. Before I begin, I´ll check all current versions of my running VCH´s. Use docker --tls version to get the desired information. For my VCH based and deployed on version 1.3.1 the output looks like: Client: Version: 18.06.1-ce API version: 1.25 (downgraded from 1.38) Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17:21:31 2018 OS/Arch: darwin/amd64 Experimental: false Server: Engine: Version: 1.3.1 API version: 1.25 (minimum version 1.19) Go version: go1.8.3 Git commit: afdab46 Built: Mon Feb 5 15:35:58 2018 OS/Arch: linux/amd64 Experimental: true …and for my VCH based on version 1.4.1 it looks like this: Client: Version: 18.06.1-ce API version: 1.25 (downgraded from 1.38) Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17:21:31 2018 OS/Arch: darwin/amd64 Experimental: false Server: Engine: Version: 1.4.1 API version: 1.25 (minimum version 1.19) Go version: go1.8.7 Git commit: b3a6192 Built: Tue Jul 3 15:05:57 2018 OS/Arch: linux/amd64 Experimental: true Now let´s upgrade our VCH from version 1.4.1 to our latest version: #vSphereIntegratedContainers 1.4.3 is available with cool new enhancements like shell auto-complete function or vic-machine create --no-proxy option to skip proxying for certain URLs or domains. Read more about the new release here: https://t.co/f21abf64TQ #VIC — Robert Guske (@vmw_rguske) September 18, 2018 The upgrade option from vic-machine requires the VM-ID from our VCH and we can get this ID by using vic-machine inspect. That´s the point were you´ll hopefully say to yourself “AH! We´ve already made use of this option to get the VCH version displayed” and you`re right “Watson” ;-)! And I´ve also mentioned that I´ll come back to this point. Because we now will fire up this command from the new VIC Engine Bundle… ./vic-machine-darwin inspect \\ --target lab-vcsa67-001.lab.jarvis.local/Datacenter-North \\ --user adm.jarvis@LAB.JARVIS.LOCAL \\ --thumbprint 4F:D3:9B:50:00:31:D9:84:9D:DA:CF:57:21:D6:0D:11:89:78:97:26 \\ --name vch-elk …and the output will show us this time the following: INFO[0000] vSphere password for adm.jarvis@LAB.JARVIS.LOCAL: INFO[0004] ### Inspecting VCH #### INFO[0004] Validating target INFO[0005] INFO[0005] Installer version: v1.4.3-20031-fa9732d INFO[0005] VCH version: v1.4.1-19562-b3a6192 INFO[0005] INFO[0005] VCH upgrade status: INFO[0005] Upgrade available WARN[0005] Unable to identify address acceptable to host certificate, using assigned client IP as host address. INFO[0005] INFO[0005] VCH ID: vm-1190 INFO[0005] INFO[0005] VCH Admin Portal: INFO[0005] https://192.168.100.220:2378 INFO[0005] INFO[0005] Published ports can be reached at: INFO[0005] 192.168.100.22 INFO[0005] INFO[0005] Management traffic will use: INFO[0005] 192.168.100.220 INFO[0005] INFO[0005] Docker environment variables: INFO[0005] DOCKER_HOST=192.168.100.220:2376 INFO[0005] INFO[0005] Connect to docker: INFO[0005] docker -H 192.168.100.220:2376 --tls info INFO[0005] Completed successfully INFO[0005] Installer version: v1.4.3-20031-fa9732d INFO[0005] VCH version: v1.4.1-19562-b3a6192 INFO[0005] VCH upgrade status: INFO[0005] Upgrade available Upgrade available sounds good to me! Here we go: ./vic-machine-darwin upgrade \\ --target lab-vcsa67-001.lab.jarvis.local/Datacenter-North \\ --user administrator@jarvis.local \\ --id vm-1190 \\ --force …and a couple of seconds later… INFO[0000] vSphere password for administrator@jarvis.local: INFO[0004] ### Upgrading VCH #### INFO[0004] Validating target INFO[0004] INFO[0004] VCH ID: VirtualMachine:vm-1190 INFO[0005] Creating directory [lab-ds-001] vch-elk INFO[0005] Datastore path is [lab-ds-001] vch-elk INFO[0005] Uploading ISO images INFO[0006] Uploading appliance.iso as V1.4.3-20031-FA9732D-appliance.iso INFO[0016] Uploading bootstrap.iso as V1.4.3-20031-FA9732D-bootstrap.iso","date":"2018-10-19","objectID":"/post/upgrade-vmware-vsphere-integrated-containers/:5:1","tags":["October2018","VIC","Container","VMware","vSphere","Docker","Upgrade"],"title":"Upgrade vSphere Integrated Containers","uri":"/post/upgrade-vmware-vsphere-integrated-containers/"},{"categories":["VMware","Container"],"content":"Conclusion We´ve went through each individual phase of the upgrade-process of vSphere Integrated Containers. The upgrade to a newer version is simple and improvements were made in version 1.4.3. When upgrading to a newer version we just have to observe the following aspects: Upgrading vSphere Integrated Containers is not that difficult and it´s well documented. Upgrading vSphere Integrated Containers Upgrade between untagged, open source builds of the same release like 1.4.3 open source build to 1.4.3 official build is not supported The upgrade of the vSphere Client Plug-In is now in the VIC Appliance upgrade integrated with version 1.4.3 While upgrading the Virtual Conatiner Host(s), the VCH get´s rebooted which means that when you have NOT configured a dedicated Container-Network for your VCH, your application cannot be reached over the VCH IP-Address and it´s exposed port. Tip To avoid this, make use of one of the big benefits vSphere Integrated Containers can leverage here and configure a dedicated Container-Network to your VCH! In addition have a look at vSphere Integrated Containers Part IV: docker run a Container-VM You can easily test this by instantiating a (vmwarecna) NGINX Container-VM with the --net option, start upgrading the Virtual Container Host and see if the C-VM will become unreachable. Example: docker --tls run --name vmwarenginx --net vch-blog-net -d -p 8080:80 vmwarecna/nginx While upgrading the VCH, the NGINX Container-VM is still reachable. Please find the Release Notes for vSphere Integrated Containers here: VMware vSphere Integrated Containers Docs I hope you won´t find this post not too long for this kind of topic, but I wanted to be as detailed as possible. ","date":"2018-10-19","objectID":"/post/upgrade-vmware-vsphere-integrated-containers/:6:0","tags":["October2018","VIC","Container","VMware","vSphere","Docker","Upgrade"],"title":"Upgrade vSphere Integrated Containers","uri":"/post/upgrade-vmware-vsphere-integrated-containers/"},{"categories":["VMware","Container","Cloud Native"],"content":"This post will show you how easy it is to instantiate a Nginx Web Server in form of a so called Container-VM directly on vSphere.","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-4-docker-run-a-container-vm/","tags":["July2018","VIC","Container","Docker","VMware","vSphere","docker"],"title":"vSphere Integrated Containers Part IV: docker run a Container-VM","uri":"/post/vmware-vsphere-integrated-containers-part-4-docker-run-a-container-vm/"},{"categories":["VMware","Container","Cloud Native"],"content":"Instantiate a nginx cVM Now that we´ve become more familiar with the Virtual Container Host, it´s time to run our first Container-VM on it. The following example will show you how easy it is to instantiate an Nginx Web Server container-image. We first pull down the image from Docker Hub to our Datastore by using docker -H 192.168.100.222:2376 --tls pull vmwarecna/nginx. You´ll see some running tasks in your vCenter taskbar under Recent Tasks. Validate the downloaded image with docker -H 192.168.100.222:2376 --tls images. docker -H 192.168.100.222:2376 --tls images REPOSITORY TAG IMAGE ID CREATED SIZE vmwarecna/nginx latest 2492b68e515c 3 years ago 93.5MB Alright! This was successful and by runnig the following command, the VIC Engine will deploy a Nginx Container-VM with 512MB vRAM (-m) which will listen to port (-p) 8080. docker -H 192.168.100.222:2376 --tls run --name nginx1 -m 512M -d -p 8080:80 vmwarecna/nginx Let´s check if our container is up and running. First in vCenter: …and second via command line: docker -H 192.168.100.222:2376 --tls ps -a docker -H 192.168.100.222:2376 --tls ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1ce3a5a70099 nginx \"nginx -g daemon off;\" Up 1m 192.168.100.2:8080-\u003e80/tcp nginx1 Just like any other Container Host, virtual or physical, you´ll reach your container via the Container Host IP-Address and port. I intended to show the same in my example before we configure a dedicated Container Network afterwards. We can reach the Nginx Web Server site via the VCH IP-Address and port 8080. If you´re wondering why I always used -H vch-ip-address --tls in my example that´s because I´ve not “exported” my VCH through it´s IP-Address and port yet. We can do this by the following: export DOCKER_HOST=192.168.100.222:2376 After we´ve exported the host, we can go the classical way and use commands like: docker pull, docker images, docker ps -a etc. To get a complete list of all available and supported docker commands you´ve to go here: VIC - Supported Docker Commands ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-4-docker-run-a-container-vm/:1:0","tags":["July2018","VIC","Container","Docker","VMware","vSphere","docker"],"title":"vSphere Integrated Containers Part IV: docker run a Container-VM","uri":"/post/vmware-vsphere-integrated-containers-part-4-docker-run-a-container-vm/"},{"categories":["VMware","Container","Cloud Native"],"content":"A dedicated Network for Conatiners I´ve mentioned in the Networking-Part of this post, that I´ll come back to the point where I´ll configure a Container Network to an existing VCH. As a prerequisite we have to create a new Distributed Port Group on our vDistributed Switch and ensure that all other network related configuration (e.g. VLAN) are set. I´ve gave my new dPG the name vic-container. As also mentioned we´ll make use of the vic-machine configure option to add the new network to our VCH. Before we begin, we need to know the internal ID of it. Use vic-machine inspect with the virtual machine name of your VCH: ./vic-machine-darwin inspect \\ --target lab-vcsa67-001.lab.jarvis.local/Datacenter-South \\ --user adm.jarvis@LAB.JARVIS.LOCAL \\ --thumbprint 4F:D3:9B:50:00:31:D9:84:9D:DA:CF:57:21:D6:0D:11:89:78:97:26 \\ --name vch02 The output will show us the internal ID which is in my case vm-212: INFO[0008] ### Inspecting VCH #### INFO[0008] Validating target INFO[0009] INFO[0009] VCH ID: VirtualMachine:vm-212 INFO[0009] INFO[0009] Installer version: v1.4.0-18893-6c385b0 INFO[0009] VCH version: v1.4.0-18893-6c385b0 INFO[0009] INFO[0009] VCH upgrade status: INFO[0009] Installer has same version as VCH INFO[0009] No upgrade available with this installer version WARN[0009] Unable to identify address acceptable to host certificate, using assigned client IP as host address. INFO[0009] INFO[0009] VCH Admin Portal: INFO[0009] https://192.168.100.222:2378 INFO[0009] INFO[0009] Published ports can be reached at: INFO[0009] 192.168.100.222 INFO[0009] INFO[0009] Docker environment variables: INFO[0009] DOCKER_HOST=192.168.100.222:2376 INFO[0009] INFO[0009] Connect to docker: INFO[0009] docker -H 192.168.100.222:2376 --tls info INFO[0009] Completed successfully Okay, now that we have the ID we can continue with adding the new container network by using vic-machine configure: ./vic-machine-darwin configure \\ --target lab-vcsa67-001.lab.jarvis.local/Datacenter-South \\ --user adm.jarvis@lab.jarvis.local \\ --thumbprint 4F:D3:9B:50:00:31:D9:84:9D:DA:CF:57:21:D6:0D:11:89:78:97:26 \\ --id vm-212 \\ --container-network vic-container:vic-container-network \\ --container-network-gateway vic-container:192.168.100.1/24 \\ --container-network-ip-range vic-container:192.168.100.0/24 \\ --container-network-dns vic-container:192.168.100.80 \\ --timeout 5m I´ve used the option --timeout 5m because I ran in a timeout without it. I´m pretty sure this was performance related to my nested environment. But anyway! The configuration was successful and we can validate it with vic-machine inspect: docker --tls inspect vic-container-network [ { \"Name\": \"vic-container-network\", \"Id\": \"a6d0296afcace305048480244cca65564708a6dec474eb7d7a865593afdb593b\", \"Created\": \"2018-07-12T22:11:39.685754355Z\", \"Scope\": \"\", \"Driver\": \"external\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"192.168.100.0/24\", \"Gateway\": \"192.168.100.1\" } ] }, \"Internal\": false, \"Attachable\": false, \"Containers\": {}, \"Options\": {}, \"Labels\": {} } ] At the end we need to validate the functionality as well and we´ll deploy a new Nginx but this time with the dedicated Container Network vic-container-network and without declaring the port. docker --tls run --name nginx3 --net vic-container-network vmwarecna/nginx There you go! :-) ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-4-docker-run-a-container-vm/:2:0","tags":["July2018","VIC","Container","Docker","VMware","vSphere","docker"],"title":"vSphere Integrated Containers Part IV: docker run a Container-VM","uri":"/post/vmware-vsphere-integrated-containers-part-4-docker-run-a-container-vm/"},{"categories":["VMware","Container","Cloud Native"],"content":"World Cup 2018 CLI Dashboad Container Just before coming to the end of my VIC series and because it´s Worldcup time, I´d like to show you a nice World Cup 2018 CLI dashboard by Cédric Blondeau running in a container. docker --tls pull cedricbl/world-cup-2018-cli-dashboard \u0026\u0026 \\ docker --tls run --name Worldcup2018 -ti -e TZ=Europe/Berlin cedricbl/world-cup-2018-cli-dashboard Isn´t that cool?! :-) Try it out! Previous articles: vSphere Integrated Containers Part III: Deployment of a Virtual Container Host vSphere Integrated Containers Part II: vSphere Client Plug-In vSphere Integrated Containers Part I: OVA Deployment vSphere Integrated Containers: Introduction ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-4-docker-run-a-container-vm/:3:0","tags":["July2018","VIC","Container","Docker","VMware","vSphere","docker"],"title":"vSphere Integrated Containers Part IV: docker run a Container-VM","uri":"/post/vmware-vsphere-integrated-containers-part-4-docker-run-a-container-vm/"},{"categories":["VMware","Container","Cloud Native"],"content":"In this part I´d like to guide you through the Deployment of a Virtual Container Host aka VCH. I definitely don ́t want to make it a habit to use already written descriptions, but this one described it very well, thus I had like to quote this: Quote A VCH is the virtual functional equivalent of a Linux VM running Docker. From a Docker client point of view, the Virtual Container Host looks very similar to a native Docker host. Hence, there are differences between a native Docker host and a Virtual Container Host (VCH), and between a Linux container and a container VM. Source: Architecture Overview (VIC 1.2) Now let´s download the vic-bundle.tar.gz to our desktop locally to make use of the vic-machine command line utility what gives us the possibility to deploy VCHs and to manage their lifecycle. Open the VIC Getting Started page which you can reach by using the VIC IP, FQDN or DNS short name and port 9443. So in my case: https://vic01:9443. You´ll see in the lower left corner Infrastructure Deployment Tools where you can download the vic-bundle.tar.gz file. The other option is to use the url directly: https://vic01:9443/files/. It´s up to you which way you choose, it´s just important to have the vic-machine utility on the machine where you´ll execute the commands from (local or a remote-execution host). Before we finally can begin with the deployment of our first VCH, we need to open the required port for the outgoing communication, via Serial over LAN, and using port 2377 between the ESXi Host(s) and our VCH. This is the first time we´ll make use of the vic-machine utility which is basically for lifecycle-management operations of a Virtual Container Host. There´s support for three platforms on which you can use vic-machine. Here is an short example for the create command: ### Windows ./vic-machine-windows create --option argument --option argument ### Linux ./vic-machine-linux create --option argument --option argument ### MacOS ./vic-machine-darwin create --option argument --option argument In the following you´ll find all neccessary pieces of information: Running vic-machine Commands. Before we can really open up the required port it´s neccassary to know the Thumbprint of your vCenter Server. By executing the following you should get what you need: ### In my example ./vic-machine-darwin ls --target administrator@jarvis.local@lab-vcsa67-001/Datacenter-South I´ll receive an certificate related error but the result counts - the output shows us the vCenter Server Thumbprint. INFO[0000] vSphere password for administrator@jarvis.local: INFO[0003] ### Listing VCHs #### ERRO[0003] Failed to verify certificate for target=lab-vcsa67-001 (thumbprint=4F:D3:9B:50:00:31:D9:84:9D:DA:CF:57:21:D6:0D:11:89:78:97:26) ERRO[0003] List cannot continue - failed to create validator: x509: certificate is valid for lab-vcsa67-001.lab.jarvis.local, not lab-vcsa67-001 ERRO[0003] -------------------- ERRO[0003] vic-machine-darwin ls failed: list failed Switch to your downloaded and unzipped vic-bundle folder. Execute the following command to open the needed ports: ./vic-machine-darwin update firewall \\ --target lab-vcsa67-001/Datacenter-South \\ --user administrator@jarvis.local \\ --compute-resource nested-vSAN \\ --thumbprint 4F:D3:9B:50:00:31:D9:84:9D:DA:CF:57:21:D6:0D:11:89:78:97:26 \\ --allow You´ll get an output similar like this: Now let´s go to the point of what the title of Part III is promising you and we´ll start with the deployment of a VCH through the wizard which got available through the vic-plugin installation. Open the vSphere Integrated Containers subsection in your vSphere Client and under Virtual Conatiner Hosts choose + NEW VIRTUAL CONTAINER HOST. ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/:0:0","tags":["July2018","VIC","Container","VMware","vSphere","Docker"],"title":"vSphere Integrated Containers Part III: Deployment of a Virtual Container Host","uri":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/"},{"categories":["VMware","Container","Cloud Native"],"content":"General The General section is more important than it appears at the first glance! Because besides the possibility to configure an endpoint for log aggregation and the level of logging deepness, we could enter a naming-prefix for the instanciated Container-VMs what gives us, from a security point of view, more capabilities and flexibility. For instance, we could establish a NSX Security Group by using the naming-prefix as a membership-criteria. Consequently, our Container-VMs becomes dynamically a member of the Security Group which gives us back the control of the East-West traffic within our Datacenter. ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/:1:0","tags":["July2018","VIC","Container","VMware","vSphere","Docker"],"title":"vSphere Integrated Containers Part III: Deployment of a Virtual Container Host","uri":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/"},{"categories":["VMware","Container","Cloud Native"],"content":"Compute Next we have to chose our Compute-Resource where our VCH runs on. That can be an ESXi Host, a Cluster or a Resource-Pool. The default settings for the CPU and Memory Limits (internally referred to as MAX) is set to Unlimited. I assume in most cases these settings can be untouched but if you decide to configure limits (MAX) and reservations (MIN) please remind the following: Info Reservation A reservation specifies the guaranteed minimum allocation for a virtual machine Shares Shares specify the relative importance of a virtual machine (or resource pool). If a virtual machine has twice as many shares of a resource as another virtual machine, it is entitled to consume twice as much of that resource when these two virtual machines are competing for resources. Shares are typically specified as High, Normal, or Low and these values specify share values with a 4:2:1 ratio, respectively. You can also select Custom to assign a specific number of shares (which expresses a proportional weight) to each virtual machine.. Limit Limit specifies an upper bound for CPU, memory, or storage I/O resources that can be allocated to a virtual machine. A server can allocate more than the reservation to a virtual machine, but never allocates more than the limit, even if there are unused resources on the system. The limit is expressed in concrete units (megahertz, megabytes, or I/O operations per second). CPU, memory, and storage I/O resource limits default to unlimited. When the memory limit is unlimited, the amount of memory configured for the virtual machine when it was created becomes its effective limit. Source: vSphere Resource Management VMware vSphere 6.5 Tip A must have with regards to Resource Management is definetely the VMware vSphere 6.5 Host Resource Deep Dive by Frank Dennemann and Niels Hagoort ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/:2:0","tags":["July2018","VIC","Container","VMware","vSphere","Docker"],"title":"vSphere Integrated Containers Part III: Deployment of a Virtual Container Host","uri":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/"},{"categories":["VMware","Container","Cloud Native"],"content":"Storage To persistently store our container-images we can configure a Datastore in point 3. It´s also possible to set a maximum for the Container-VM Image size but keep in mind, if you configure it once, it can´t be re-configured afterwards. 8GB is set by default and due to the small footprint of a container, what is one of the major benefits of it, this predefined size should be sufficient in most cases. VIC can operate with any type of datastores: local VMFS, shared like NFS and iSCSI or vSAN. I´m running my Homelab on a single physical server and to make use of the benefits from Cluster-Features like DRS, vMotion or HA, I´ve implemented three nested ESXi Hosts and configured a vSAN All-Flash Cluster. Reference: virtuallyGhetto.com | Nested Virtualization by William Lam Therefore, I´ve chosen my vSAN Datastore. Containers are ephemeral but this does not necessarily mean that the same applies to data. Of course persistence belongs to containers as well, and databases like e.g. a MongoDB or MySQL have to be stored persistently. As you can see through the picture above, we can make use of docker volumes which can be created and assigned by using the command docker volume create. ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/:3:0","tags":["July2018","VIC","Container","VMware","vSphere","Docker"],"title":"vSphere Integrated Containers Part III: Deployment of a Virtual Container Host","uri":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/"},{"categories":["VMware","Container","Cloud Native"],"content":"Network This part is a little bit more comprehensive and like in most cases a picture quickly creates clarity. Resource: VIC Documentation Networking Requirements The picture above shows you the available networks, which are described by the following: Docker Client Management Network: the network used to interact with the VCH VM via a Docker client vSphere Management Network: the network used by the VCH VM and the Container-VMs to interact with vSphere Public Network: the equivalent of eth0 on a Docker host. This is the network used to expose services to the public world (via –p) Bridge Network(s): the equivalent of Docker0 on a Docker Host. Container Network(s): these are networks containers can attach to directly for inbound/ outbound communications to by-pass the VCH VM I´d like to get your attention on the last network - the Container Network. With VIC, you have the ability to directly connect a Container-VM to a dedicated network what is a differentiator to e.g. the native Docker Container-Host. These Container Networks will be configured during the VCH deployment or afterwards by using the vic-machine configure command. More about this later on. The mapped container networks are available for use by the Docker API and with that a container developer can use classical docker commands like docker network ls to list the container networks or docker run, docker createwith the --network=*mapped-network-name*option. There exists already two great posts about this topic. Check it out: Connecting Containers Directly to External Networks Basic Network Configuration with vSphere Integrated Containers Engine ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/:4:0","tags":["July2018","VIC","Container","VMware","vSphere","Docker"],"title":"vSphere Integrated Containers Part III: Deployment of a Virtual Container Host","uri":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/"},{"categories":["VMware","Container","Cloud Native"],"content":"Security By default, Virtual Container Hosts authenticate connections from Docker API clients by using server and client TLS certificates. This configuration is commonly referred to as tlsverify in documentation about containers and Docker. I like to refer you to the Offical VIC Documentation Page under Section VCH Security. ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/:5:0","tags":["July2018","VIC","Container","VMware","vSphere","Docker"],"title":"vSphere Integrated Containers Part III: Deployment of a Virtual Container Host","uri":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/"},{"categories":["VMware","Container","Cloud Native"],"content":"Operations User The operations user account must exist before you create a VCH. If you are deploying the VCH to a cluster, vSphere Integrated Containers Engine can configure the operations user account with all of the necessary permissions for you. You can see that I use my user Jarvis for day-to-day operations for my VCH´s. More here: Configure the Operations User Important If you are deploying the VCH to a standalone host that is managed by vCenter Server, you must configure the operations user account manually. The option to grant any necessary permissions automatically only applies when deploying VCHs to clusters. ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/:6:0","tags":["July2018","VIC","Container","VMware","vSphere","Docker"],"title":"vSphere Integrated Containers Part III: Deployment of a Virtual Container Host","uri":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/"},{"categories":["VMware","Container","Cloud Native"],"content":"Summary Like always when a summary appears by the end of a wizard - validate your configuration again! I´d recommend to copy the command lines to your notes, so that you can use them for further (manual) deployments. It seems that we´ve gone a long way until here, right? But let us recap: we´ve downloaded the OVA we´ve deployed the VIC-Appliance we´ve installed the VIC vSphere Client Plug-In …and now we´ve ran through the VCH deployment wizard and we´re just one click away to have our first Virtual Container Host deployed. Hit FINISH! Now let´s check the deployment in our vCenter… …and over the command line by using vic-machine (as mentioned) as well: ./vic-machine-darwin ls \\ --target \"lab-vcsa67-001\" \\ --user \"administrator@jarvis.local\" \\ --thumbprint 4F:D3:9B:50:00:31:D9:84:9D:DA:CF:57:21:D6:0D:11:89:78:97:26 INFO[0000] vSphere password for administrator@jarvis.local: INFO[0006] ### Listing VCHs #### INFO[0006] Validating target ID PATH NAME VERSION UPGRADE STATUS vm-212 /Datacenter-South/host/nested-vSAN/Resources vch02 v1.4.0-18893-6c385b0 Up to date Looks good! What infos else? docker -H 192.168.100.223:2376 --tls info Alright, we´ve deployed our first Virtual Container Host we can go ahead with Part IV. Continue with: vSphere Integrated Containers Part IV: docker run a Container-VM Previous articles: vSphere Integrated Containers Part II: vSphere Client Plug-In vSphere Integrated Containers Part I: OVA Deployment vSphere Integrated Containers: Introduction ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/:7:0","tags":["July2018","VIC","Container","VMware","vSphere","Docker"],"title":"vSphere Integrated Containers Part III: Deployment of a Virtual Container Host","uri":"/post/vmware-vsphere-integrated-containers-part-3-deployment-of-a-virtual-container-host/"},{"categories":["VMware","Container","Cloud Native"],"content":"In this section, I´d like to give you a walkthrough on how to install the VIC vSphere Client Plug-In for the vCenter Server Appliance. I´m already using the vCenter Server Appliance aka vCSA in Version 6.7.0. In general, it is important to know that a minimum version of vCenter Server 6.5.0d is required to make use of the plug-in. However! It is also worth to be mentioned that the plug-in is not necessary or a requirement to run VIC as well as for the deployment of a Virtual Container Host aka VCH. But it makes the initial deployment of a VCH easier at the first beginning until you are more and more familiar with the vic-machine utility. The first step from the installation of the vSphere Client Plug-In is to download the vic-machine-bundle from the VIC-Appliance by using curl. To set up the necessary commands from within the vCSA we first have to enable SSH over the Virtual Appliance Management Interface aka VAMI by using port 5480. By default SSH is disabled. https://lab-vcsa67-001:5480 Toggle the switch to enable SSH Login on the vCSA But wait! To make sure if everything went right after the installation, we should check before and after. And how can we check if there isn´t a vic plug-in before the installation and afterwards it gets shown? Correct - over the Managed Object Browser aka MOB. https://lab-vcsa67-001/mob You won´t find anything declared with vic- under content/ ExtensionManager/ extensionList. Now let’s establish a ssh-connection to the vCSA. ssh root@192.168.100.72 Write and execute: shell After the login we now want to download and unpack the vic-bundle tar-file from the VIC Appliance to start the installation of the VIC vSphere Client Plug-In. You should just change my IP-Address to yours in the following lines. export VIC_ADDRESS=192.168.100.160 export VIC_BUNDLE=vic_v1.4.0.tar.gz curl -kL https://${VIC_ADDRESS}:9443/files/${VIC_BUNDLE} -o ${VIC_BUNDLE} tar -zxf ${VIC_BUNDLE} cd vic/ui/VCSA By using the Linux command ls (list)-ltr (sort by change date) you should see the downloaded file. The next and final step before the installation of the vCenter plug-in can begin, is the execution of the installation script. Execute the install.sh script by entering: ./install.sh …and provide the requested data (vCSA FQDN as well as vCenter Server Administrator Credentials). By the end it should look like this: Now let´s check again the extensionList over the vCenter Managed Object Manager - MOB. Voila! Okay, now we are close to make use of the vic plug-in capabilities in our vSphere-WebClient but before we can go ahead, we need to restart the H5-Client as well as the Flex-based vSphere Web Client by running the following commands: service-control --stop vsphere-ui \u0026\u0026 service-control --start vsphere-ui service-control --stop vsphere-client \u0026\u0026 service-control --start vsphere-client Because we want to hold our vCSA as clean as a Appliance should be, we have to get rid of our tracks by starting the cleaning process through the deletion of the unpacked tar-file. rm *.tar.gz rm -R vic Disable SSH when you are finished! The VIC plug-in should now be available under Menu/ vSphere Integrated Containers. Now we have established the Integration into vSphere but what about the Containers? It won´t last long… Continue with: vSphere Integrated Containers Part III: Deployment of a Virtual Container Host vSphere Integrated Containers Part IV: docker run a Container-VM Previous articles: vSphere Integrated Containers Part I: OVA Deployment vSphere Integrated Containers: Introduction ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-2-vsphere-client-plugin/:0:0","tags":["July2018","VIC","Container","Docker","VMware","vSphere"],"title":"vSphere Integrated Containers Part II: vSphere Client Plug-In","uri":"/post/vmware-vsphere-integrated-containers-part-2-vsphere-client-plugin/"},{"categories":["VMware","Container","Cloud Native"],"content":"I am describing the deployment pre-requisites for vSphere Integrated Containers through this post.","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/","tags":["July2018","VIC","Container","Docker","VMware","vSphere"],"title":"vSphere Integrated Containers Part I: OVA Deployment","uri":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/"},{"categories":["VMware","Container","Cloud Native"],"content":"Environment Pre-Requisites vSphere Enterprise Plus license or vSphere Remote Office Branch Office (ROBO) Advanced (!) Dependency on the vDistributed Switch VIC also supports VMware NSX User with administrative credentials to vCenter Internet Access for downloading images min. two vDistributed Switch Port Groups for public communication (VCH to external network) for inter containers communication create a dedicated port group for use as the bridge network for each VCH If DHCP is not available on these segments, please, request a range of free IP-Addresses. Info You´ll find all necessary pieces of information with regards to Licensing as well as Deployment Requirements on the official VIC Github Page. ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/:1:0","tags":["July2018","VIC","Container","Docker","VMware","vSphere"],"title":"vSphere Integrated Containers Part I: OVA Deployment","uri":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/"},{"categories":["VMware","Container","Cloud Native"],"content":"Deploying the OVA To start with, we have to download the latest bits from myvmware.com here –\u003e VIC Version 1.4.1. After having downloaded the 3,12 GB ova-file we´ll start provisioning the VIC Virtual Appliance over the vSphere Web Client onto your vSphere Datacenter, Cluster or ESXi Host. I suppose that you are already familiar with these steps but if not, please go here first. If you´re already familiar with it and you´re more interested in automting an OVA deployment, I´d recommend reading this Post by Romain Decker . ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/:2:0","tags":["July2018","VIC","Container","Docker","VMware","vSphere"],"title":"vSphere Integrated Containers Part I: OVA Deployment","uri":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/"},{"categories":["VMware","Container","Cloud Native"],"content":"Pick the OVA file ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/:2:1","tags":["July2018","VIC","Container","Docker","VMware","vSphere"],"title":"vSphere Integrated Containers Part I: OVA Deployment","uri":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/"},{"categories":["VMware","Container","Cloud Native"],"content":"Select a compute resource ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/:2:2","tags":["July2018","VIC","Container","Docker","VMware","vSphere"],"title":"vSphere Integrated Containers Part I: OVA Deployment","uri":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/"},{"categories":["VMware","Container","Cloud Native"],"content":"Review details ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/:2:3","tags":["July2018","VIC","Container","Docker","VMware","vSphere"],"title":"vSphere Integrated Containers Part I: OVA Deployment","uri":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/"},{"categories":["VMware","Container","Cloud Native"],"content":"License agreements ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/:2:4","tags":["July2018","VIC","Container","Docker","VMware","vSphere"],"title":"vSphere Integrated Containers Part I: OVA Deployment","uri":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/"},{"categories":["VMware","Container","Cloud Native"],"content":"Select storage ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/:2:5","tags":["July2018","VIC","Container","Docker","VMware","vSphere"],"title":"vSphere Integrated Containers Part I: OVA Deployment","uri":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/"},{"categories":["VMware","Container","Cloud Native"],"content":"Select networks At this point, I´d like to stress out again to the VIC documentation regarding the use of SSH . SSH is needed when you perform upgrades or the following: ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/:2:6","tags":["July2018","VIC","Container","Docker","VMware","vSphere"],"title":"vSphere Integrated Containers Part I: OVA Deployment","uri":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/"},{"categories":["VMware","Container","Cloud Native"],"content":"Appliance Configuration If you decide to use static IP-Addresses like me, please use spaces and not commas to separate multiple DNS-Servers. ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/:2:7","tags":["July2018","VIC","Container","Docker","VMware","vSphere"],"title":"vSphere Integrated Containers Part I: OVA Deployment","uri":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/"},{"categories":["VMware","Container","Cloud Native"],"content":"Network Properties I´ve also decided to create an example user through the wizard, which gets the username prefix you´ve chosen in point 5 in this section. I´m fine with the predetermined prefix vic. ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/:2:8","tags":["July2018","VIC","Container","Docker","VMware","vSphere"],"title":"vSphere Integrated Containers Part I: OVA Deployment","uri":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/"},{"categories":["VMware","Container","Cloud Native"],"content":"Ready to complete Lean back and let the vCenter do its job… … …FINISHED! Tip If you think “Hey this H5-Client Dark Theme looks very slick! Where I can toggle the switch?\" unfortunately one has to name, that this is not a feature in the vSphere H5-Client, it´s a Browser-Extension by Jens L. aka BeryJu and available for Chrome and Firefox. You´ll find him on Github as well as on his Blog. Thanks, Jens for the nice work. Attention When you add the extension, VMware will not provide support when you´re facing issues! And - I´d recommend using only browsers where the language is set to English! In other cases, you could hit the issue Other browser language than English breaks CSS inject #36 Here you´ll find the extensions. Dark-vCenter for Google Chrome Dark-vCenter for Mozilla Firefox The next step is to complete the VIC appliance installation through the establishment of the connection to our vCenter Server as well as Platform Service Controller. Here we have to enter the vCenter Server address (FQDN) and the Single Sign-on credentials for a vSphere administrator account. In my case, I´m using an embedded PSC and thus, I can leave the fields for the External PSC Instance empty. If you´ve entered your credentials correctly, you´ll be forwarded to the VIC Getting Started page which you could always open by using the IP-Address or better using the FQDN (of course the short name as well) over port 9443. In my example https://vic01.lab.jarvis.local:9443/ Continue with: vSphere Integrated Containers Part II: vSphere Client Plug-In vSphere Integrated Containers Part III: Deployment of a Virtual Container Host vSphere Integrated Containers Part IV: docker run a Container-VM ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/:2:9","tags":["July2018","VIC","Container","Docker","VMware","vSphere"],"title":"vSphere Integrated Containers Part I: OVA Deployment","uri":"/post/vmware-vsphere-integrated-containers-part-1-ova-deployment/"},{"categories":["VMware","Container","Cloud Native"],"content":"An introduction to VMware's approach to provide a Docker API to Developers and VI Admins.","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-introduction/","tags":["July2018","VIC","Container","VMware","vSphere"],"title":"VMware vSphere Integrated Containers: Introduction","uri":"/post/vmware-vsphere-integrated-containers-introduction/"},{"categories":["VMware","Container","Cloud Native"],"content":"I´m very keen on everything related to Cloud-Native and all topics around the decoupling of a process from the underlying operating system and as well much more on the infrastructure underneath in order to run those distributed systems, which are the result by the end of the day. For me personally, it became so present in 2016 for the first time and far more when I decided to work for VMware. Did you know that VMware is a huge contributor to Open Source Projects and a Platinum Member at the Cloud Native Computing Foundation - CNCF? You´ll find an overview of our open source projects here. One of these Open Source Projects is vSphere Integrated Containers aka VIC. With vSphere Integrated Containers, Container Images get instantiated as a Virtual Machine by using PhotonOS, a minimal Linux Distribution by VMware, basically to be up and running in a few seconds. Running Containers as a Virtual Machine means that IT-Ops Teams still have the ability to treat them like classical workload before. Therefore you don´t have to build out a separate, tailored infrastructure stack and can continue to leverage existing Scalability, Security and Monitoring capabilities. Figure I: Virtual Container Host vs. Docker Container Host\" Figure I: Virtual Container Host vs. Docker Container Host vSphere Integrated Containers also supports running native Docker container hosts on vSphere. It allows developers to self-provision Docker container hosts for use as a development sandbox, a build server or a swarm cluster. Now you can treat a Docker host as ephemerally as a container. In my role as a VMware Technical Account Manager I have the chance to meet individuals in various roles at Enterprises. Many share the same strategic priority: Transform some of their monolithic applications. vSphere Integrated Containers can leverage a solution for use cases like App Repackaging and Developer Sandbox. Just to name a few. VMware vSphere Integrated Containers Version 1.4 was already launched by our Cloud-Native Business Unit (CNABU) on the 15th of March this year. vSphere Integrated Containers 1.4 is out! Adds support for vSphere 6.7, ROBO Advanced, VCH/C-VM Host affinity and more. https://t.co/UqsIl6xZoX @cloudnativeapps #VMwareVIC — Patrick Daigle (@pdaigle) May 15, 2018 And while I´m writing this series… vSphere Integrated Containers 1.4.1 has just been released - Docs: https://t.co/ZWmmWQ8zHO; RN: https://t.co/Uj2K57ueCx; DL: https://t.co/B2mjmsgpVQ @cloudnativeapps #Docker #vSphere #RunAllTheThings — Bjoern Brundert (@bbrundert) July 13, 2018 …VIC version 1.4.1. has been released and includes: Added the ability to manage use of DRS VM affinity groups for existing VCHs using vic-machine configure. When copying a CLI command on the Summary page of the VCH Creation Wizard, a confirmation message is now shown. New Built-In Repositories view in vSphere Integrated Containers Management Portal provides better visibility on all repositories that reside in the vSphere Integrated Containers Registry. Security and bug fixes. More details through the Release-Notes here. ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-introduction/:0:0","tags":["July2018","VIC","Container","VMware","vSphere"],"title":"VMware vSphere Integrated Containers: Introduction","uri":"/post/vmware-vsphere-integrated-containers-introduction/"},{"categories":["VMware","Container","Cloud Native"],"content":"VIC Component Overview You´ll get an quick overview over the VIC components through this Lightboard Video by Patrick Daigle: ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-introduction/:1:0","tags":["July2018","VIC","Container","VMware","vSphere"],"title":"VMware vSphere Integrated Containers: Introduction","uri":"/post/vmware-vsphere-integrated-containers-introduction/"},{"categories":["VMware","Container","Cloud Native"],"content":"Components vSphere Integrated Containers Engine, a container runtime for vSphere that allows you to provision containers as virtual machines. vSphere Integrated Containers Plug-In for vSphere Client, that provides information about your vSphere Integrated Containers setup and allows you to deploy virtual container hosts directly from the vSphere Client. Harbor, an enterprise-class container registry server that stores and distributes container images. Multi-tenant content signing and validation Security and vulnerability analysis Audit logging Identity integration and role-based access control Image replication between instances Extensible API and graphical UI vSphere Integrated Containers Management Portal based on project Admiral. ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-introduction/:1:1","tags":["July2018","VIC","Container","VMware","vSphere"],"title":"VMware vSphere Integrated Containers: Introduction","uri":"/post/vmware-vsphere-integrated-containers-introduction/"},{"categories":["VMware","Container","Cloud Native"],"content":"Download VMware vSphere Integrated Containers ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-introduction/:2:0","tags":["July2018","VIC","Container","VMware","vSphere"],"title":"VMware vSphere Integrated Containers: Introduction","uri":"/post/vmware-vsphere-integrated-containers-introduction/"},{"categories":["VMware","Container","Cloud Native"],"content":"More ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-introduction/:3:0","tags":["July2018","VIC","Container","VMware","vSphere"],"title":"VMware vSphere Integrated Containers: Introduction","uri":"/post/vmware-vsphere-integrated-containers-introduction/"},{"categories":["VMware","Container","Cloud Native"],"content":"[VIC Solutions Brief] Run Modern Apps with vSphere Integrated Containers ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-introduction/:3:1","tags":["July2018","VIC","Container","VMware","vSphere"],"title":"VMware vSphere Integrated Containers: Introduction","uri":"/post/vmware-vsphere-integrated-containers-introduction/"},{"categories":["VMware","Container","Cloud Native"],"content":"Whitepapers Containers on Virtual Machines or Bare Metal? Architecture Overview (VIC 1.2) ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-introduction/:3:2","tags":["July2018","VIC","Container","VMware","vSphere"],"title":"VMware vSphere Integrated Containers: Introduction","uri":"/post/vmware-vsphere-integrated-containers-introduction/"},{"categories":["VMware","Container","Cloud Native"],"content":"Documentation Official documentation page VIC on Github VIC UI on Github ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-introduction/:3:3","tags":["July2018","VIC","Container","VMware","vSphere"],"title":"VMware vSphere Integrated Containers: Introduction","uri":"/post/vmware-vsphere-integrated-containers-introduction/"},{"categories":["VMware","Container","Cloud Native"],"content":"Release Notes Release Notes vSphere Integrated Containers ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-introduction/:3:4","tags":["July2018","VIC","Container","VMware","vSphere"],"title":"VMware vSphere Integrated Containers: Introduction","uri":"/post/vmware-vsphere-integrated-containers-introduction/"},{"categories":["VMware","Container","Cloud Native"],"content":"VMware Hands-on Labs HOL-1830-02-CNA - vSphere Integrated Containers – Getting Started ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-introduction/:3:5","tags":["July2018","VIC","Container","VMware","vSphere"],"title":"VMware vSphere Integrated Containers: Introduction","uri":"/post/vmware-vsphere-integrated-containers-introduction/"},{"categories":["VMware","Container","Cloud Native"],"content":"Offical VMware Cloud-Native Blog VMware Cloud Native Blog Blog search query - VIC ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-introduction/:3:6","tags":["July2018","VIC","Container","VMware","vSphere"],"title":"VMware vSphere Integrated Containers: Introduction","uri":"/post/vmware-vsphere-integrated-containers-introduction/"},{"categories":["VMware","Container","Cloud Native"],"content":"Recommended VMworld Videos vSphere Integrated Containers Deep Dive: Cool Hacks, Debugging, and Demos (CNA2547BU) Running Docker on Your Existing Infrastructure with vSphere Integrated Containers (CNA1699BU) Deploy vSphere Integrated Containers in Production: Case Study with Allegis (CNA1200BU) ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-introduction/:3:7","tags":["July2018","VIC","Container","VMware","vSphere"],"title":"VMware vSphere Integrated Containers: Introduction","uri":"/post/vmware-vsphere-integrated-containers-introduction/"},{"categories":["VMware","Container","Cloud Native"],"content":"Lightboard Videos VMware Cloud-Native YouTube Channel - VIC Lightboard Videos Overview vSphere Integrated Containers: Networking Overview vSphere Integrated Containers: Storage Continue with: vSphere Integrated Containers Part I: OVA Deployment vSphere Integrated Containers Part II: vSphere Client Plug-In vSphere Integrated Containers Part III: Deployment of a Virtual Container Host vSphere Integrated Containers Part IV: docker run a Container-VM ","date":"2018-07-13","objectID":"/post/vmware-vsphere-integrated-containers-introduction/:3:8","tags":["July2018","VIC","Container","VMware","vSphere"],"title":"VMware vSphere Integrated Containers: Introduction","uri":"/post/vmware-vsphere-integrated-containers-introduction/"},{"categories":["Blogging"],"content":"Why should I start blogging?","date":"2018-07-09","objectID":"/post/sometimes-you-gotta-run-before-you-can-walk/","tags":["July2018","Blogging"],"title":"Sometimes you gotta run before you can walk","uri":"/post/sometimes-you-gotta-run-before-you-can-walk/"},{"categories":["Blogging"],"content":"…and the first lines are the main ones when you´re working on your first post. Many, many thoughts had been running through my head before I decided to write a blog…“Where to start?”…“Is it just another blog?”…and the most frightening idea…“What if YOU, the reader, don´t like what I´m writing about?” So what…I´d like to handle these thoughts like Tony Stark did it in one of my absolute favorite movies Iron Man (1), where he told J.A.R.V.I.S.: “SOMETIMES YOU GOTTA RUN BEFORE YOU CAN WALK.” … ","date":"2018-07-09","objectID":"/post/sometimes-you-gotta-run-before-you-can-walk/:0:0","tags":["July2018","Blogging"],"title":"Sometimes you gotta run before you can walk","uri":"/post/sometimes-you-gotta-run-before-you-can-walk/"},{"categories":["Blogging"],"content":"BIG THANKS to my well appreciated fellows @embano1, @bbrundert, @alec1823 and @tom_schwaller for giving me the impulse and the support (review \u0026 feedback) for this. ","date":"2018-07-09","objectID":"/post/sometimes-you-gotta-run-before-you-can-walk/:1:0","tags":["July2018","Blogging"],"title":"Sometimes you gotta run before you can walk","uri":"/post/sometimes-you-gotta-run-before-you-can-walk/"},{"categories":null,"content":"Please be aware that all content posted on this website represents only my personal opinion and is based on experience I´ve made – it does not represent VMware’s positions, strategies or opinions. ","date":"2018-07-03","objectID":"/disclaimer/:0:0","tags":null,"title":"Disclaimer","uri":"/disclaimer/"},{"categories":null,"content":"Welcome to Robert Guske’s blog ","date":"2018-07-03","objectID":"/about/:1:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Contact Mail Robert Guske @vmw_rguske rguske apiVersion:v1kind:aboutmetadata:name:Robert Guskeregion:EMEAcountry:Germanycompany:VMware Global, Inc.businessUnit:Modern Applications \u0026 Management Business Group (MAMBG)replicas:2---apiVersion:v1kind:Rolemetadata:name:[\"Lead Tanzu Emerging Solutions Engineer\",\"Office of the CTO, Ambassador\"]subject:- kind:Experiseargs:- Cloud Native Technologies- Strategic \u0026 Technical Guidance- Application Platforms- Virtualization ","date":"2018-07-03","objectID":"/about/:2:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Professional Information Robert Guske is an accomplished, hands-on, \u0026 well informed IT professional with 15+ years of experience in providing strategic guidance regarding IT technology and infrastructural enhancements. In his current role as a Lead Tanzu Emergings Solution Engineer within VMware’s Modern Applications \u0026 Management Business Group (MAMBG), he’s supporting clients and organizations adopting new technologies to accomplish business goals and IT objectives, which are mainly oriented towards application modernization as well as on the platforms to run such modern applications (Microservices). Robert is also part of VMware’s Office of the CTO Ambassador program and helps to ensure an active collaboration between VMware’s R\u0026D and field organizations as well as customers across Europe. ","date":"2018-07-03","objectID":"/about/:3:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Open-Source Contributions He’s very keen on cloud native technologies and an active team-member and contributor of the open source project VMware Event Broker Appliance (https://vmweventbroker.io). This project has the intention to reveal the hidden potential of events in VMware’s SDDC by providing event-driven interactions with VMware solutions like e.g. vSphere. Unofficial VEBA Mascot - Otto the Orca\" Unofficial VEBA Mascot - Otto the Orca VMware Event Broker Appliance Homepage: https://vmweventbroker.io Official VEBA Repository: https://github.com/vmware-samples/vcenter-event-broker-appliance/ VEBA @ vBrownBag: https://www.youtube.com/watch?v=tOjp5_qn-Fg\u0026feature=youtu.be VEBA related articles written by Robert: https://rguske.github.io/tags/veba/ ","date":"2018-07-03","objectID":"/about/:4:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Achievements Details via Robert’s Linked profile. ","date":"2018-07-03","objectID":"/about/:5:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Certifications ","date":"2018-07-03","objectID":"/about/:5:1","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"vExpert Status: Active 2019 - 2021 Acknowledgement by sharing passion and gained experience with the community. About the program The annual VMware vExpert title is given to individuals who have significantly contributed to the community of VMware users over the past year. The title is awarded to individuals (not employers) for their commitment to sharing their knowledge and passion for VMware technology above and beyond their job requirements. ","date":"2018-07-03","objectID":"/about/:5:2","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"vExpert Subprograms The vExpert Application Modernization subprogram rewards those, who have stronger contributed on this specific topic (Application modernization). ","date":"2018-07-03","objectID":"/about/:5:3","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"Public Events - Speaking, Demoing, Staff and Initiatives ","date":"2018-07-03","objectID":"/about/:6:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"2021 ","date":"2018-07-03","objectID":"/about/:7:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"VMworld 2021 Activity @ Outcome Evidence VMworld {Code} Connect 2021, Virtual Event SPEAKER - Presented: DIY Deployment of Event-Driven Automation in vSphere Environments #CODE2762 https://youtu.be/ieUqfir5Oag ","date":"2018-07-03","objectID":"/about/:7:1","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"VMware UserCon 2021 Activity @ Outcome Evidence UserCon Germany 2021, Virtual Event CO-SPEAKER - Presented (in German): NSX Advanced Load Balancer (Avi) - Load Balancer für Next Gen und traditionelle Apps. https://youtu.be/8fLXPXs23GI ","date":"2018-07-03","objectID":"/about/:7:2","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"2020 Activity @ Outcome Evidence UserCon Germany 2020, Virtual Event CO-SPEAKER - Presented: Event-Diven Automation with the VMware Event Broker Appliance - Reloaded. https://www.vmug.com/events/all-events/ VMware Usergroup Conference 2020: Gleich startet der nächste Session-Block: https://t.co/XN4DJ3XZBG @vmw_rguske @embano1 @raimes @LarsFiechtner @microlytix #dachvmug #vmug #vmware pic.twitter.com/ilP6acuElv — Jochen Ramm (@virtMark) December 15, 2020 ","date":"2018-07-03","objectID":"/about/:8:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"2019 Activity @ Outcome Evidence UserCon Germany 2019, Frankfurt SPEAKER - Presented: Event-Driven interactions with vSphere using Functions as a Service. https://www.vmug.com/events/all-events/ Here comes my amazing friend and colleague @vmw_rguske presenting the next #DEVMUG breakout session on \"Event-driven interactions with vSphere using Functions as a Service\" + highlighting the @VMwareTAM program #RunAllTheThings pic.twitter.com/qgDeCoEthh — Bjoern Brundert (@bbrundert) May 22, 2019 Activity @ Outcome Evidence Container Days Hamburg 2019 STAFF - at the VMware booth to present the Modern Apps strategy and vision. Tweet below Hamburg is welcoming all @ConDaysEU attendees with best weather 😎☀️. Enjoy #CDS19 pic.twitter.com/D74PZFVZwg — Robert Guske (@vmw_rguske) June 25, 2019 Great to have @cmcluck on stage for kicking off #CDS19. The main hall was fully packed. I feel energized ⚡️ pic.twitter.com/R8ophr0gfw — Robert Guske (@vmw_rguske) June 25, 2019 ","date":"2018-07-03","objectID":"/about/:9:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"2018 Activity @ Outcome Evidence VMworld 2018, Barcelona STAFF - at the VMware Cloud Foundation booth - VMworld Expo Area. Tweet below It was my pleasure working with you @shmimahadevan this year at the @vmwcf booth. „The VCF-booth tag team“ #VMworld pic.twitter.com/4OrbEyL4B5 — Robert Guske (@vmw_rguske) November 8, 2018 Activity @ Outcome Evidence VMworld 2018, Barcelona STAFF - Representing the VMware TAM program at the VMworld TAM Customer Central area. Tweet below This is a great #VMwareAd, the company that brought you VMs, will be the leader in enterprise Containers in due course. #VMworld https://t.co/H7LsSCCq6t — Sanjay Poonen (@spoonen) November 5, 2018 Activity @ Outcome Evidence VMware TAM Customer Roundtable 2018, Hamburg HOST \u0026 SPEAKER - An exclusive event for VMware TAM customers. Robert organized and hosted the event as well as presented on it. Tweet below 1st @VMwareTAM Customer #Automation #Roundtable in #Hamburg took place. Thanks to all who participated, shared valueable content and gave us helpful feedback. Many thx to @Jay_Jay_HH and @joerglew for your support here👊🏻 You guys 🤘🏻 pic.twitter.com/Rpisc5NmUj — Robert Guske (@vmw_rguske) June 5, 2018 ","date":"2018-07-03","objectID":"/about/:10:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"2017 Activity @ Outcome Evidence VMworld 2017, Barcelona STAFF - VMworld Hands-on Labs team-member. Tweet below VMworld 2017 - Hands-on Labs Staff\" VMworld 2017 - Hands-on Labs Staff Providing conference attendees a unique Hands-on Labs experience was part of his role and he took it seriously 😊 . Certificate of Achievement - VMworld 2017 HOL\" Certificate of Achievement - VMworld 2017 HOL ","date":"2018-07-03","objectID":"/about/:11:0","tags":null,"title":"About me","uri":"/about/"},{"categories":null,"content":"2016 Activity @ Outcome Evidence Bechtle IT Forum Nord, Hamburg SPEAKER - The IT Forum Nord is a customer facing event hosted by the Bechtle IT Systemhaus Hamburg GmbH. Employed as a Datacenter Consultant at this time, he presented on the topic Hyperconverged Solutions by Hewlett Packard Enterprise. Link to the customer invitation Speaker - Bechtle IT Forum Nord 2016\" Speaker - Bechtle IT Forum Nord 2016 ","date":"2018-07-03","objectID":"/about/:12:0","tags":null,"title":"About me","uri":"/about/"}]